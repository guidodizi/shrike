{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Shrike: Compliant Azure ML Utilities The shrike library is a set of Python utilities for running experiments in the Azure Machine Learning platform ( a.k.a. Azure ML). This library contains four elements, which are: shrike.compliant_logging : utilities for compliant logging and exception handling; shrike.pipeline : helper code for managing, validating and submitting Azure ML pipelines based on azure-ml-component ( a.k.a. the Component SDK); shrike.build : helper code for packaging, building, validating, signing and registering Azure ML components. shrike.spark : utilities for running jobs, especially those leveraging Spark .NET, in HDInsight. Documentation For the full documentation of shrike with detailed examples and API reference, please see the docs page . For a list of problems (along with guidance and solutions) designed specifically to help you learn how to use shrike, please refer to the information in this README file (located in another GitHub repository). Installation The shrike library is publicly available in PyPi. There are three optional extra dependencies: pipeline , build , and dev . The pipeline dependency is for submitting Azure ML pipelines, build is for signing and registering components, and dev is for the development environment of shrike . If you are only planning on using the compliant-logging feature, please pip install without any extras: pip install shrike If you are planning on signing and registering components, please pip install with [build] : pip install shrike [build] If you are planning on submitting Azure ML pipelines, please pip install with [pipeline] : pip install shrike [pipeline] If you would like to contribute to the source code, please pip install with all the dependencies: pip install shrike [pipeline,build,dev] Alternatively, for local development, you may use the Conda environment defined in environment.yml . It pins the appropriate versions of pip, Python, and installs all shrike together with all extras as an editable package. Migration from aml-build-tooling , aml-ds-pipeline-contrib , and confidential-ml-utils If you have been using the aml-build-tooling , aml-ds-pipeline-contrib , or confidential-ml-utils libraries, please use the migration script ( migration.py ) to convert your repo or files and adopt the shrike package with one simple command: python migraton . py - -input_path PATH / TO / YOUR / REPO / OR / FILE :warning: This command will update files in-place . Please make a copy of your repo/file if you do not want to do so. Need Support? If you have any feature requests, technical questions, or find any bugs, please do not hesitate to reach out to us. For bug reports and feature requests, you are welcome to open an issue . If you are a Microsoft employee, please refer to the support page for details; If you are outside Microsoft, please send an email to aml-ds@microsoft.com . Contributing This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com. When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Trademarks This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft's Trademark & Brand Guidelines . Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.","title":"Home"},{"location":"#shrike-compliant-azure-ml-utilities","text":"The shrike library is a set of Python utilities for running experiments in the Azure Machine Learning platform ( a.k.a. Azure ML). This library contains four elements, which are: shrike.compliant_logging : utilities for compliant logging and exception handling; shrike.pipeline : helper code for managing, validating and submitting Azure ML pipelines based on azure-ml-component ( a.k.a. the Component SDK); shrike.build : helper code for packaging, building, validating, signing and registering Azure ML components. shrike.spark : utilities for running jobs, especially those leveraging Spark .NET, in HDInsight.","title":"Shrike: Compliant Azure ML Utilities"},{"location":"#documentation","text":"For the full documentation of shrike with detailed examples and API reference, please see the docs page . For a list of problems (along with guidance and solutions) designed specifically to help you learn how to use shrike, please refer to the information in this README file (located in another GitHub repository).","title":"Documentation"},{"location":"#installation","text":"The shrike library is publicly available in PyPi. There are three optional extra dependencies: pipeline , build , and dev . The pipeline dependency is for submitting Azure ML pipelines, build is for signing and registering components, and dev is for the development environment of shrike . If you are only planning on using the compliant-logging feature, please pip install without any extras: pip install shrike If you are planning on signing and registering components, please pip install with [build] : pip install shrike [build] If you are planning on submitting Azure ML pipelines, please pip install with [pipeline] : pip install shrike [pipeline] If you would like to contribute to the source code, please pip install with all the dependencies: pip install shrike [pipeline,build,dev] Alternatively, for local development, you may use the Conda environment defined in environment.yml . It pins the appropriate versions of pip, Python, and installs all shrike together with all extras as an editable package.","title":"Installation"},{"location":"#migration-from-aml-build-tooling-aml-ds-pipeline-contrib-and-confidential-ml-utils","text":"If you have been using the aml-build-tooling , aml-ds-pipeline-contrib , or confidential-ml-utils libraries, please use the migration script ( migration.py ) to convert your repo or files and adopt the shrike package with one simple command: python migraton . py - -input_path PATH / TO / YOUR / REPO / OR / FILE :warning: This command will update files in-place . Please make a copy of your repo/file if you do not want to do so.","title":"Migration from aml-build-tooling, aml-ds-pipeline-contrib, and confidential-ml-utils"},{"location":"#need-support","text":"If you have any feature requests, technical questions, or find any bugs, please do not hesitate to reach out to us. For bug reports and feature requests, you are welcome to open an issue . If you are a Microsoft employee, please refer to the support page for details; If you are outside Microsoft, please send an email to aml-ds@microsoft.com .","title":"Need Support?"},{"location":"#contributing","text":"This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com. When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"Contributing"},{"location":"#trademarks","text":"This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft's Trademark & Brand Guidelines . Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.","title":"Trademarks"},{"location":"build/build-for-sign-and-register/","text":"Creating an Azure DevOps Build for Signing and Registering By reading through this doc, you will be able to have a high-level understanding of how to use shrike.build , and create a single-YAML pipeline build in Azure DevOps for validating, signing and registering Azure ML components. Requirements To enjoy this tutorial, you need to have at least one Azure ML component YAML specification file in your team's repository, have an Azure ML service connection set up in your Azure DevOps for your Azure subscription, have an ESRP service connection set up in your Azure DevOps, and have a basic knowledge of Azure DevOps pipeline YAML schema . Configuration Command line arguments and configuration YAML file are both supported by shrike.build . The order of precedence from least to greatest (the last listed variables override all other variables) is: default values, configuration file, command line arguments. An example of configuration YAML file: # Choose from two signing mode: aml, or aether signing_mode : aml # Two methods are provided to find \"active\" components: all, or smart # For \"all\" option, all the components will be validated/signed/registered # For \"smart\" option, only those changed components will be processed. activation_method : all # Regular expression that a branch must satisfy in order for this code to # sign or register components. compliant_branch : ^refs/heads/main$ # Glob path of all component specification files. component_specification_glob : 'steps/**/module_spec.yaml' log_format : '[%(name)s][%(levelname)s] - %(message)s' # List of workspace ARM IDs (fill in the <> with the appropriate values for your Azure ML workspace) workspaces : - /subscriptions/<Subscription-Id>/resourcegroups/<Name-Of-Resource-Group>/providers/Microsoft.MachineLearningServices/workspaces/<Azure-ML-Workspace-Name> # Boolean argument: What to do when the same version of a component has already been registered. # Default: False fail_if_version_exists : False # Boolean argument: Will the build number be used or not use_build_number : True To consume this configuration file, we should pass its path to the command line, that is python -m shrike.build.commands.prepare --configuration-file PATH/TO/MY_CONFIGURATION_FILE If we want to override the values of activation_method and fail_if_version_exists at runtime, we should append them to the command line: python -m shrike.build.commands.prepare --configuration-file PATH/TO/MY_CONFIGURATION_FILE --activation-method smart --fail-if-version-exists \"Smart\" mode The shrike package supports a \"smart\" activation_method . To use it, just include the following line to your build configuration file. activation_method : smart Using this \"smart\" mode will only register the components that were modified, given a list of modified files. The logic used to identify which components are modified is as follows. The modified file needs to be tracked in git to be picked up by the tool. If it isn't tracked in git, it won't be considered - even if it listed in a component's additional_includes file. If a file located in the component folder is changed, then the component is considered to be modified. If a file listed in the additional_includes file (file directly listed, or its parent folder listed) is changed, then the component is considered to be modified. The paths listed in the additional_includes file are all assumed to be relative to the location of that file. Note: A corollary of point 2 above is that if you modify a function in a helper file listed in the additional_includes , your component will be considered as modified even if it does not use that function at all. That is why we use quotes around \"smart\": the logic is not smart enough to detect only the components truly affected by a change (implementing that logic would be a much more complicated task). Note: Another corollary of point 2 is that if you want to use the \"smart\" mode, you need to be as accurate as possible with the files listed in the additional_includes , otherwise components might be registered even though the changes didn't really affect them. Imagine the extreme case where you have a huge utils directory listed in additional_includes instead of the specific list of utils files: every change to that directory, even if not relevant to your component of interest, will trigger the registration. This would defeat the purpose of having a smart mode in the first place. :warning: It is worth reiterating that for the tool to work properly, the name of the compliant branch in your config file should be of the form \" ^refs/heads/<YourCompliantBranchName>$ \" . (Notice how it starts with \" ^refs/heads/ \" and ends with \" $ \".) However, regular expressions are not supported by the \"smart\" mode , since there would be some ambiguity in determining the list of modified files when there are several compliant (i.e. reference) branches. :warning: To identify the latest merge into the compliant branch, the tool relies on the Azure DevOps convention that the commit message starts with \"Merged PR\". If you customize the commit message, please make sure it still starts with \"Merged PR\", otherwise the \"smart\" logic will not work properly. :information_source: In some (rare) instances, we have seen pull requests being successfully merged, but with the build failing. In these cases, the new components introduced/modified in the problematic PR have not been signed/registered, and unless they are modified by a subsequent PR, they will not be picked up by the \"smart\" mode. There are 2 common workarounds to this issue. The most straightforward is to activate the \"all\" activation mode in a PR following the failed build, then revert to \"smart\" for the PR after that. This will ensure all components are registered, but will also mess up the components results recycling logic: some components will wrongly be considered as a new, hence their results won't be recycled. The second option is to do a mock PR that just bumps up the version numbers or adds a dummy comment to the specification files of the components modified in the problematic PR. This option has the advantage of not interfering with components results recycling, but is harder to implement if the problematic PR affects many components. Preparation step In this section, we briefly describe the workflow of the prepare command in the shrike library, that is Search all Azure ML components in the working directory by matching the glob path of component specification files, Add repo and commit info to \"tags\" and \"description\" section of spec.yaml , Validate all \"active\" components, Build all \"active\" components, and Create files catlog.json and catalog.json.sig for each \"active\" component. Note: While building \"active\" components, all additional dependency files specified in .additional_includes will be copied into the component build folder by the prepare command. However, for those dependecy files that are not checked into the repository, such as OdinML Jar (from NuGet packages) and .zip files, we need to write extra \"tasks\" to copy them into the component build folder. A sample YAML script of preparation step - task : AzureCLI@2 displayName : Preparation inputs : azureSubscription : $(MY_AML_WORKSPACE_SERVICE_CONNECTION) scriptLocation : inlineScript scriptType : pscore inlineScript : | python -m shrike.build.commands.prepare --configuration-file PATH/TO/MY_CONFIGURATION_FILE workingDirectory : $(MY_WORK_DIRECTORY) Customized validation on components (optional) At the prepare step of the signing and registering build, the shrike.build.command.prepare.validate_all_components() function executes an azure cli command az ml component validate --file ${component_spec_path} to validate whether the given component spec YAML file has any syntax errors or matches the strucuture of the pre-defined schema. Apart from the standard validation via az cli, users can also enforce customized \"strict\" validation on Azure ML components. There are two parameters - enable_component_validation (type: boolean , default: False ) and component_validation (type: dict , default: None ) that could be specified in the configuration file. If config.enable_component_validation is True , it will first check whether the components are compliant, then run the user-provided customized validation. We expect users to write JSONPath expressions to query Azure ML component spec YAML elements. For example, the path of component name is $.name , while the path of image is $.environment.docker.image . Then, users are expected to translate their specific \"strict\" validation rules to regular expression patterns. For example, enforcing the component name to start with \"smartreply.\" could be translated to a string pattern ^smartreply.[A-Za-z0-9-_.]+$ . After that, the JSONPath expressions and corresponding regular expressions will be combined into a dict and assigned to config.component_validation in the configuration file. Assuming we enforce two \"strict\" validation requirements on the component: (1) the component name starts with smartreply. , (2) all the input parameter descriptions start with a capital letter. Below is an example of the configuration file that specifies the above two validation requirements. activation_method : all compliant_branch : ^refs/heads/develop$ component_specification_glob : 'components/**/module_spec.yaml' log_format : '[%(name)s][%(levelname)s] - %(message)s' signing_mode : aml workspaces : - /subscriptions/<Subscription-Id>/resourcegroups/<Name-Of-Resource-Group>/providers/Microsoft.MachineLearningServices/workspaces/<Azure-ML-Workspace-Name> allow_duplicate_versions : True use_build_number : True # strict component validation enable_component_validation : True component_validation : '$.name' : '^smartreply.[A-Za-z0-9-_.]+$' '$.inputs..description' : '^[A-Z].*' Please refer to this proposal doc for more details on the customized validation. ESRP CodeSign After creating catlog.json and catalog.json.sig files for each built component in the preparation step, we leverage the ESRP, that is Engineer Sercurity and Release Platform , to sign the contents of components. In the sample YAML script below, we need to customize ConnectedServiceName and FolderPath . In TEEGit repo, the name of ESRP service connection for Torus tenant (Tenant Id: \u200bcdc5aeea-15c5-4db6-b079-fcadd2505dc2\u200b) is Substrate AI ESRP . For other repos, if the service connection for ESRP has not been set up yet, please refer to the ESRP CodeSign task Wiki for detailed instructions. - task : EsrpCodeSigning@1 displayName : ESRP CodeSigning inputs : ConnectedServiceName : $(MY_ESRP_SERVICE_CONNECTION) FolderPath : $(MY_WORK_DIRECTORY) Pattern : '*.sig' signConfigType : inlineSignParams inlineOperation : | [ { \"KeyCode\": \"CP-460703-Pgp\", \"OperationCode\": \"LinuxSign\", \"parameters\": {}, \"toolName\": \"sign\", \"toolVersion\": \"1.0\" } ] SessionTimeout : 20 VerboseLogin : true Note: This step requires one-time authorization from the administrator of your ESRP service connection. Please contact your manager or tech lead for authorization questions. Component registration The last step is to register all signed components in your Azure ML workspaces. The register class in the shrike library implements the registration procedure by executing the Azure CLI command az ml component --create --file {component} . The Python call is python - m shrike . build . commands . register -- configuration - file path / to / config In this step, the register class can detect signed and built components. There are five configuration parameters related to the registration step: --compliant-branch , --source-branch , --fail-if-version-exists , --use-build-number , and --all-component-version . They should be customized in the configure-file according to your specific use case. The register class checks whether the value of source_branch matches that of compliant_branch before starting registration. If their pattern doesn't match, an error message will be logged and the registration step will be terminated. If fail_if_version_exists is True, an error is raised and the registration step is terminated when the version number of some signed component already exists in the workspace; Otherwise, only a warning is raised and the registration step continues. If all_component_version is not None , the value of all_component_version is used as the version number for all signed components. If use_build_number is True, the build number is used as the version number for all signed components (Overriding the value of all_component_version if all_component_version is not None ). A sample YAML task for registration is - task : AzureCLI@2 displayName : AML Component Registration inputs : azureSubscription : $(MY_AML_WORKSPACE_SERVICE_CONNECTION) scriptLocation : inlineScript scriptType : pscore inlineScript : | python -m shrike.build.commands.register --configuration-file PATH/TO/MY_CONFIGURATION_FILE workingDirectory : $(MY_WORK_DIRECTORY) Note: The shrike library is version-aware. For a component of product-ready version number (e.g., a.b.c), it is set as the default version in the registration step; Otherwise, for a component of non-product-ready version number (e.g., a.b.c-alpha), it will not be labelled as default. Handling components which use binaries For some components (e.g., Linux/Windows components running .NET Core DLLs or Windows Exes, or HDI components leveraging the ODIN-ML JAR or Spark .NET), the signed snapshot needs to contain some binaries. As long as those binaries are compiled from human-reviewed source code or come from internal (authenticated) feeds , this is fine. Teams may inject essentially arbitrary logic into their Azure DevOps pipeline, either for compiling C# code, or downloading \\& extracting NuGets from the Polymer NuGet feed. \u00c6ther-style code signing This tool also assists with \u00c6ther-style code signing. Just write a configuration file like: component_specification_glob : '**/ModuleAutoApprovalManifest.json' signing_mode : aether and then run a code signing step like this just after the \"prepare\" command. Note: your ESRP service connection will need to have access to the CP-230012 key, otherwise you'll encounter the error described in: Got unauthorized to access CP-230012 when calling Aether-style signing service - task : EsrpCodeSigning@1 displayName : sign modules inputs : ConnectedServiceName : $(MY_ESRP_SERVICE_CONNECTION) FolderPath : $(MY_WORK_DIRECTORY) Pattern : '*.cat' signConfigType : inlineSignParams inlineOperation : | [ { \"keyCode\": \"CP-230012\", \"operationSetCode\": \"SigntoolSign\", \"parameters\": [ { \"parameterName\": \"OpusName\", \"parameterValue\": \"Microsoft\" }, { \"parameterName\": \"OpusInfo\", \"parameterValue\": \"http://www.microsoft.com\" }, { \"parameterName\": \"PageHash\", \"parameterValue\": \"/NPH\" }, { \"parameterName\": \"FileDigest\", \"parameterValue\": \"/fd sha256\" }, { \"parameterName\": \"TimeStamp\", \"parameterValue\": \"/tr \\\"http://rfc3161.gtm.corp.microsoft.com/TSS/HttpTspServer\\\" /td sha256\" } ], \"toolName\": \"signtool.exe\", \"toolVersion\": \"6.2.9304.0\" } ] SessionTimeout : 20 VerboseLogin : true \u00c6ther does not support \"true\" CI/CD, but you will be able to use your build drops to register compliant \u00c6ther modules following Signed Builds . For reference, you may imitate this build used by the AML Data Science team . Note: there is no need to run the Azure ML-style and \u00c6ther-style code signing in separate jobs. So long as they both run in a Windows VM, it may be the same job. Per-component builds If you want your team to be able to manually trigger \"\u00c6ther-style\" per-component builds from their compliant branches, consider creating a separate build definition with the following changes. Top of build definition. name : $(Date:yyyyMMdd)$(Rev:.r)-dev parameters : - name : aml_component type : string default : '**' ``` Inline script portion of your \"prepare\" and \"register\" steps (you will need to customize the configuration file name and glob to your repository). ``` bash python -m shrike.build.commands.register --configuration-file sign-register-config-dev.yaml --component-specification-glob src/steps/${{ parameters.aml_component }}/component_spec.yaml Then, members of your team can manually trigger builds via the Azure DevOps UI, setting the aml_component parameter to the name of the component they want to code-sign and register. Tips Another way of achieving similar functionality is to run several \"smart mode\" builds which trigger against all compliant/* branches. To do so, you will need several build config files, one for each compliant branch. Name your builds something like *-dev so that these versions of the components don't get registered as default. See the Search Relevance team's example for something complete: [yaml] SearchRelevance AML Components Signing and Registering - dev .","title":"Sign & register components"},{"location":"build/build-for-sign-and-register/#creating-an-azure-devops-build-for-signing-and-registering","text":"By reading through this doc, you will be able to have a high-level understanding of how to use shrike.build , and create a single-YAML pipeline build in Azure DevOps for validating, signing and registering Azure ML components.","title":"Creating an Azure DevOps Build for Signing and Registering"},{"location":"build/build-for-sign-and-register/#requirements","text":"To enjoy this tutorial, you need to have at least one Azure ML component YAML specification file in your team's repository, have an Azure ML service connection set up in your Azure DevOps for your Azure subscription, have an ESRP service connection set up in your Azure DevOps, and have a basic knowledge of Azure DevOps pipeline YAML schema .","title":"Requirements"},{"location":"build/build-for-sign-and-register/#configuration","text":"Command line arguments and configuration YAML file are both supported by shrike.build . The order of precedence from least to greatest (the last listed variables override all other variables) is: default values, configuration file, command line arguments. An example of configuration YAML file: # Choose from two signing mode: aml, or aether signing_mode : aml # Two methods are provided to find \"active\" components: all, or smart # For \"all\" option, all the components will be validated/signed/registered # For \"smart\" option, only those changed components will be processed. activation_method : all # Regular expression that a branch must satisfy in order for this code to # sign or register components. compliant_branch : ^refs/heads/main$ # Glob path of all component specification files. component_specification_glob : 'steps/**/module_spec.yaml' log_format : '[%(name)s][%(levelname)s] - %(message)s' # List of workspace ARM IDs (fill in the <> with the appropriate values for your Azure ML workspace) workspaces : - /subscriptions/<Subscription-Id>/resourcegroups/<Name-Of-Resource-Group>/providers/Microsoft.MachineLearningServices/workspaces/<Azure-ML-Workspace-Name> # Boolean argument: What to do when the same version of a component has already been registered. # Default: False fail_if_version_exists : False # Boolean argument: Will the build number be used or not use_build_number : True To consume this configuration file, we should pass its path to the command line, that is python -m shrike.build.commands.prepare --configuration-file PATH/TO/MY_CONFIGURATION_FILE If we want to override the values of activation_method and fail_if_version_exists at runtime, we should append them to the command line: python -m shrike.build.commands.prepare --configuration-file PATH/TO/MY_CONFIGURATION_FILE --activation-method smart --fail-if-version-exists","title":"Configuration"},{"location":"build/build-for-sign-and-register/#smart-mode","text":"The shrike package supports a \"smart\" activation_method . To use it, just include the following line to your build configuration file. activation_method : smart Using this \"smart\" mode will only register the components that were modified, given a list of modified files. The logic used to identify which components are modified is as follows. The modified file needs to be tracked in git to be picked up by the tool. If it isn't tracked in git, it won't be considered - even if it listed in a component's additional_includes file. If a file located in the component folder is changed, then the component is considered to be modified. If a file listed in the additional_includes file (file directly listed, or its parent folder listed) is changed, then the component is considered to be modified. The paths listed in the additional_includes file are all assumed to be relative to the location of that file. Note: A corollary of point 2 above is that if you modify a function in a helper file listed in the additional_includes , your component will be considered as modified even if it does not use that function at all. That is why we use quotes around \"smart\": the logic is not smart enough to detect only the components truly affected by a change (implementing that logic would be a much more complicated task). Note: Another corollary of point 2 is that if you want to use the \"smart\" mode, you need to be as accurate as possible with the files listed in the additional_includes , otherwise components might be registered even though the changes didn't really affect them. Imagine the extreme case where you have a huge utils directory listed in additional_includes instead of the specific list of utils files: every change to that directory, even if not relevant to your component of interest, will trigger the registration. This would defeat the purpose of having a smart mode in the first place. :warning: It is worth reiterating that for the tool to work properly, the name of the compliant branch in your config file should be of the form \" ^refs/heads/<YourCompliantBranchName>$ \" . (Notice how it starts with \" ^refs/heads/ \" and ends with \" $ \".) However, regular expressions are not supported by the \"smart\" mode , since there would be some ambiguity in determining the list of modified files when there are several compliant (i.e. reference) branches. :warning: To identify the latest merge into the compliant branch, the tool relies on the Azure DevOps convention that the commit message starts with \"Merged PR\". If you customize the commit message, please make sure it still starts with \"Merged PR\", otherwise the \"smart\" logic will not work properly. :information_source: In some (rare) instances, we have seen pull requests being successfully merged, but with the build failing. In these cases, the new components introduced/modified in the problematic PR have not been signed/registered, and unless they are modified by a subsequent PR, they will not be picked up by the \"smart\" mode. There are 2 common workarounds to this issue. The most straightforward is to activate the \"all\" activation mode in a PR following the failed build, then revert to \"smart\" for the PR after that. This will ensure all components are registered, but will also mess up the components results recycling logic: some components will wrongly be considered as a new, hence their results won't be recycled. The second option is to do a mock PR that just bumps up the version numbers or adds a dummy comment to the specification files of the components modified in the problematic PR. This option has the advantage of not interfering with components results recycling, but is harder to implement if the problematic PR affects many components.","title":"\"Smart\" mode"},{"location":"build/build-for-sign-and-register/#preparation-step","text":"In this section, we briefly describe the workflow of the prepare command in the shrike library, that is Search all Azure ML components in the working directory by matching the glob path of component specification files, Add repo and commit info to \"tags\" and \"description\" section of spec.yaml , Validate all \"active\" components, Build all \"active\" components, and Create files catlog.json and catalog.json.sig for each \"active\" component. Note: While building \"active\" components, all additional dependency files specified in .additional_includes will be copied into the component build folder by the prepare command. However, for those dependecy files that are not checked into the repository, such as OdinML Jar (from NuGet packages) and .zip files, we need to write extra \"tasks\" to copy them into the component build folder. A sample YAML script of preparation step - task : AzureCLI@2 displayName : Preparation inputs : azureSubscription : $(MY_AML_WORKSPACE_SERVICE_CONNECTION) scriptLocation : inlineScript scriptType : pscore inlineScript : | python -m shrike.build.commands.prepare --configuration-file PATH/TO/MY_CONFIGURATION_FILE workingDirectory : $(MY_WORK_DIRECTORY)","title":"Preparation step"},{"location":"build/build-for-sign-and-register/#customized-validation-on-components-optional","text":"At the prepare step of the signing and registering build, the shrike.build.command.prepare.validate_all_components() function executes an azure cli command az ml component validate --file ${component_spec_path} to validate whether the given component spec YAML file has any syntax errors or matches the strucuture of the pre-defined schema. Apart from the standard validation via az cli, users can also enforce customized \"strict\" validation on Azure ML components. There are two parameters - enable_component_validation (type: boolean , default: False ) and component_validation (type: dict , default: None ) that could be specified in the configuration file. If config.enable_component_validation is True , it will first check whether the components are compliant, then run the user-provided customized validation. We expect users to write JSONPath expressions to query Azure ML component spec YAML elements. For example, the path of component name is $.name , while the path of image is $.environment.docker.image . Then, users are expected to translate their specific \"strict\" validation rules to regular expression patterns. For example, enforcing the component name to start with \"smartreply.\" could be translated to a string pattern ^smartreply.[A-Za-z0-9-_.]+$ . After that, the JSONPath expressions and corresponding regular expressions will be combined into a dict and assigned to config.component_validation in the configuration file. Assuming we enforce two \"strict\" validation requirements on the component: (1) the component name starts with smartreply. , (2) all the input parameter descriptions start with a capital letter. Below is an example of the configuration file that specifies the above two validation requirements. activation_method : all compliant_branch : ^refs/heads/develop$ component_specification_glob : 'components/**/module_spec.yaml' log_format : '[%(name)s][%(levelname)s] - %(message)s' signing_mode : aml workspaces : - /subscriptions/<Subscription-Id>/resourcegroups/<Name-Of-Resource-Group>/providers/Microsoft.MachineLearningServices/workspaces/<Azure-ML-Workspace-Name> allow_duplicate_versions : True use_build_number : True # strict component validation enable_component_validation : True component_validation : '$.name' : '^smartreply.[A-Za-z0-9-_.]+$' '$.inputs..description' : '^[A-Z].*' Please refer to this proposal doc for more details on the customized validation.","title":"Customized validation on components (optional)"},{"location":"build/build-for-sign-and-register/#esrp-codesign","text":"After creating catlog.json and catalog.json.sig files for each built component in the preparation step, we leverage the ESRP, that is Engineer Sercurity and Release Platform , to sign the contents of components. In the sample YAML script below, we need to customize ConnectedServiceName and FolderPath . In TEEGit repo, the name of ESRP service connection for Torus tenant (Tenant Id: \u200bcdc5aeea-15c5-4db6-b079-fcadd2505dc2\u200b) is Substrate AI ESRP . For other repos, if the service connection for ESRP has not been set up yet, please refer to the ESRP CodeSign task Wiki for detailed instructions. - task : EsrpCodeSigning@1 displayName : ESRP CodeSigning inputs : ConnectedServiceName : $(MY_ESRP_SERVICE_CONNECTION) FolderPath : $(MY_WORK_DIRECTORY) Pattern : '*.sig' signConfigType : inlineSignParams inlineOperation : | [ { \"KeyCode\": \"CP-460703-Pgp\", \"OperationCode\": \"LinuxSign\", \"parameters\": {}, \"toolName\": \"sign\", \"toolVersion\": \"1.0\" } ] SessionTimeout : 20 VerboseLogin : true Note: This step requires one-time authorization from the administrator of your ESRP service connection. Please contact your manager or tech lead for authorization questions.","title":"ESRP CodeSign"},{"location":"build/build-for-sign-and-register/#component-registration","text":"The last step is to register all signed components in your Azure ML workspaces. The register class in the shrike library implements the registration procedure by executing the Azure CLI command az ml component --create --file {component} . The Python call is python - m shrike . build . commands . register -- configuration - file path / to / config In this step, the register class can detect signed and built components. There are five configuration parameters related to the registration step: --compliant-branch , --source-branch , --fail-if-version-exists , --use-build-number , and --all-component-version . They should be customized in the configure-file according to your specific use case. The register class checks whether the value of source_branch matches that of compliant_branch before starting registration. If their pattern doesn't match, an error message will be logged and the registration step will be terminated. If fail_if_version_exists is True, an error is raised and the registration step is terminated when the version number of some signed component already exists in the workspace; Otherwise, only a warning is raised and the registration step continues. If all_component_version is not None , the value of all_component_version is used as the version number for all signed components. If use_build_number is True, the build number is used as the version number for all signed components (Overriding the value of all_component_version if all_component_version is not None ). A sample YAML task for registration is - task : AzureCLI@2 displayName : AML Component Registration inputs : azureSubscription : $(MY_AML_WORKSPACE_SERVICE_CONNECTION) scriptLocation : inlineScript scriptType : pscore inlineScript : | python -m shrike.build.commands.register --configuration-file PATH/TO/MY_CONFIGURATION_FILE workingDirectory : $(MY_WORK_DIRECTORY) Note: The shrike library is version-aware. For a component of product-ready version number (e.g., a.b.c), it is set as the default version in the registration step; Otherwise, for a component of non-product-ready version number (e.g., a.b.c-alpha), it will not be labelled as default.","title":"Component registration"},{"location":"build/build-for-sign-and-register/#handling-components-which-use-binaries","text":"For some components (e.g., Linux/Windows components running .NET Core DLLs or Windows Exes, or HDI components leveraging the ODIN-ML JAR or Spark .NET), the signed snapshot needs to contain some binaries. As long as those binaries are compiled from human-reviewed source code or come from internal (authenticated) feeds , this is fine. Teams may inject essentially arbitrary logic into their Azure DevOps pipeline, either for compiling C# code, or downloading \\& extracting NuGets from the Polymer NuGet feed.","title":"Handling components which use binaries"},{"location":"build/build-for-sign-and-register/#ther-style-code-signing","text":"This tool also assists with \u00c6ther-style code signing. Just write a configuration file like: component_specification_glob : '**/ModuleAutoApprovalManifest.json' signing_mode : aether and then run a code signing step like this just after the \"prepare\" command. Note: your ESRP service connection will need to have access to the CP-230012 key, otherwise you'll encounter the error described in: Got unauthorized to access CP-230012 when calling Aether-style signing service - task : EsrpCodeSigning@1 displayName : sign modules inputs : ConnectedServiceName : $(MY_ESRP_SERVICE_CONNECTION) FolderPath : $(MY_WORK_DIRECTORY) Pattern : '*.cat' signConfigType : inlineSignParams inlineOperation : | [ { \"keyCode\": \"CP-230012\", \"operationSetCode\": \"SigntoolSign\", \"parameters\": [ { \"parameterName\": \"OpusName\", \"parameterValue\": \"Microsoft\" }, { \"parameterName\": \"OpusInfo\", \"parameterValue\": \"http://www.microsoft.com\" }, { \"parameterName\": \"PageHash\", \"parameterValue\": \"/NPH\" }, { \"parameterName\": \"FileDigest\", \"parameterValue\": \"/fd sha256\" }, { \"parameterName\": \"TimeStamp\", \"parameterValue\": \"/tr \\\"http://rfc3161.gtm.corp.microsoft.com/TSS/HttpTspServer\\\" /td sha256\" } ], \"toolName\": \"signtool.exe\", \"toolVersion\": \"6.2.9304.0\" } ] SessionTimeout : 20 VerboseLogin : true \u00c6ther does not support \"true\" CI/CD, but you will be able to use your build drops to register compliant \u00c6ther modules following Signed Builds . For reference, you may imitate this build used by the AML Data Science team . Note: there is no need to run the Azure ML-style and \u00c6ther-style code signing in separate jobs. So long as they both run in a Windows VM, it may be the same job.","title":"&AElig;ther-style code signing"},{"location":"build/build-for-sign-and-register/#per-component-builds","text":"If you want your team to be able to manually trigger \"\u00c6ther-style\" per-component builds from their compliant branches, consider creating a separate build definition with the following changes. Top of build definition. name : $(Date:yyyyMMdd)$(Rev:.r)-dev parameters : - name : aml_component type : string default : '**' ``` Inline script portion of your \"prepare\" and \"register\" steps (you will need to customize the configuration file name and glob to your repository). ``` bash python -m shrike.build.commands.register --configuration-file sign-register-config-dev.yaml --component-specification-glob src/steps/${{ parameters.aml_component }}/component_spec.yaml Then, members of your team can manually trigger builds via the Azure DevOps UI, setting the aml_component parameter to the name of the component they want to code-sign and register. Tips Another way of achieving similar functionality is to run several \"smart mode\" builds which trigger against all compliant/* branches. To do so, you will need several build config files, one for each compliant branch. Name your builds something like *-dev so that these versions of the components don't get registered as default. See the Search Relevance team's example for something complete: [yaml] SearchRelevance AML Components Signing and Registering - dev .","title":"Per-component builds"},{"location":"build/prepare/","text":"Prepare Prepare all_files_in_snapshot ( self , manifest ) Return a list of all normalized files in the snapshot. The input ( manifest ) is assumed to be some file, whether AML-style component spec or Aether-style auto-approval manifest, in the \"root\" of the snapshot. Source code in shrike/build/commands/prepare.py def all_files_in_snapshot ( self , manifest : str ) -> List [ str ]: \"\"\" Return a list of all normalized files in the snapshot. The input (`manifest`) is assumed to be some file, whether AML-style component spec or Aether-style auto-approval manifest, in the \"root\" of the snapshot. \"\"\" folder_path = self . folder_path ( manifest ) log . info ( \"Absolute path for current component is: \" + folder_path ) # Generate a list of all files in this components folder (including subdirectories) rv = [] # Make sure we pick up Linux-style \"hidden\" files like .amlignore and # hidden \"directories\", as well as hidden files in hidden directories. # https://stackoverflow.com/a/65205404 # https://stackoverflow.com/a/41447012 for root , _ , file_paths in os . walk ( folder_path ): for file in file_paths : file_path = os . path . join ( root , file ) normalized_path = self . normalize_path ( file_path ) rv . append ( normalized_path ) return rv build_all_components ( self , files ) For each component specification file, run az ml component build , and register the status (+ register error if build failed). Returns the list of \"built\" component files. Source code in shrike/build/commands/prepare.py def build_all_components ( self , files : List [ str ]) -> List [ str ]: \"\"\" For each component specification file, run `az ml component build`, and register the status (+ register error if build failed). Returns the list of \"built\" component files. \"\"\" rv = [] for component in files : path = Path ( component ) rv . append ( str ( path . parent / \".build\" / path . name )) build_component_success = self . execute_azure_cli_command ( f \"ml component build --file { component } \" ) if build_component_success : log . info ( f \"Component { component } is built.\" ) else : self . register_error ( f \"Error when building component { component } .\" ) return rv compliance_validation ( self , component ) This function checks whether a given component spec YAML file meets all the requirements for running in the compliant AML. Specifically, it checks (1) whether the image URL is compliant\uff1b \uff082\uff09whether the pip index-url is compliant; (3) whether \"default\" is only Conda channel Source code in shrike/build/commands/prepare.py def compliance_validation ( self , component : str ) -> bool : \"\"\" This function checks whether a given component spec YAML file meets all the requirements for running in the compliant AML. Specifically, it checks (1) whether the image URL is compliant\uff1b \uff082\uff09whether the pip index-url is compliant; (3) whether \"default\" is only Conda channel \"\"\" with open ( component , \"r\" ) as spec_file : spec = YAML ( typ = \"safe\" ) . load ( spec_file ) # Check whether the docker image URL is compliant image_url = jsonpath_ng . parse ( \"$.environment.docker.image\" ) . find ( spec ) if len ( image_url ) > 0 : if ( urlparse ( image_url [ 0 ] . value ) . path . split ( \"/\" )[ 0 ] not in ALLOWED_CONTAINER_REGISTRIES ): log . error ( f \"The container base image in { component } is not allowed for compliant run.\" ) return False # check whether the package feed is compliant package_dependencies , conda_channels = self . _extract_dependencies_and_channels ( component = component ) if len ( package_dependencies ) > 0 : for dependency in package_dependencies : if re . match ( \"^--index-url\" , dependency ) or re . match ( \"^--extra-index-url\" , dependency ): if dependency . split ( \" \" )[ 1 ] not in ALLOWED_PACKAGE_FEEDS : log . error ( f \"The package feed in { component } is not allowed for compliant run.\" ) return False if ( f \"--index-url { ALLOWED_PACKAGE_FEEDS [ 0 ] } \" not in package_dependencies and f \"--extra-index-url { ALLOWED_PACKAGE_FEEDS [ 0 ] } \" not in package_dependencies ): log . error ( f \"The Polymer package feed is not found in environment of { component } \" ) return False # Check whether \"default\" is only Conda channel if len ( conda_channels ) > 1 or ( len ( conda_channels ) == 1 and conda_channels [ 0 ] != \".\" ): log . error ( \"Only the default conda channel is allowed for compliant run.\" ) return False return True component_is_active ( self , component , modified_files ) This function returns True if any of the 'modified_files' potentially affects the 'component' (i.e. if it is directly in one of the 'component' subfolders, or if it is covered by the additional_includes files). If the component has been deleted, returns False. Source code in shrike/build/commands/prepare.py def component_is_active ( self , component , modified_files ) -> bool : \"\"\" This function returns True if any of the 'modified_files' potentially affects the 'component' (i.e. if it is directly in one of the 'component' subfolders, or if it is covered by the additional_includes files). If the component has been deleted, returns False. \"\"\" log . info ( \"Assessing whether component '\" + component + \"' is active...\" ) # Let's first take care of the case where the component has been deleted if not ( Path ( component ) . exists ()): return False # Let's grab the contents of the additional_includes file if it exists. # First, we figure out the name of the additional_includes file, based on the component name component_name_without_extension = Path ( component ) . name . split ( \".yaml\" )[ 0 ] # Then, we construct the path of the additional_includes file component_additional_includes_path = os . path . join ( Path ( component ) . parent , component_name_without_extension + \".additional_includes\" , ) # And we finally load it if Path ( component_additional_includes_path ) . exists (): with open ( component_additional_includes_path , \"r\" ) as component_additional_includes : component_additional_includes_contents = ( component_additional_includes . readlines () ) else : component_additional_includes_contents = None # make the paths in the additional_includes file absolute if not ( component_additional_includes_contents is None ): for line_number in range ( 0 , len ( component_additional_includes_contents )): component_additional_includes_contents [ line_number ] = str ( Path ( os . path . join ( Path ( component ) . parent , component_additional_includes_contents [ line_number ] . rstrip ( \" \\n \" ), ) ) . resolve () ) # loop over all modified files; if current file is in subfolder of component or covered by additional includes, return True for modified_file in modified_files : if self . is_in_subfolder ( modified_file , component ) or self . is_in_additional_includes ( modified_file , component_additional_includes_contents ): return True return False create_catalog_files ( self , files ) Create the appropriate kind of catalog file(s), using the configured method (\"aml\" or \"aether\"). Source code in shrike/build/commands/prepare.py def create_catalog_files ( self , files : List [ str ]): \"\"\" Create the appropriate kind of catalog file(s), using the configured method (\"aml\" or \"aether\"). \"\"\" signing_mode = self . config . signing_mode if signing_mode == \"aml\" : self . create_catalog_files_for_aml ( files ) elif signing_mode == \"aether\" : self . create_catalog_files_for_aether ( files ) else : raise ValueError ( f \"Invalid signing_mode provided: ' { signing_mode } '\" ) create_catalog_files_for_aether ( self , files ) Create Aether-friendly .cat files, by first creating a CDF file, then finding and running makecat.exe to create the catalog file. Source code in shrike/build/commands/prepare.py def create_catalog_files_for_aether ( self , files : List [ str ]) -> None : \"\"\" Create Aether-friendly .cat files, by first creating a CDF file, then finding and running `makecat.exe` to create the catalog file. \"\"\" makecat_default = self . config . makecat_default makecat_directory = self . config . makecat_directory makecat = os . path . join ( makecat_directory , makecat_default ) if not os . path . exists ( makecat ): log . info ( f \"Default makecat location { makecat } does not exist\" ) for path in Path ( makecat_directory ) . rglob ( \"makecat.exe\" ): if \"x64\" in str ( path ) . lower (): makecat = path break log . info ( f \"Makecat location: { makecat } \" ) for file in files : directory = os . path . dirname ( file ) name = os . path . split ( directory )[ - 1 ] cat_name = f \" { name } .cat\" cdf_name = f \" { name } .cdf\" path_to_cdf = os . path . join ( directory , cdf_name ) cdf_contents = f \"\"\"[CatalogHeader] Name= { cat_name } PublicVersion=0x0000001 EncodingType=0x00010001 PageHashes=true CATATTR1=0x00010001:OSAttr:2:6.2 [CatalogFiles] \"\"\" files_in_module = self . all_files_in_snapshot ( file ) hash_lines = map ( lambda p : f \"<HASH> { p } = { p } \" , files_in_module ) all_hashes = \" \\n \" . join ( hash_lines ) cdf_contents += all_hashes log . info ( f \"CDF file contents: \\n { cdf_contents } \" ) with open ( path_to_cdf , \"w\" , encoding = \"ascii\" ) as output : output . write ( cdf_contents ) success = self . execute_command ([ str ( makecat ), path_to_cdf , \"-v\" ]) if success : log . info ( f \"Creating Aether catalog files for { name } is successful.\" ) shutil . move ( cat_name , directory ) else : self . register_error ( f \"Error when creating Aether catalog files for { name } .\" ) log . info ( f \"Removing { cdf_name } \" ) os . remove ( path_to_cdf ) log . info ( f \"Finish creating aether catalog files for { name } .\" ) create_catalog_files_for_aml ( self , files ) Create AML-friendly catalog.json and catalog.json.sig files, using SHA-256 hash. Source code in shrike/build/commands/prepare.py def create_catalog_files_for_aml ( self , files : List [ str ]) -> None : \"\"\" Create AML-friendly catalog.json and catalog.json.sig files, using SHA-256 hash. \"\"\" # For each component spec file in the input list, we'll do the following... for f in files : log . info ( f \"Processing file { f } \" ) component_folder_path = self . folder_path ( f ) # remove catalog files if already present log . info ( \"Deleting old catalog files if present\" ) delete_two_catalog_files ( component_folder_path ) files_for_catalog = self . all_files_in_snapshot ( f ) log . info ( \"The following list of files will be added to the catalog.\" ) log . info ( files_for_catalog ) # Prepare the catlog stub: {'HashAlgorithm': 'SHA256', 'CatalogItems': {}} catalog = create_catalog_stub () # Add an entry to the catalog for each file for file_for_catalog in files_for_catalog : catalog = add_file_to_catalog ( file_for_catalog , catalog , component_folder_path ) # order the CatalogItems dictionary catalog [ \"CatalogItems\" ] = collections . OrderedDict ( sorted ( catalog [ \"CatalogItems\" ] . items ()) ) # Write the 2 catalog files log . info ( catalog ) write_two_catalog_files ( catalog , component_folder_path ) log . info ( \"Finished creating catalog files.\" ) customized_validation ( jsonpath , regex , component ) staticmethod This function leverages regular expressionm atching and JSONPath expression to enforce user-provided \"strict\" validation on Azure ML components Source code in shrike/build/commands/prepare.py @staticmethod def customized_validation ( jsonpath : str , regex : str , component : str ) -> bool : \"\"\" This function leverages regular expressionm atching and JSONPath expression to enforce user-provided \"strict\" validation on Azure ML components \"\"\" with open ( component , \"r\" ) as spec_file : spec = YAML ( typ = \"safe\" ) . load ( spec_file ) parsed_patterns = jsonpath_ng . parse ( jsonpath ) . find ( spec ) validation_success = True if len ( parsed_patterns ) > 0 : for parsed_pattern in parsed_patterns : if not re . match ( regex , parsed_pattern . value ): log . error ( f \"The parsed pattern { parsed_pattern } in { component } doesn't match the regular expression { regex } \" ) validation_success = False return validation_success find_component_specification_files ( self ) Find the list of \"active\" component specification files using the configured method (\"all\" or \"smart\"). Source code in shrike/build/commands/prepare.py def find_component_specification_files ( self ) -> List [ str ]: \"\"\" Find the list of \"active\" component specification files using the configured method (\"all\" or \"smart\"). \"\"\" activation_method = self . config . activation_method if activation_method == \"all\" : rv = self . find_component_specification_files_using_all () elif activation_method == \"smart\" : rv = self . find_component_specification_files_using_smart () else : raise ValueError ( f \"Invalid activation_method provided: ' { activation_method } '\" ) return rv find_component_specification_files_using_all ( self , dir = None ) Find all component specification files in the configured working directory matching the configured glob. Return the absolute paths of these files in the format of a list of string. Source code in shrike/build/commands/prepare.py def find_component_specification_files_using_all ( self , dir = None ) -> List [ str ]: \"\"\" Find all component specification files in the configured working directory matching the configured glob. Return the absolute paths of these files in the format of a list of string. \"\"\" if dir is None : dir = self . config . working_directory all_spec_yaml_files_absolute_paths = [ str ( p . absolute ()) for p in Path ( dir ) . glob ( self . config . component_specification_glob ) ] return all_spec_yaml_files_absolute_paths find_component_specification_files_using_smart ( self ) This function returns the list of components (as a list of absolute paths) potentially affected by the latest commit. Source code in shrike/build/commands/prepare.py def find_component_specification_files_using_smart ( self ) -> List [ str ]: \"\"\" This function returns the list of components (as a list of absolute paths) potentially affected by the latest commit. \"\"\" log . info ( \"Determining which components are potentially affected by the current change.\" ) [ repo , current_branch , compliant_branch ] = self . identify_repo_and_branches () modified_files = self . get_modified_files ( repo , current_branch , compliant_branch ) active_components = self . infer_active_components_from_modified_files ( modified_files ) return active_components folder_path ( self , file ) Return the normalized path of the directory containing a file. Source code in shrike/build/commands/prepare.py def folder_path ( self , file : str ) -> str : \"\"\" Return the normalized path of the directory containing a file. \"\"\" return self . normalize_path ( Path ( file ) . parent , directory = True ) get_compliant_commit_corresponding_to_pull_request ( self , repo , compliant_branch ) This function will return the most recent commit in the repo that truly corresponds to the triggered build. It is identified thanks to the 'Build.SourceVersionMessage' DevOps environment variable (see https://docs.microsoft.com/en-us/azure/devops/pipelines/build/variables?view=azure-devops&tabs=yaml) that contains the true commit message. This is used to address the race condition occuring when a commit sneaks in before the \"prepare\" step was run on the previous commit. Source code in shrike/build/commands/prepare.py def get_compliant_commit_corresponding_to_pull_request ( self , repo , compliant_branch ): \"\"\" This function will return the most recent commit in the repo that truly corresponds to the triggered build. It is identified thanks to the 'Build.SourceVersionMessage' DevOps environment variable (see https://docs.microsoft.com/en-us/azure/devops/pipelines/build/variables?view=azure-devops&tabs=yaml) that contains the true commit message. This is used to address the race condition occuring when a commit sneaks in before the \"prepare\" step was run on the previous commit. \"\"\" # this is the true commit message corresponding to the PR that triggered the build true_commit_message = self . get_true_commit_message () # this is the most recent commit current_commit = repo . remotes . origin . refs [ compliant_branch ] . commit # if the most recent commit corresponds to the true commit message, then return it if ( true_commit_message . startswith ( current_commit . summary )): return current_commit # otherwise, let's iterate through the parents until we find it candidate_commit = current_commit for c in candidate_commit . iter_parents (): if ( true_commit_message . startswith ( c . summary )): return c # if the corresponding commit cannot be found, return the most recent one and log a warning log . warning ( \"Could not find the in the git repo the commit that triggered this PR. Returning the most recent but beware, the 'smart' mode likely will not work properly.\" ) return current_commit get_modified_files ( self , repo , current_branch , compliant_branch ) This function returns the paths of files that have been modified. 3 scenarios are supported. 1/ 'Build - before Merge'; when the 'prepare' command is run as part of a build, but before the actual merge (in this case, the name of the current branch starts with 'refs/pull/' - this is the default Azure DevOps behavior). 2/ 'Build - after Merge'; when the 'prepare' command is run as part of a build, after the actual merge (in this case, the name of the current branch is the same as the name of the compliant branch). 3/ 'Manual'; when the prepare command is run manually (typically before publishing the PR). Source code in shrike/build/commands/prepare.py def get_modified_files ( self , repo , current_branch , compliant_branch ) -> Set [ str ]: \"\"\" This function returns the paths of files that have been modified. 3 scenarios are supported.\\n 1/ 'Build - before Merge'; when the 'prepare' command is run as part of a build, but before the actual merge (in this case, the name of the current branch starts with 'refs/pull/' - this is the default Azure DevOps behavior).\\n 2/ 'Build - after Merge'; when the 'prepare' command is run as part of a build, after the actual merge (in this case, the name of the current branch is the same as the name of the compliant branch).\\n 3/ 'Manual'; when the prepare command is run manually (typically before publishing the PR). \"\"\" res = set () # Grab the diff differently depending on the scenario if current_branch . replace ( \"refs/heads/\" , \"\" ) == compliant_branch : # 'Build - after Merge' case: we will take the diff between the # tree of the latest commit to the compliant branch, and the tree # of the previous commit to the compliant branch corresponding to a # PR (we assume the commit summary starts with 'Merged PR') log . info ( \"We are in the 'Build - after Merge' case (the current branch is the compliant branch).\" ) current_commit = self . get_compliant_commit_corresponding_to_pull_request ( repo , compliant_branch ) self . log_commit_info ( current_commit , \"Current commit to compliant branch\" ) previous_commit = ( self . get_previous_compliant_commit_corresponding_to_pull_request ( current_commit , consider_current_commit = False , ) ) self . log_commit_info ( previous_commit , \"Previous PR commit to compliant branch\" ) elif current_branch . startswith ( \"refs/pull/\" ): # 'Build - before Merge': we will take the diff between the tree of # the current commit, and the tree of the previous commit to the # compliant branch corresponding to a PR (we assume the commit # summary starts with 'Merged PR') log . info ( \"We are in the 'Build - before Merge' case (the current branch is not the compliant branch and its name starts with 'refs/pull/').\" ) current_commit = repo . commit () self . log_commit_info ( current_commit , \"Current commit to current branch\" ) latest_commit_to_compliant_branch = repo . remotes . origin . refs [ compliant_branch ] . commit previous_commit = ( self . get_previous_compliant_commit_corresponding_to_pull_request ( latest_commit_to_compliant_branch , consider_current_commit = True , ) ) self . log_commit_info ( previous_commit , \"Previous PR commit to compliant branch\" ) else : # 'Manual' Case: we will take the diff between the current branch # and the compliant branch (we're assuming the compliant branch is # locally up to date here) log . info ( \"We are in the 'Manual' case (the current branch is NOT the compliant branch and its name does not start with 'refs/pull/').\" ) try : current_commit = repo . heads [ current_branch ] . commit # this won't work when running the Manual case from the DevOps portal, but the below will except ( IndexError , AttributeError ): current_commit = repo . commit () self . log_commit_info ( current_commit , \"Current commit to current branch\" ) try : previous_commit = repo . heads [ compliant_branch ] . commit # this won't work when running the Manual case from the DevOps portal, but the below will except ( IndexError , AttributeError ): latest_commit_to_compliant_branch = repo . remotes . origin . refs [ compliant_branch ] . commit previous_commit = ( self . get_previous_compliant_commit_corresponding_to_pull_request ( latest_commit_to_compliant_branch , consider_current_commit = True , ) ) self . log_commit_info ( previous_commit , \"Previous commit to compliant branch\" ) # take the actual diff diff = current_commit . tree . diff ( previous_commit . tree ) # let's build a set with the paths of modified files found in the diff object log . debug ( \"Working directory: \" + self . config . working_directory ) log . debug ( \"repo.working_dir: \" + repo . working_dir ) log . debug ( \"repo.working_tree_dir: \" + repo . working_tree_dir ) log . debug ( \"repo.git_dir: \" + repo . git_dir ) for d in diff : log . debug ( \"d.a_path: \" + d . a_path ) log . debug ( \"Path(d.a_path).absolute(): \" + str ( Path ( d . a_path ) . absolute ())) log . debug ( \"Path(d.a_path).resolve(): \" + str ( Path ( d . a_path ) . resolve ())) r_a = str ( Path ( repo . git_dir ) . parent / Path ( d . a_path )) res . add ( r_a ) r_b = str ( Path ( repo . git_dir ) . parent / Path ( d . b_path )) res . add ( r_b ) log . info ( \"The list of modified files is:\" ) log . info ( res ) return res get_previous_compliant_commit_corresponding_to_pull_request ( self , latest_commit , consider_current_commit ) This function will return the previous commit in the repo 's compliant_branch_name corresponding to a PR (i.e. that starts with \"Merged PR\"). If consider_current_commit is set to True, the latest_commit will be considered. If set to false, only previous commits will be considered. Source code in shrike/build/commands/prepare.py def get_previous_compliant_commit_corresponding_to_pull_request ( self , latest_commit , consider_current_commit ): \"\"\" This function will return the previous commit in the `repo`'s `compliant_branch_name` corresponding to a PR (i.e. that starts with \"Merged PR\"). If `consider_current_commit` is set to True, the `latest_commit` will be considered. If set to false, only previous commits will be considered. \"\"\" target_string = \"Merged PR\" if consider_current_commit and latest_commit . summary . startswith ( target_string ): return latest_commit previous_commit = latest_commit for c in previous_commit . iter_parents (): if c . summary . startswith ( target_string ): previous_commit = c break return previous_commit identify_repo_and_branches ( self ) This function returns the current repository, along with the name of the current and compliant branches [repo, current_branch, compliant_branch]. Throws if no repo can be found. Source code in shrike/build/commands/prepare.py def identify_repo_and_branches ( self ): \"\"\" This function returns the current repository, along with the name of the current and compliant branches [repo, current_branch, compliant_branch]. Throws if no repo can be found. \"\"\" # identify the repository curr_path = Path ( self . config . working_directory ) . resolve () try : repo = Repo ( curr_path , search_parent_directories = True ) log . info ( \"Found a valid repository in \" + repo . git_dir ) except ( InvalidGitRepositoryError , NoSuchPathError ): message = ( str ( curr_path ) + \" or its parents do not contain a valid repo path or cannot be accessed.\" ) raise Exception ( message ) try : current_branch = str ( repo . head . ref ) # when running from our build the repo head is detached so this will throw an exception except TypeError : current_branch = os . environ . get ( \"BUILD_SOURCEBRANCH\" ) or os . environ . get ( \"GITHUB_REF\" ) log . info ( \"The current branch is: '\" + str ( current_branch ) + \"'.\" ) # Identify the compliant branch if not ( self . config . compliant_branch . startswith ( \"^refs/heads/\" )) or not ( self . config . compliant_branch . endswith ( \"$\" ) ): raise Exception ( \"The name of the compliant branch found in the config file should start with '^refs/heads/' and end with '$'. Currently it is: '\" + self . config . compliant_branch + \"'.\" ) else : compliant_branch = self . config . compliant_branch . replace ( \"^refs/heads/\" , \"\" )[ 0 : - 1 ] log . info ( \"The compliant branch is: '\" + compliant_branch + \"'.\" ) return [ repo , current_branch , compliant_branch ] infer_active_components_from_modified_files ( self , modified_files ) This function returns the list of components (as a list of directories paths) potentially affected by changes in the modified_files . Source code in shrike/build/commands/prepare.py def infer_active_components_from_modified_files ( self , modified_files ) -> List [ str ]: \"\"\" This function returns the list of components (as a list of directories paths) potentially affected by changes in the `modified_files`. \"\"\" rv = [] # We will go over components one by one all_components_in_repo = self . find_component_specification_files_using_all () log . info ( \"List of all components in repo:\" ) log . info ( all_components_in_repo ) for component in all_components_in_repo : if self . component_is_active ( component , modified_files ): rv . append ( component ) # No need to dedup rv since we are only considering components once log . info ( \"The active components are:\" ) log . info ( rv ) return rv is_in_additional_includes ( self , modified_file , component_additional_includes_contents ) This function returns True if 'modified_file' is covered by the additional_includes file 'component_additional_includes_contents'. Source code in shrike/build/commands/prepare.py def is_in_additional_includes ( self , modified_file , component_additional_includes_contents ) -> bool : \"\"\" This function returns True if 'modified_file' is covered by the additional_includes file 'component_additional_includes_contents'. \"\"\" # first tackle the trivial case of no additional_includes file if component_additional_includes_contents is None : log . debug ( \"The component's additional_includes file is empty, returning False.\" ) return False # now the regular scenario for line in component_additional_includes_contents : # when the line from additional_includes is a file, we directly chech its path against that of modified_file if Path ( line ) . is_file (): if str ( Path ( modified_file ) . resolve ()) == str ( Path ( line ) . resolve () ): # can't use 'samefile' here because modified_file is not guaranteed to exist, we resolve the path and do basic == test log . info ( \"'\" + modified_file + \" is directly listed in the additional_includes file.\" ) return True # slightly more complicated case: when the line in additional_includes is a directory, we can just call the is_in_subfolder function if Path ( line ) . is_dir (): if self . is_in_subfolder ( modified_file , line ): log . info ( \"'\" + modified_file + \" is in one of the directories listed in the additional_includes file.\" ) return True log . debug ( \"'\" + modified_file + \" is NOT referenced by the additional_includes file (neither directly nor indirectly).\" ) return False is_in_subfolder ( self , modified_file , component ) This function returns True if 'modified_file' is in a subfolder of 'component' ('component' can be either the path to a file, or a directory). If the component has been deleted, returns False. Source code in shrike/build/commands/prepare.py def is_in_subfolder ( self , modified_file , component ) -> bool : \"\"\" This function returns True if 'modified_file' is in a subfolder of 'component' ('component' can be either the path to a file, or a directory). If the component has been deleted, returns False. \"\"\" # Let's first take care of the case where the component has been deleted if not ( Path ( component ) . exists ()): log . debug ( \"'\" + component + \"' does not exist, returning False.\" ) return False # Case where the component has not been deleted for parent in Path ( modified_file ) . parents : if parent . exists (): if Path ( component ) . is_dir (): if parent . samefile ( Path ( component )): log . info ( \"'\" + modified_file + \" is in a subfolder of '\" + component + \"'.\" ) return True else : if parent . samefile ( Path ( component ) . parent ): log . info ( \"'\" + modified_file + \" is in a subfolder of '\" + component + \"'.\" ) return True log . debug ( \"'\" + modified_file + \" is NOT in a subfolder of '\" + component + \"'.\" ) return False run_with_config ( self ) Run the subclasses command with the specified configuration object. Before this method is invoked, there is no guarantee that self.config will be populated; after it is invoked, that is guaranteed. Implementations of this method should NOT mutate the logging tree in any way. They should also NOT raise any exceptions; rather they should call the register_error method, which will ensure non-zero exit code. Implementations can raise specific \"status information\" (e.g., a component is not \"active\") by calling register_component_status . Source code in shrike/build/commands/prepare.py def run_with_config ( self ): log . info ( \"Running component preparation logic.\" ) self . telemetry_logging ( command = \"prepare\" ) component_files = self . find_component_specification_files () if not self . config . suppress_adding_repo_pr_tags : try : component_files = self . add_repo_and_last_pr_to_tags ( component_files ) except StopIteration : log . warning ( \"`add_repo_and_last_pr_to_tags` not successful. Please make sure your component files are in Git. Otherwise, please set `suppress_adding_repo_pr_tags` to True.\" ) if self . config . signing_mode == \"aml\" : self . ensure_component_cli_installed () self . attach_workspace () self . validate_all_components ( component_files ) built_component_files = self . build_all_components ( component_files ) else : built_component_files = component_files self . create_catalog_files ( built_component_files ) self . _create_requirements_files ( component_files ) validate_all_components ( self , files ) For each component specification file, run az ml component validate , run compliance and customized validation if enabled, and register the status (+ register error if validation failed). Source code in shrike/build/commands/prepare.py def validate_all_components ( self , files : List [ str ]) -> None : \"\"\" For each component specification file, run `az ml component validate`, run compliance and customized validation if enabled, and register the status (+ register error if validation failed). \"\"\" for component in files : validate_component_success = self . execute_azure_cli_command ( f \"ml component validate --file { component } \" ) compliance_validation_success = True customized_validation_success = True if self . config . enable_component_validation : log . info ( f \"Running compliance validation on { component } \" ) compliance_validation_success = self . compliance_validation ( component ) if len ( self . config . component_validation ) > 0 : log . info ( f \"Running customized validation on { component } \" ) for jsonpath , regex in self . config . component_validation . items (): customized_validation_success = ( customized_validation_success if self . customized_validation ( jsonpath , regex , component ) else False ) if ( validate_component_success and compliance_validation_success and customized_validation_success ): # If the az ml validation succeeds, we continue to check whether # the \"code\" snapshot parameter is specified in the spec file # https://componentsdk.z22.web.core.windows.net/components/component-spec-topics/code-snapshot.html with open ( component , \"r\" ) as spec_file : spec = YAML ( typ = \"safe\" ) . load ( spec_file ) spec_code = spec . get ( \"code\" ) if spec_code and spec_code not in [ \".\" , \"./\" ]: self . register_component_status ( component , \"validate\" , \"failed\" ) self . register_error ( \"Code snapshot parameter is not supported. Please use .additional_includes for your component.\" ) else : log . info ( f \"Component { component } is valid.\" ) self . register_component_status ( component , \"validate\" , \"succeeded\" ) else : self . register_component_status ( component , \"validate\" , \"failed\" ) self . register_error ( f \"Error when validating component { component } .\" )","title":"prepare"},{"location":"build/prepare/#prepare","text":"","title":"Prepare"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare","text":"","title":"Prepare"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.all_files_in_snapshot","text":"Return a list of all normalized files in the snapshot. The input ( manifest ) is assumed to be some file, whether AML-style component spec or Aether-style auto-approval manifest, in the \"root\" of the snapshot. Source code in shrike/build/commands/prepare.py def all_files_in_snapshot ( self , manifest : str ) -> List [ str ]: \"\"\" Return a list of all normalized files in the snapshot. The input (`manifest`) is assumed to be some file, whether AML-style component spec or Aether-style auto-approval manifest, in the \"root\" of the snapshot. \"\"\" folder_path = self . folder_path ( manifest ) log . info ( \"Absolute path for current component is: \" + folder_path ) # Generate a list of all files in this components folder (including subdirectories) rv = [] # Make sure we pick up Linux-style \"hidden\" files like .amlignore and # hidden \"directories\", as well as hidden files in hidden directories. # https://stackoverflow.com/a/65205404 # https://stackoverflow.com/a/41447012 for root , _ , file_paths in os . walk ( folder_path ): for file in file_paths : file_path = os . path . join ( root , file ) normalized_path = self . normalize_path ( file_path ) rv . append ( normalized_path ) return rv","title":"all_files_in_snapshot()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.build_all_components","text":"For each component specification file, run az ml component build , and register the status (+ register error if build failed). Returns the list of \"built\" component files. Source code in shrike/build/commands/prepare.py def build_all_components ( self , files : List [ str ]) -> List [ str ]: \"\"\" For each component specification file, run `az ml component build`, and register the status (+ register error if build failed). Returns the list of \"built\" component files. \"\"\" rv = [] for component in files : path = Path ( component ) rv . append ( str ( path . parent / \".build\" / path . name )) build_component_success = self . execute_azure_cli_command ( f \"ml component build --file { component } \" ) if build_component_success : log . info ( f \"Component { component } is built.\" ) else : self . register_error ( f \"Error when building component { component } .\" ) return rv","title":"build_all_components()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.compliance_validation","text":"This function checks whether a given component spec YAML file meets all the requirements for running in the compliant AML. Specifically, it checks (1) whether the image URL is compliant\uff1b \uff082\uff09whether the pip index-url is compliant; (3) whether \"default\" is only Conda channel Source code in shrike/build/commands/prepare.py def compliance_validation ( self , component : str ) -> bool : \"\"\" This function checks whether a given component spec YAML file meets all the requirements for running in the compliant AML. Specifically, it checks (1) whether the image URL is compliant\uff1b \uff082\uff09whether the pip index-url is compliant; (3) whether \"default\" is only Conda channel \"\"\" with open ( component , \"r\" ) as spec_file : spec = YAML ( typ = \"safe\" ) . load ( spec_file ) # Check whether the docker image URL is compliant image_url = jsonpath_ng . parse ( \"$.environment.docker.image\" ) . find ( spec ) if len ( image_url ) > 0 : if ( urlparse ( image_url [ 0 ] . value ) . path . split ( \"/\" )[ 0 ] not in ALLOWED_CONTAINER_REGISTRIES ): log . error ( f \"The container base image in { component } is not allowed for compliant run.\" ) return False # check whether the package feed is compliant package_dependencies , conda_channels = self . _extract_dependencies_and_channels ( component = component ) if len ( package_dependencies ) > 0 : for dependency in package_dependencies : if re . match ( \"^--index-url\" , dependency ) or re . match ( \"^--extra-index-url\" , dependency ): if dependency . split ( \" \" )[ 1 ] not in ALLOWED_PACKAGE_FEEDS : log . error ( f \"The package feed in { component } is not allowed for compliant run.\" ) return False if ( f \"--index-url { ALLOWED_PACKAGE_FEEDS [ 0 ] } \" not in package_dependencies and f \"--extra-index-url { ALLOWED_PACKAGE_FEEDS [ 0 ] } \" not in package_dependencies ): log . error ( f \"The Polymer package feed is not found in environment of { component } \" ) return False # Check whether \"default\" is only Conda channel if len ( conda_channels ) > 1 or ( len ( conda_channels ) == 1 and conda_channels [ 0 ] != \".\" ): log . error ( \"Only the default conda channel is allowed for compliant run.\" ) return False return True","title":"compliance_validation()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.component_is_active","text":"This function returns True if any of the 'modified_files' potentially affects the 'component' (i.e. if it is directly in one of the 'component' subfolders, or if it is covered by the additional_includes files). If the component has been deleted, returns False. Source code in shrike/build/commands/prepare.py def component_is_active ( self , component , modified_files ) -> bool : \"\"\" This function returns True if any of the 'modified_files' potentially affects the 'component' (i.e. if it is directly in one of the 'component' subfolders, or if it is covered by the additional_includes files). If the component has been deleted, returns False. \"\"\" log . info ( \"Assessing whether component '\" + component + \"' is active...\" ) # Let's first take care of the case where the component has been deleted if not ( Path ( component ) . exists ()): return False # Let's grab the contents of the additional_includes file if it exists. # First, we figure out the name of the additional_includes file, based on the component name component_name_without_extension = Path ( component ) . name . split ( \".yaml\" )[ 0 ] # Then, we construct the path of the additional_includes file component_additional_includes_path = os . path . join ( Path ( component ) . parent , component_name_without_extension + \".additional_includes\" , ) # And we finally load it if Path ( component_additional_includes_path ) . exists (): with open ( component_additional_includes_path , \"r\" ) as component_additional_includes : component_additional_includes_contents = ( component_additional_includes . readlines () ) else : component_additional_includes_contents = None # make the paths in the additional_includes file absolute if not ( component_additional_includes_contents is None ): for line_number in range ( 0 , len ( component_additional_includes_contents )): component_additional_includes_contents [ line_number ] = str ( Path ( os . path . join ( Path ( component ) . parent , component_additional_includes_contents [ line_number ] . rstrip ( \" \\n \" ), ) ) . resolve () ) # loop over all modified files; if current file is in subfolder of component or covered by additional includes, return True for modified_file in modified_files : if self . is_in_subfolder ( modified_file , component ) or self . is_in_additional_includes ( modified_file , component_additional_includes_contents ): return True return False","title":"component_is_active()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.create_catalog_files","text":"Create the appropriate kind of catalog file(s), using the configured method (\"aml\" or \"aether\"). Source code in shrike/build/commands/prepare.py def create_catalog_files ( self , files : List [ str ]): \"\"\" Create the appropriate kind of catalog file(s), using the configured method (\"aml\" or \"aether\"). \"\"\" signing_mode = self . config . signing_mode if signing_mode == \"aml\" : self . create_catalog_files_for_aml ( files ) elif signing_mode == \"aether\" : self . create_catalog_files_for_aether ( files ) else : raise ValueError ( f \"Invalid signing_mode provided: ' { signing_mode } '\" )","title":"create_catalog_files()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.create_catalog_files_for_aether","text":"Create Aether-friendly .cat files, by first creating a CDF file, then finding and running makecat.exe to create the catalog file. Source code in shrike/build/commands/prepare.py def create_catalog_files_for_aether ( self , files : List [ str ]) -> None : \"\"\" Create Aether-friendly .cat files, by first creating a CDF file, then finding and running `makecat.exe` to create the catalog file. \"\"\" makecat_default = self . config . makecat_default makecat_directory = self . config . makecat_directory makecat = os . path . join ( makecat_directory , makecat_default ) if not os . path . exists ( makecat ): log . info ( f \"Default makecat location { makecat } does not exist\" ) for path in Path ( makecat_directory ) . rglob ( \"makecat.exe\" ): if \"x64\" in str ( path ) . lower (): makecat = path break log . info ( f \"Makecat location: { makecat } \" ) for file in files : directory = os . path . dirname ( file ) name = os . path . split ( directory )[ - 1 ] cat_name = f \" { name } .cat\" cdf_name = f \" { name } .cdf\" path_to_cdf = os . path . join ( directory , cdf_name ) cdf_contents = f \"\"\"[CatalogHeader] Name= { cat_name } PublicVersion=0x0000001 EncodingType=0x00010001 PageHashes=true CATATTR1=0x00010001:OSAttr:2:6.2 [CatalogFiles] \"\"\" files_in_module = self . all_files_in_snapshot ( file ) hash_lines = map ( lambda p : f \"<HASH> { p } = { p } \" , files_in_module ) all_hashes = \" \\n \" . join ( hash_lines ) cdf_contents += all_hashes log . info ( f \"CDF file contents: \\n { cdf_contents } \" ) with open ( path_to_cdf , \"w\" , encoding = \"ascii\" ) as output : output . write ( cdf_contents ) success = self . execute_command ([ str ( makecat ), path_to_cdf , \"-v\" ]) if success : log . info ( f \"Creating Aether catalog files for { name } is successful.\" ) shutil . move ( cat_name , directory ) else : self . register_error ( f \"Error when creating Aether catalog files for { name } .\" ) log . info ( f \"Removing { cdf_name } \" ) os . remove ( path_to_cdf ) log . info ( f \"Finish creating aether catalog files for { name } .\" )","title":"create_catalog_files_for_aether()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.create_catalog_files_for_aml","text":"Create AML-friendly catalog.json and catalog.json.sig files, using SHA-256 hash. Source code in shrike/build/commands/prepare.py def create_catalog_files_for_aml ( self , files : List [ str ]) -> None : \"\"\" Create AML-friendly catalog.json and catalog.json.sig files, using SHA-256 hash. \"\"\" # For each component spec file in the input list, we'll do the following... for f in files : log . info ( f \"Processing file { f } \" ) component_folder_path = self . folder_path ( f ) # remove catalog files if already present log . info ( \"Deleting old catalog files if present\" ) delete_two_catalog_files ( component_folder_path ) files_for_catalog = self . all_files_in_snapshot ( f ) log . info ( \"The following list of files will be added to the catalog.\" ) log . info ( files_for_catalog ) # Prepare the catlog stub: {'HashAlgorithm': 'SHA256', 'CatalogItems': {}} catalog = create_catalog_stub () # Add an entry to the catalog for each file for file_for_catalog in files_for_catalog : catalog = add_file_to_catalog ( file_for_catalog , catalog , component_folder_path ) # order the CatalogItems dictionary catalog [ \"CatalogItems\" ] = collections . OrderedDict ( sorted ( catalog [ \"CatalogItems\" ] . items ()) ) # Write the 2 catalog files log . info ( catalog ) write_two_catalog_files ( catalog , component_folder_path ) log . info ( \"Finished creating catalog files.\" )","title":"create_catalog_files_for_aml()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.customized_validation","text":"This function leverages regular expressionm atching and JSONPath expression to enforce user-provided \"strict\" validation on Azure ML components Source code in shrike/build/commands/prepare.py @staticmethod def customized_validation ( jsonpath : str , regex : str , component : str ) -> bool : \"\"\" This function leverages regular expressionm atching and JSONPath expression to enforce user-provided \"strict\" validation on Azure ML components \"\"\" with open ( component , \"r\" ) as spec_file : spec = YAML ( typ = \"safe\" ) . load ( spec_file ) parsed_patterns = jsonpath_ng . parse ( jsonpath ) . find ( spec ) validation_success = True if len ( parsed_patterns ) > 0 : for parsed_pattern in parsed_patterns : if not re . match ( regex , parsed_pattern . value ): log . error ( f \"The parsed pattern { parsed_pattern } in { component } doesn't match the regular expression { regex } \" ) validation_success = False return validation_success","title":"customized_validation()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.find_component_specification_files","text":"Find the list of \"active\" component specification files using the configured method (\"all\" or \"smart\"). Source code in shrike/build/commands/prepare.py def find_component_specification_files ( self ) -> List [ str ]: \"\"\" Find the list of \"active\" component specification files using the configured method (\"all\" or \"smart\"). \"\"\" activation_method = self . config . activation_method if activation_method == \"all\" : rv = self . find_component_specification_files_using_all () elif activation_method == \"smart\" : rv = self . find_component_specification_files_using_smart () else : raise ValueError ( f \"Invalid activation_method provided: ' { activation_method } '\" ) return rv","title":"find_component_specification_files()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.find_component_specification_files_using_all","text":"Find all component specification files in the configured working directory matching the configured glob. Return the absolute paths of these files in the format of a list of string. Source code in shrike/build/commands/prepare.py def find_component_specification_files_using_all ( self , dir = None ) -> List [ str ]: \"\"\" Find all component specification files in the configured working directory matching the configured glob. Return the absolute paths of these files in the format of a list of string. \"\"\" if dir is None : dir = self . config . working_directory all_spec_yaml_files_absolute_paths = [ str ( p . absolute ()) for p in Path ( dir ) . glob ( self . config . component_specification_glob ) ] return all_spec_yaml_files_absolute_paths","title":"find_component_specification_files_using_all()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.find_component_specification_files_using_smart","text":"This function returns the list of components (as a list of absolute paths) potentially affected by the latest commit. Source code in shrike/build/commands/prepare.py def find_component_specification_files_using_smart ( self ) -> List [ str ]: \"\"\" This function returns the list of components (as a list of absolute paths) potentially affected by the latest commit. \"\"\" log . info ( \"Determining which components are potentially affected by the current change.\" ) [ repo , current_branch , compliant_branch ] = self . identify_repo_and_branches () modified_files = self . get_modified_files ( repo , current_branch , compliant_branch ) active_components = self . infer_active_components_from_modified_files ( modified_files ) return active_components","title":"find_component_specification_files_using_smart()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.folder_path","text":"Return the normalized path of the directory containing a file. Source code in shrike/build/commands/prepare.py def folder_path ( self , file : str ) -> str : \"\"\" Return the normalized path of the directory containing a file. \"\"\" return self . normalize_path ( Path ( file ) . parent , directory = True )","title":"folder_path()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.get_compliant_commit_corresponding_to_pull_request","text":"This function will return the most recent commit in the repo that truly corresponds to the triggered build. It is identified thanks to the 'Build.SourceVersionMessage' DevOps environment variable (see https://docs.microsoft.com/en-us/azure/devops/pipelines/build/variables?view=azure-devops&tabs=yaml) that contains the true commit message. This is used to address the race condition occuring when a commit sneaks in before the \"prepare\" step was run on the previous commit. Source code in shrike/build/commands/prepare.py def get_compliant_commit_corresponding_to_pull_request ( self , repo , compliant_branch ): \"\"\" This function will return the most recent commit in the repo that truly corresponds to the triggered build. It is identified thanks to the 'Build.SourceVersionMessage' DevOps environment variable (see https://docs.microsoft.com/en-us/azure/devops/pipelines/build/variables?view=azure-devops&tabs=yaml) that contains the true commit message. This is used to address the race condition occuring when a commit sneaks in before the \"prepare\" step was run on the previous commit. \"\"\" # this is the true commit message corresponding to the PR that triggered the build true_commit_message = self . get_true_commit_message () # this is the most recent commit current_commit = repo . remotes . origin . refs [ compliant_branch ] . commit # if the most recent commit corresponds to the true commit message, then return it if ( true_commit_message . startswith ( current_commit . summary )): return current_commit # otherwise, let's iterate through the parents until we find it candidate_commit = current_commit for c in candidate_commit . iter_parents (): if ( true_commit_message . startswith ( c . summary )): return c # if the corresponding commit cannot be found, return the most recent one and log a warning log . warning ( \"Could not find the in the git repo the commit that triggered this PR. Returning the most recent but beware, the 'smart' mode likely will not work properly.\" ) return current_commit","title":"get_compliant_commit_corresponding_to_pull_request()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.get_modified_files","text":"This function returns the paths of files that have been modified. 3 scenarios are supported. 1/ 'Build - before Merge'; when the 'prepare' command is run as part of a build, but before the actual merge (in this case, the name of the current branch starts with 'refs/pull/' - this is the default Azure DevOps behavior). 2/ 'Build - after Merge'; when the 'prepare' command is run as part of a build, after the actual merge (in this case, the name of the current branch is the same as the name of the compliant branch). 3/ 'Manual'; when the prepare command is run manually (typically before publishing the PR). Source code in shrike/build/commands/prepare.py def get_modified_files ( self , repo , current_branch , compliant_branch ) -> Set [ str ]: \"\"\" This function returns the paths of files that have been modified. 3 scenarios are supported.\\n 1/ 'Build - before Merge'; when the 'prepare' command is run as part of a build, but before the actual merge (in this case, the name of the current branch starts with 'refs/pull/' - this is the default Azure DevOps behavior).\\n 2/ 'Build - after Merge'; when the 'prepare' command is run as part of a build, after the actual merge (in this case, the name of the current branch is the same as the name of the compliant branch).\\n 3/ 'Manual'; when the prepare command is run manually (typically before publishing the PR). \"\"\" res = set () # Grab the diff differently depending on the scenario if current_branch . replace ( \"refs/heads/\" , \"\" ) == compliant_branch : # 'Build - after Merge' case: we will take the diff between the # tree of the latest commit to the compliant branch, and the tree # of the previous commit to the compliant branch corresponding to a # PR (we assume the commit summary starts with 'Merged PR') log . info ( \"We are in the 'Build - after Merge' case (the current branch is the compliant branch).\" ) current_commit = self . get_compliant_commit_corresponding_to_pull_request ( repo , compliant_branch ) self . log_commit_info ( current_commit , \"Current commit to compliant branch\" ) previous_commit = ( self . get_previous_compliant_commit_corresponding_to_pull_request ( current_commit , consider_current_commit = False , ) ) self . log_commit_info ( previous_commit , \"Previous PR commit to compliant branch\" ) elif current_branch . startswith ( \"refs/pull/\" ): # 'Build - before Merge': we will take the diff between the tree of # the current commit, and the tree of the previous commit to the # compliant branch corresponding to a PR (we assume the commit # summary starts with 'Merged PR') log . info ( \"We are in the 'Build - before Merge' case (the current branch is not the compliant branch and its name starts with 'refs/pull/').\" ) current_commit = repo . commit () self . log_commit_info ( current_commit , \"Current commit to current branch\" ) latest_commit_to_compliant_branch = repo . remotes . origin . refs [ compliant_branch ] . commit previous_commit = ( self . get_previous_compliant_commit_corresponding_to_pull_request ( latest_commit_to_compliant_branch , consider_current_commit = True , ) ) self . log_commit_info ( previous_commit , \"Previous PR commit to compliant branch\" ) else : # 'Manual' Case: we will take the diff between the current branch # and the compliant branch (we're assuming the compliant branch is # locally up to date here) log . info ( \"We are in the 'Manual' case (the current branch is NOT the compliant branch and its name does not start with 'refs/pull/').\" ) try : current_commit = repo . heads [ current_branch ] . commit # this won't work when running the Manual case from the DevOps portal, but the below will except ( IndexError , AttributeError ): current_commit = repo . commit () self . log_commit_info ( current_commit , \"Current commit to current branch\" ) try : previous_commit = repo . heads [ compliant_branch ] . commit # this won't work when running the Manual case from the DevOps portal, but the below will except ( IndexError , AttributeError ): latest_commit_to_compliant_branch = repo . remotes . origin . refs [ compliant_branch ] . commit previous_commit = ( self . get_previous_compliant_commit_corresponding_to_pull_request ( latest_commit_to_compliant_branch , consider_current_commit = True , ) ) self . log_commit_info ( previous_commit , \"Previous commit to compliant branch\" ) # take the actual diff diff = current_commit . tree . diff ( previous_commit . tree ) # let's build a set with the paths of modified files found in the diff object log . debug ( \"Working directory: \" + self . config . working_directory ) log . debug ( \"repo.working_dir: \" + repo . working_dir ) log . debug ( \"repo.working_tree_dir: \" + repo . working_tree_dir ) log . debug ( \"repo.git_dir: \" + repo . git_dir ) for d in diff : log . debug ( \"d.a_path: \" + d . a_path ) log . debug ( \"Path(d.a_path).absolute(): \" + str ( Path ( d . a_path ) . absolute ())) log . debug ( \"Path(d.a_path).resolve(): \" + str ( Path ( d . a_path ) . resolve ())) r_a = str ( Path ( repo . git_dir ) . parent / Path ( d . a_path )) res . add ( r_a ) r_b = str ( Path ( repo . git_dir ) . parent / Path ( d . b_path )) res . add ( r_b ) log . info ( \"The list of modified files is:\" ) log . info ( res ) return res","title":"get_modified_files()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.get_previous_compliant_commit_corresponding_to_pull_request","text":"This function will return the previous commit in the repo 's compliant_branch_name corresponding to a PR (i.e. that starts with \"Merged PR\"). If consider_current_commit is set to True, the latest_commit will be considered. If set to false, only previous commits will be considered. Source code in shrike/build/commands/prepare.py def get_previous_compliant_commit_corresponding_to_pull_request ( self , latest_commit , consider_current_commit ): \"\"\" This function will return the previous commit in the `repo`'s `compliant_branch_name` corresponding to a PR (i.e. that starts with \"Merged PR\"). If `consider_current_commit` is set to True, the `latest_commit` will be considered. If set to false, only previous commits will be considered. \"\"\" target_string = \"Merged PR\" if consider_current_commit and latest_commit . summary . startswith ( target_string ): return latest_commit previous_commit = latest_commit for c in previous_commit . iter_parents (): if c . summary . startswith ( target_string ): previous_commit = c break return previous_commit","title":"get_previous_compliant_commit_corresponding_to_pull_request()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.identify_repo_and_branches","text":"This function returns the current repository, along with the name of the current and compliant branches [repo, current_branch, compliant_branch]. Throws if no repo can be found. Source code in shrike/build/commands/prepare.py def identify_repo_and_branches ( self ): \"\"\" This function returns the current repository, along with the name of the current and compliant branches [repo, current_branch, compliant_branch]. Throws if no repo can be found. \"\"\" # identify the repository curr_path = Path ( self . config . working_directory ) . resolve () try : repo = Repo ( curr_path , search_parent_directories = True ) log . info ( \"Found a valid repository in \" + repo . git_dir ) except ( InvalidGitRepositoryError , NoSuchPathError ): message = ( str ( curr_path ) + \" or its parents do not contain a valid repo path or cannot be accessed.\" ) raise Exception ( message ) try : current_branch = str ( repo . head . ref ) # when running from our build the repo head is detached so this will throw an exception except TypeError : current_branch = os . environ . get ( \"BUILD_SOURCEBRANCH\" ) or os . environ . get ( \"GITHUB_REF\" ) log . info ( \"The current branch is: '\" + str ( current_branch ) + \"'.\" ) # Identify the compliant branch if not ( self . config . compliant_branch . startswith ( \"^refs/heads/\" )) or not ( self . config . compliant_branch . endswith ( \"$\" ) ): raise Exception ( \"The name of the compliant branch found in the config file should start with '^refs/heads/' and end with '$'. Currently it is: '\" + self . config . compliant_branch + \"'.\" ) else : compliant_branch = self . config . compliant_branch . replace ( \"^refs/heads/\" , \"\" )[ 0 : - 1 ] log . info ( \"The compliant branch is: '\" + compliant_branch + \"'.\" ) return [ repo , current_branch , compliant_branch ]","title":"identify_repo_and_branches()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.infer_active_components_from_modified_files","text":"This function returns the list of components (as a list of directories paths) potentially affected by changes in the modified_files . Source code in shrike/build/commands/prepare.py def infer_active_components_from_modified_files ( self , modified_files ) -> List [ str ]: \"\"\" This function returns the list of components (as a list of directories paths) potentially affected by changes in the `modified_files`. \"\"\" rv = [] # We will go over components one by one all_components_in_repo = self . find_component_specification_files_using_all () log . info ( \"List of all components in repo:\" ) log . info ( all_components_in_repo ) for component in all_components_in_repo : if self . component_is_active ( component , modified_files ): rv . append ( component ) # No need to dedup rv since we are only considering components once log . info ( \"The active components are:\" ) log . info ( rv ) return rv","title":"infer_active_components_from_modified_files()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.is_in_additional_includes","text":"This function returns True if 'modified_file' is covered by the additional_includes file 'component_additional_includes_contents'. Source code in shrike/build/commands/prepare.py def is_in_additional_includes ( self , modified_file , component_additional_includes_contents ) -> bool : \"\"\" This function returns True if 'modified_file' is covered by the additional_includes file 'component_additional_includes_contents'. \"\"\" # first tackle the trivial case of no additional_includes file if component_additional_includes_contents is None : log . debug ( \"The component's additional_includes file is empty, returning False.\" ) return False # now the regular scenario for line in component_additional_includes_contents : # when the line from additional_includes is a file, we directly chech its path against that of modified_file if Path ( line ) . is_file (): if str ( Path ( modified_file ) . resolve ()) == str ( Path ( line ) . resolve () ): # can't use 'samefile' here because modified_file is not guaranteed to exist, we resolve the path and do basic == test log . info ( \"'\" + modified_file + \" is directly listed in the additional_includes file.\" ) return True # slightly more complicated case: when the line in additional_includes is a directory, we can just call the is_in_subfolder function if Path ( line ) . is_dir (): if self . is_in_subfolder ( modified_file , line ): log . info ( \"'\" + modified_file + \" is in one of the directories listed in the additional_includes file.\" ) return True log . debug ( \"'\" + modified_file + \" is NOT referenced by the additional_includes file (neither directly nor indirectly).\" ) return False","title":"is_in_additional_includes()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.is_in_subfolder","text":"This function returns True if 'modified_file' is in a subfolder of 'component' ('component' can be either the path to a file, or a directory). If the component has been deleted, returns False. Source code in shrike/build/commands/prepare.py def is_in_subfolder ( self , modified_file , component ) -> bool : \"\"\" This function returns True if 'modified_file' is in a subfolder of 'component' ('component' can be either the path to a file, or a directory). If the component has been deleted, returns False. \"\"\" # Let's first take care of the case where the component has been deleted if not ( Path ( component ) . exists ()): log . debug ( \"'\" + component + \"' does not exist, returning False.\" ) return False # Case where the component has not been deleted for parent in Path ( modified_file ) . parents : if parent . exists (): if Path ( component ) . is_dir (): if parent . samefile ( Path ( component )): log . info ( \"'\" + modified_file + \" is in a subfolder of '\" + component + \"'.\" ) return True else : if parent . samefile ( Path ( component ) . parent ): log . info ( \"'\" + modified_file + \" is in a subfolder of '\" + component + \"'.\" ) return True log . debug ( \"'\" + modified_file + \" is NOT in a subfolder of '\" + component + \"'.\" ) return False","title":"is_in_subfolder()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.run_with_config","text":"Run the subclasses command with the specified configuration object. Before this method is invoked, there is no guarantee that self.config will be populated; after it is invoked, that is guaranteed. Implementations of this method should NOT mutate the logging tree in any way. They should also NOT raise any exceptions; rather they should call the register_error method, which will ensure non-zero exit code. Implementations can raise specific \"status information\" (e.g., a component is not \"active\") by calling register_component_status . Source code in shrike/build/commands/prepare.py def run_with_config ( self ): log . info ( \"Running component preparation logic.\" ) self . telemetry_logging ( command = \"prepare\" ) component_files = self . find_component_specification_files () if not self . config . suppress_adding_repo_pr_tags : try : component_files = self . add_repo_and_last_pr_to_tags ( component_files ) except StopIteration : log . warning ( \"`add_repo_and_last_pr_to_tags` not successful. Please make sure your component files are in Git. Otherwise, please set `suppress_adding_repo_pr_tags` to True.\" ) if self . config . signing_mode == \"aml\" : self . ensure_component_cli_installed () self . attach_workspace () self . validate_all_components ( component_files ) built_component_files = self . build_all_components ( component_files ) else : built_component_files = component_files self . create_catalog_files ( built_component_files ) self . _create_requirements_files ( component_files )","title":"run_with_config()"},{"location":"build/prepare/#shrike.build.commands.prepare.Prepare.validate_all_components","text":"For each component specification file, run az ml component validate , run compliance and customized validation if enabled, and register the status (+ register error if validation failed). Source code in shrike/build/commands/prepare.py def validate_all_components ( self , files : List [ str ]) -> None : \"\"\" For each component specification file, run `az ml component validate`, run compliance and customized validation if enabled, and register the status (+ register error if validation failed). \"\"\" for component in files : validate_component_success = self . execute_azure_cli_command ( f \"ml component validate --file { component } \" ) compliance_validation_success = True customized_validation_success = True if self . config . enable_component_validation : log . info ( f \"Running compliance validation on { component } \" ) compliance_validation_success = self . compliance_validation ( component ) if len ( self . config . component_validation ) > 0 : log . info ( f \"Running customized validation on { component } \" ) for jsonpath , regex in self . config . component_validation . items (): customized_validation_success = ( customized_validation_success if self . customized_validation ( jsonpath , regex , component ) else False ) if ( validate_component_success and compliance_validation_success and customized_validation_success ): # If the az ml validation succeeds, we continue to check whether # the \"code\" snapshot parameter is specified in the spec file # https://componentsdk.z22.web.core.windows.net/components/component-spec-topics/code-snapshot.html with open ( component , \"r\" ) as spec_file : spec = YAML ( typ = \"safe\" ) . load ( spec_file ) spec_code = spec . get ( \"code\" ) if spec_code and spec_code not in [ \".\" , \"./\" ]: self . register_component_status ( component , \"validate\" , \"failed\" ) self . register_error ( \"Code snapshot parameter is not supported. Please use .additional_includes for your component.\" ) else : log . info ( f \"Component { component } is valid.\" ) self . register_component_status ( component , \"validate\" , \"succeeded\" ) else : self . register_component_status ( component , \"validate\" , \"failed\" ) self . register_error ( f \"Error when validating component { component } .\" )","title":"validate_all_components()"},{"location":"build/register/","text":"Register Register find_signed_component_specification_files ( self , dir = None ) Find all signed component (AML format) generated in the prepare step. Return the absolute paths of these components in the format of a list of string. Source code in shrike/build/commands/register.py def find_signed_component_specification_files ( self , dir = None ) -> List [ str ]: \"\"\" Find all signed component (AML format) generated in the `prepare` step. Return the absolute paths of these components in the format of a list of string. \"\"\" if dir is None : dir = self . config . working_directory signed_component_spec_files = [] # Find the path of spec files in the '.build' folders for spec_path in Path ( dir ) . glob ( \"**/.build/\" + Path ( self . config . component_specification_glob ) . name ): # Check whether the component is signed by examining catalog files if ( spec_path . parent . joinpath ( \"catalog.json\" ) . exists () and spec_path . parent . joinpath ( \"catalog.json.sig\" ) . exists () ): signed_component_spec_files . append ( os . path . abspath ( spec_path )) log . info ( f \"Find a signed component for AML: { spec_path } \" ) elif spec_path . parent . joinpath ( \".build.cat\" ) . exists (): log . info ( f \"Find a signed component for Aether: { spec_path } \" ) else : log . warning ( f \"Find an unsigned component: { spec_path } \" ) log . info ( str ( spec_path . parent . joinpath ( \"catalog.json\" ))) if len ( signed_component_spec_files ) == 0 : log . info ( \"Cannot find any signed components for AML.\" ) else : log . info ( f \"Find { len ( signed_component_spec_files ) } signed components for AML.\" ) return signed_component_spec_files list_registered_component ( self ) Log all registered component in the attached workspace by using az ml command. Source code in shrike/build/commands/register.py def list_registered_component ( self ) -> None : \"\"\" Log all registered component in the attached workspace by using az ml command. \"\"\" list_registered_component_success = self . execute_azure_cli_command ( f \"ml component list -o table\" ) if not list_registered_component_success : self . register_error ( f \"Error when listing registered components.\" ) register_all_signed_components ( self , files ) For each signed component specification file, run az ml component create , and register the status (+ register error if registration failed). Source code in shrike/build/commands/register.py def register_all_signed_components ( self , files : List [ str ]) -> None : \"\"\" For each signed component specification file, run `az ml component create`, and register the status (+ register error if registration failed). \"\"\" for component in files : register_command , stderr_is_failure = self . register_component_command ( component ) register_component_success = self . execute_azure_cli_command ( command = register_command , stderr_is_failure = stderr_is_failure , ) if register_component_success : log . info ( f \"Component { component } is registered.\" ) self . register_component_status ( component , \"register\" , \"succeeded\" ) else : self . register_component_status ( component , \"register\" , \"failed\" ) self . register_error ( f \"Error when registering component { component } .\" ) run_with_config ( self ) Running component registration logic. Source code in shrike/build/commands/register.py def run_with_config ( self ): \"\"\" Running component registration logic. \"\"\" self . telemetry_logging ( command = \"register\" ) self . validate_branch () self . ensure_component_cli_installed () component_path = self . find_signed_component_specification_files () if len ( component_path ) > 0 : for workspace_id in self . config . workspaces : log . info ( f \"Start registering signed components in { workspace_id } \" ) self . attach_workspace ( workspace_id ) log . info ( \"List of components in workspace before current registration.\" ) self . list_registered_component () self . register_all_signed_components ( files = component_path ) log . info ( \"List of components in workspace after current registration.\" ) self . list_registered_component () validate_branch ( self ) Check whether the current source branch name matches the configured regular expression of branch name. Fail if it doesn't match. Source code in shrike/build/commands/register.py def validate_branch ( self ) -> None : \"\"\" Check whether the current source branch name matches the configured regular expression of branch name. Fail if it doesn't match. \"\"\" log . info ( f \"Expected branch: { self . config . compliant_branch } \" ) log . info ( f \"Current branch: { self . config . source_branch } \" ) if re . match ( self . config . compliant_branch , self . config . source_branch ): log . info ( f \"Current branch matches configured regular expression.\" ) else : raise ValueError ( f \"Current branch name doesn't match configured name pattern.\" )","title":"register"},{"location":"build/register/#register","text":"","title":"Register"},{"location":"build/register/#shrike.build.commands.register.Register","text":"","title":"Register"},{"location":"build/register/#shrike.build.commands.register.Register.find_signed_component_specification_files","text":"Find all signed component (AML format) generated in the prepare step. Return the absolute paths of these components in the format of a list of string. Source code in shrike/build/commands/register.py def find_signed_component_specification_files ( self , dir = None ) -> List [ str ]: \"\"\" Find all signed component (AML format) generated in the `prepare` step. Return the absolute paths of these components in the format of a list of string. \"\"\" if dir is None : dir = self . config . working_directory signed_component_spec_files = [] # Find the path of spec files in the '.build' folders for spec_path in Path ( dir ) . glob ( \"**/.build/\" + Path ( self . config . component_specification_glob ) . name ): # Check whether the component is signed by examining catalog files if ( spec_path . parent . joinpath ( \"catalog.json\" ) . exists () and spec_path . parent . joinpath ( \"catalog.json.sig\" ) . exists () ): signed_component_spec_files . append ( os . path . abspath ( spec_path )) log . info ( f \"Find a signed component for AML: { spec_path } \" ) elif spec_path . parent . joinpath ( \".build.cat\" ) . exists (): log . info ( f \"Find a signed component for Aether: { spec_path } \" ) else : log . warning ( f \"Find an unsigned component: { spec_path } \" ) log . info ( str ( spec_path . parent . joinpath ( \"catalog.json\" ))) if len ( signed_component_spec_files ) == 0 : log . info ( \"Cannot find any signed components for AML.\" ) else : log . info ( f \"Find { len ( signed_component_spec_files ) } signed components for AML.\" ) return signed_component_spec_files","title":"find_signed_component_specification_files()"},{"location":"build/register/#shrike.build.commands.register.Register.list_registered_component","text":"Log all registered component in the attached workspace by using az ml command. Source code in shrike/build/commands/register.py def list_registered_component ( self ) -> None : \"\"\" Log all registered component in the attached workspace by using az ml command. \"\"\" list_registered_component_success = self . execute_azure_cli_command ( f \"ml component list -o table\" ) if not list_registered_component_success : self . register_error ( f \"Error when listing registered components.\" )","title":"list_registered_component()"},{"location":"build/register/#shrike.build.commands.register.Register.register_all_signed_components","text":"For each signed component specification file, run az ml component create , and register the status (+ register error if registration failed). Source code in shrike/build/commands/register.py def register_all_signed_components ( self , files : List [ str ]) -> None : \"\"\" For each signed component specification file, run `az ml component create`, and register the status (+ register error if registration failed). \"\"\" for component in files : register_command , stderr_is_failure = self . register_component_command ( component ) register_component_success = self . execute_azure_cli_command ( command = register_command , stderr_is_failure = stderr_is_failure , ) if register_component_success : log . info ( f \"Component { component } is registered.\" ) self . register_component_status ( component , \"register\" , \"succeeded\" ) else : self . register_component_status ( component , \"register\" , \"failed\" ) self . register_error ( f \"Error when registering component { component } .\" )","title":"register_all_signed_components()"},{"location":"build/register/#shrike.build.commands.register.Register.run_with_config","text":"Running component registration logic. Source code in shrike/build/commands/register.py def run_with_config ( self ): \"\"\" Running component registration logic. \"\"\" self . telemetry_logging ( command = \"register\" ) self . validate_branch () self . ensure_component_cli_installed () component_path = self . find_signed_component_specification_files () if len ( component_path ) > 0 : for workspace_id in self . config . workspaces : log . info ( f \"Start registering signed components in { workspace_id } \" ) self . attach_workspace ( workspace_id ) log . info ( \"List of components in workspace before current registration.\" ) self . list_registered_component () self . register_all_signed_components ( files = component_path ) log . info ( \"List of components in workspace after current registration.\" ) self . list_registered_component ()","title":"run_with_config()"},{"location":"build/register/#shrike.build.commands.register.Register.validate_branch","text":"Check whether the current source branch name matches the configured regular expression of branch name. Fail if it doesn't match. Source code in shrike/build/commands/register.py def validate_branch ( self ) -> None : \"\"\" Check whether the current source branch name matches the configured regular expression of branch name. Fail if it doesn't match. \"\"\" log . info ( f \"Expected branch: { self . config . compliant_branch } \" ) log . info ( f \"Current branch: { self . config . source_branch } \" ) if re . match ( self . config . compliant_branch , self . config . source_branch ): log . info ( f \"Current branch matches configured regular expression.\" ) else : raise ValueError ( f \"Current branch name doesn't match configured name pattern.\" )","title":"validate_branch()"},{"location":"compliant_logging/","text":"Exception Handling First execute pip install shrike to install this library. Then wrap any methods which may throw an exception with the decorator prefix_stack_trace . Here's a simple example. Your code may explicitly raise the Public* exceptions ( PublicValueError , PublicRuntimeError , PublicArgumentError , PublicKeyError , PublicTypeError ) when you know that the content of the exception does not contain any private content. The messages in these exceptions will be preserved, even if keep_message is set to False . from shrike.compliant_logging.exceptions import prefix_stack_trace @prefix_stack_trace () def main (): print ( \"Hello, world!\" ) if __name__ == \"__main__\" : main () Logging Call shrike.compliant_logging.enable_compliant_logging to set up data-category-aware logging. Then continue to use standard Python logging functionality as before! Add a category=DataCategory.PUBLIC argument to have your log lines prefixed with SystemLog: . Here is a full-fledged example: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. import argparse import shrike from shrike.compliant_logging.constants import DataCategory import logging if __name__ == \"__main__\" : parser = argparse . ArgumentParser () parser . add_argument ( \"--prefix\" , default = \"SystemLog:\" ) parser . add_argument ( \"--log_level\" , default = \"INFO\" ) args = parser . parse_args () shrike . compliant_logging . enable_compliant_logging ( args . prefix , level = args . log_level , format = \" %(prefix)s%(levelname)s : %(name)s : %(message)s \" , ) # Output will be: # WARNING:root:private info # SystemLog:WARNING:root:public info logging . warning ( \"private info\" ) logging . warning ( \"public info\" , category = DataCategory . PUBLIC ) logger = logging . getLogger ( __name__ ) # Output will be: # SystemLog:INFO:__main__:public info # SystemLog:WARNING:__main__:public info # WARNING:__main__:private info logger . info ( \"public info\" , category = DataCategory . PUBLIC ) logger . warning ( \"public info\" , category = DataCategory . PUBLIC ) logger . warning ( \"private info\" ) Examples The simplest use case (wrap your main method in a decorator) is: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Simplest example of how to use the prefix_stack_trace decorator. \"\"\" from shrike.compliant_logging.exceptions import prefix_stack_trace # Output will be: # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\hello-world.py\", line 11, in main # SystemLog: print(1 / 0) # SystemLog: ZeroDivisionError: **Exception message scrubbed** @prefix_stack_trace () def main (): print ( \"Hello, world!\" ) print ( 1 / 0 ) if __name__ == \"__main__\" : main () Prefixing stack trace shrike offers some configuration options around prefixing the stack trace. You can: customize the prefix and the exception message; keep the original exception message (don't scrub); pass an allow_list of strings. Exception messages will be scrubbed unless the message or the exception type regex match one of the allow_list strings. # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Demonstrate how scrubbing options. \"\"\" from shrike.compliant_logging.exceptions import prefix_stack_trace # Output will be: # # MyCustomPrefix Traceback (most recent call last): # MyCustomPrefix File \".\\prefix-stack-trace.py\", line 11, in main # MyCustomPrefix print(1 / 0) # MyCustomPrefix ZeroDivisionError: **Exception message scrubbed** @prefix_stack_trace ( prefix = \"MyCustomPrefix\" ) def custom_prefix (): print ( 1 / 0 ) # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \"/mnt/c/code/shrike/docs/samples/prefix-stack-trace.py\", line 20, in scrub2 # SystemLog: print(1 / 0) # SystemLog: ZeroDivisionError: Private data was divided by zero @prefix_stack_trace ( scrub_message = \"Private data was divided by zero\" ) def custom_message (): print ( 1 / 0 ) # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \"/mnt/c/code/shrike/docs/samples/prefix-stack-trace.py\", line 24, in scrub3 # SystemLog: print(1 / 0) # SystemLog: ZeroDivisionError: division by zero @prefix_stack_trace ( keep_message = True ) def keep_exception_message (): print ( 1 / 0 ) # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \"/mnt/c/code/shrike/docs/samples/prefix-stack-trace.py\", line 28, in scrub4 # SystemLog: print(1 / 0) # SystemLog: ZeroDivisionError: division by zero @prefix_stack_trace ( keep_message = False , allow_list = [ \"ZeroDivision\" ]) def keep_allowed_exceptions (): print ( 1 / 0 ) # Output will be: # # SystemLog: 2020-11-12 16:56:59 Traceback (most recent call last): # SystemLog: 2020-11-12 16:56:59 File \"prefix-stack-trace.py\", line 56, in keep_allowed_exceptions # SystemLog: 2020-11-12 16:56:59 print(1 / 0) # SystemLog: 2020-11-12 16:56:59 ZeroDivisionError: **Exception message scrubbed** @prefix_stack_trace ( add_timestamp = True ) def add_timestamp (): print ( 1 / 0 ) if __name__ == \"__main__\" : try : custom_prefix () except : pass try : custom_message () except : pass try : keep_exception_message () except : pass try : keep_allowed_exceptions () except : pass try : add_timestamp () except : pass With statements Use this library with with statements: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Simple script with examples of how to use \"with statements\" to capture information about failed module imports. \"\"\" from shrike.compliant_logging.exceptions import PrefixStackTrace # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\docs\\logging\\with-statement.py\", line 11, in <module> # SystemLog: import my_custom_library # noqa: F401 # SystemLog: ModuleNotFoundError: **Exception message scrubbed** with PrefixStackTrace (): # Import statement which could raise an exception containing sensitive # data. import my_custom_library # noqa: F401 # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\docs\\logging\\with-statement.py\", line 22, in <module> # SystemLog: import another_custom_library # noqa: F401 # SystemLog: ModuleNotFoundError: No module named 'another_custom_library' with PrefixStackTrace ( keep_message = True ): # Import statement which will never raise an exception containing sensitive # data. import another_custom_library # noqa: F401 Directly with try / except statements Using this library directly inside try / except statements: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Simple script with examples of how to directly use the function print_prefixed_stack_trace to capture information about failed module imports. \"\"\" from shrike.compliant_logging.exceptions import print_prefixed_stack_trace_and_raise try : # Import statement which could raise an exception containing sensitive # data. import my_custom_library # noqa: F401 except BaseException as e : # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\try-except.py\", line 10, in <module> # SystemLog: import my_custom_library # SystemLog: ModuleNotFoundError: **Exception message scrubbed** print_prefixed_stack_trace_and_raise ( err = e ) try : # Import statement which will never raise an exception containing sensitive # data. import another_custom_library # noqa: F401 except BaseException as e : # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\try-except.py\", line 17, in <module> # SystemLog: import another_custom_library # SystemLog: ModuleNotFoundError: No module named 'another_custom_library' print_prefixed_stack_trace_and_raise ( err = e , keep_message = True ) Public exception types Using the Public* exception types: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Sample use of Public* exception types. \"\"\" from shrike.compliant_logging.exceptions import prefix_stack_trace , PublicValueError def divide ( a , b ): if not b : raise PublicValueError ( \"Second argument cannot be null or zero.\" ) return a / b # Output will be: # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\docs\\logging\\public-exceptions.py\", line 24, in main # SystemLog: divide(1, 0) # SystemLog: File \".\\docs\\logging\\public-exceptions.py\", line 13, in divide # SystemLog: raise PublicValueError(\"Second argument cannot be null or zero.\") # SystemLog: compliant_logging.exceptions.PublicValueError: SystemLog:Second argument cannot be null or zero. @prefix_stack_trace () def main (): divide ( 1 , 0 ) if __name__ == \"__main__\" : main () Exception or Stack trace parsing The stack_trace_extractor namespace contains simple tools to grab Python or C# stack traces and exceptions from log files. Sometimes the file that has the stack trace you need may also contain sensitive data. Use this tool to parse and print the stack trace, exception type and optionally exception message (careful as exception messages may also potentially hold private data). from shrike.compliant_logging.stack_trace_extractor import StacktraceExtractor extractor = StacktraceExtractor () extractor . extract ( \"log_file\" )","title":"Logging examples"},{"location":"compliant_logging/#exception-handling","text":"First execute pip install shrike to install this library. Then wrap any methods which may throw an exception with the decorator prefix_stack_trace . Here's a simple example. Your code may explicitly raise the Public* exceptions ( PublicValueError , PublicRuntimeError , PublicArgumentError , PublicKeyError , PublicTypeError ) when you know that the content of the exception does not contain any private content. The messages in these exceptions will be preserved, even if keep_message is set to False . from shrike.compliant_logging.exceptions import prefix_stack_trace @prefix_stack_trace () def main (): print ( \"Hello, world!\" ) if __name__ == \"__main__\" : main ()","title":"Exception Handling"},{"location":"compliant_logging/#logging","text":"Call shrike.compliant_logging.enable_compliant_logging to set up data-category-aware logging. Then continue to use standard Python logging functionality as before! Add a category=DataCategory.PUBLIC argument to have your log lines prefixed with SystemLog: . Here is a full-fledged example: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. import argparse import shrike from shrike.compliant_logging.constants import DataCategory import logging if __name__ == \"__main__\" : parser = argparse . ArgumentParser () parser . add_argument ( \"--prefix\" , default = \"SystemLog:\" ) parser . add_argument ( \"--log_level\" , default = \"INFO\" ) args = parser . parse_args () shrike . compliant_logging . enable_compliant_logging ( args . prefix , level = args . log_level , format = \" %(prefix)s%(levelname)s : %(name)s : %(message)s \" , ) # Output will be: # WARNING:root:private info # SystemLog:WARNING:root:public info logging . warning ( \"private info\" ) logging . warning ( \"public info\" , category = DataCategory . PUBLIC ) logger = logging . getLogger ( __name__ ) # Output will be: # SystemLog:INFO:__main__:public info # SystemLog:WARNING:__main__:public info # WARNING:__main__:private info logger . info ( \"public info\" , category = DataCategory . PUBLIC ) logger . warning ( \"public info\" , category = DataCategory . PUBLIC ) logger . warning ( \"private info\" )","title":"Logging"},{"location":"compliant_logging/#examples","text":"The simplest use case (wrap your main method in a decorator) is: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Simplest example of how to use the prefix_stack_trace decorator. \"\"\" from shrike.compliant_logging.exceptions import prefix_stack_trace # Output will be: # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\hello-world.py\", line 11, in main # SystemLog: print(1 / 0) # SystemLog: ZeroDivisionError: **Exception message scrubbed** @prefix_stack_trace () def main (): print ( \"Hello, world!\" ) print ( 1 / 0 ) if __name__ == \"__main__\" : main ()","title":"Examples"},{"location":"compliant_logging/#prefixing-stack-trace","text":"shrike offers some configuration options around prefixing the stack trace. You can: customize the prefix and the exception message; keep the original exception message (don't scrub); pass an allow_list of strings. Exception messages will be scrubbed unless the message or the exception type regex match one of the allow_list strings. # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Demonstrate how scrubbing options. \"\"\" from shrike.compliant_logging.exceptions import prefix_stack_trace # Output will be: # # MyCustomPrefix Traceback (most recent call last): # MyCustomPrefix File \".\\prefix-stack-trace.py\", line 11, in main # MyCustomPrefix print(1 / 0) # MyCustomPrefix ZeroDivisionError: **Exception message scrubbed** @prefix_stack_trace ( prefix = \"MyCustomPrefix\" ) def custom_prefix (): print ( 1 / 0 ) # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \"/mnt/c/code/shrike/docs/samples/prefix-stack-trace.py\", line 20, in scrub2 # SystemLog: print(1 / 0) # SystemLog: ZeroDivisionError: Private data was divided by zero @prefix_stack_trace ( scrub_message = \"Private data was divided by zero\" ) def custom_message (): print ( 1 / 0 ) # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \"/mnt/c/code/shrike/docs/samples/prefix-stack-trace.py\", line 24, in scrub3 # SystemLog: print(1 / 0) # SystemLog: ZeroDivisionError: division by zero @prefix_stack_trace ( keep_message = True ) def keep_exception_message (): print ( 1 / 0 ) # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \"/mnt/c/code/shrike/docs/samples/prefix-stack-trace.py\", line 28, in scrub4 # SystemLog: print(1 / 0) # SystemLog: ZeroDivisionError: division by zero @prefix_stack_trace ( keep_message = False , allow_list = [ \"ZeroDivision\" ]) def keep_allowed_exceptions (): print ( 1 / 0 ) # Output will be: # # SystemLog: 2020-11-12 16:56:59 Traceback (most recent call last): # SystemLog: 2020-11-12 16:56:59 File \"prefix-stack-trace.py\", line 56, in keep_allowed_exceptions # SystemLog: 2020-11-12 16:56:59 print(1 / 0) # SystemLog: 2020-11-12 16:56:59 ZeroDivisionError: **Exception message scrubbed** @prefix_stack_trace ( add_timestamp = True ) def add_timestamp (): print ( 1 / 0 ) if __name__ == \"__main__\" : try : custom_prefix () except : pass try : custom_message () except : pass try : keep_exception_message () except : pass try : keep_allowed_exceptions () except : pass try : add_timestamp () except : pass","title":"Prefixing stack trace"},{"location":"compliant_logging/#with-statements","text":"Use this library with with statements: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Simple script with examples of how to use \"with statements\" to capture information about failed module imports. \"\"\" from shrike.compliant_logging.exceptions import PrefixStackTrace # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\docs\\logging\\with-statement.py\", line 11, in <module> # SystemLog: import my_custom_library # noqa: F401 # SystemLog: ModuleNotFoundError: **Exception message scrubbed** with PrefixStackTrace (): # Import statement which could raise an exception containing sensitive # data. import my_custom_library # noqa: F401 # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\docs\\logging\\with-statement.py\", line 22, in <module> # SystemLog: import another_custom_library # noqa: F401 # SystemLog: ModuleNotFoundError: No module named 'another_custom_library' with PrefixStackTrace ( keep_message = True ): # Import statement which will never raise an exception containing sensitive # data. import another_custom_library # noqa: F401","title":"With statements"},{"location":"compliant_logging/#directly-with-try-except-statements","text":"Using this library directly inside try / except statements: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Simple script with examples of how to directly use the function print_prefixed_stack_trace to capture information about failed module imports. \"\"\" from shrike.compliant_logging.exceptions import print_prefixed_stack_trace_and_raise try : # Import statement which could raise an exception containing sensitive # data. import my_custom_library # noqa: F401 except BaseException as e : # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\try-except.py\", line 10, in <module> # SystemLog: import my_custom_library # SystemLog: ModuleNotFoundError: **Exception message scrubbed** print_prefixed_stack_trace_and_raise ( err = e ) try : # Import statement which will never raise an exception containing sensitive # data. import another_custom_library # noqa: F401 except BaseException as e : # Output will be: # # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\try-except.py\", line 17, in <module> # SystemLog: import another_custom_library # SystemLog: ModuleNotFoundError: No module named 'another_custom_library' print_prefixed_stack_trace_and_raise ( err = e , keep_message = True )","title":"Directly with try / except statements"},{"location":"compliant_logging/#public-exception-types","text":"Using the Public* exception types: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Sample use of Public* exception types. \"\"\" from shrike.compliant_logging.exceptions import prefix_stack_trace , PublicValueError def divide ( a , b ): if not b : raise PublicValueError ( \"Second argument cannot be null or zero.\" ) return a / b # Output will be: # SystemLog: Traceback (most recent call last): # SystemLog: File \".\\docs\\logging\\public-exceptions.py\", line 24, in main # SystemLog: divide(1, 0) # SystemLog: File \".\\docs\\logging\\public-exceptions.py\", line 13, in divide # SystemLog: raise PublicValueError(\"Second argument cannot be null or zero.\") # SystemLog: compliant_logging.exceptions.PublicValueError: SystemLog:Second argument cannot be null or zero. @prefix_stack_trace () def main (): divide ( 1 , 0 ) if __name__ == \"__main__\" : main ()","title":"Public exception types"},{"location":"compliant_logging/#exception-or-stack-trace-parsing","text":"The stack_trace_extractor namespace contains simple tools to grab Python or C# stack traces and exceptions from log files. Sometimes the file that has the stack trace you need may also contain sensitive data. Use this tool to parse and print the stack trace, exception type and optionally exception message (careful as exception messages may also potentially hold private data). from shrike.compliant_logging.stack_trace_extractor import StacktraceExtractor extractor = StacktraceExtractor () extractor . extract ( \"log_file\" )","title":"Exception or Stack trace parsing"},{"location":"compliant_logging/aml-metrics-logging/","text":"Logging metrics in Azure ML Portal using shrike Logging real-time metrics and sending them to Azure Machine Learning (ML) workspace portal is supported by shrike >= 1.7.0 . This page is on how to use the shrike library to log various types of metrics in AML. For the information on how to use the Azure ML Python SDK for metrics logging, please check out this documentation . This metrics logging feature in shrike is supported in both eyes-on and eyes-off environments, and it also works in the offline runs (e.g., your local laptop or any non-AML virtual machines). For jobs in detonation chambers, please check out this internal note . Before logging any metrics, call shrike.compliant_logging.enable_compliant_logging with the argument use_aml_metrics=True and category=DataCategory.PUBLIC to connect with the workspace portal and set up data-category-aware logging. Then continue to use the standard Python logging functionality as before. There are various metric-logging functions to match various metric types. The API references on all metric-logging functions are availabe on this page . Use the following methods in the logging APIs for different scenarios & metric types. Logged Value Example Code Supported Types Log image log.metric_image(name='food', path='./breadpudding.jpg', plot=None, description='desert', category=DataCategory.PUBLIC) string, matplotlib.pyplot.plot Log an array of numeric values log.metric_list(name=\"Fibonacci\", value=[0, 1, 1, 2, 3, 5, 8], category=DataCategory.PUBLIC) list, tuple Log a single value log.metric_value(name='metric_value', value=1, step=NA, category=DataCategory.PUBLIC)) scalar Log a row with 2 numerical columns log.metric_row(name='Cosine Wave', angle=0, cos=1, category=DataCategory.PUBLIC)) scalar Log a table log.metric_table(name=\"students\", value={\"name\": [\"James\", \"Robert\", \"Michael\"], \"number\": [2, 3, 1, 5]}, category=DataCategory.PUBLIC) dict Log residuals log.metric_residual(name=\"ml_residual\", value=panda.DataFrame([[1.0, 1.1], [2.0, 2.0], [3.0, 3.1]],columns=[\"pred\", \"targ\"]), col_predict=\"pred\", col_target=\"targ\", category=DataCategory.PUBLIC) dict, pandas.DataFrame, vaex.dataframe, spark dataframe Log confusion matrix log.metric_confusion_matrix(name=\"animal_classification\", value=vaex.from_arrays(x=[\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"], y=[\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]), idx_true=\"x\", idx_pred=\"y\",category=DataCategory.PUBLI) dict, pandas.DataFrame, vaex.dataframe, spark dataframe Log accuracy table log.metric_confusion_matrix(name=\"accuracy_table\", value=vaex.from_arrays(x=[0.1, 0.3, 0.7], y=[\"a\", \"b\", \"c\"]), idx_true=\"x\", idx_pred=\"y\",category=DataCategory.PUBLI) dict, pandas.DataFrame, vaex.dataframe, spark dataframe Here is a full-fledged example: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. import argparse import logging import random import matplotlib.pyplot as plt from shrike.compliant_logging import enable_compliant_logging from shrike.compliant_logging.constants import DataCategory def run ( args ): n = args . list_length list1 = [ random . randint ( 0 , 100 ) for i in range ( n )] list2 = [ random . randint ( 0 , 100 ) for i in range ( n )] log = logging . getLogger ( __name__ ) log . info ( \"Start metric logging in azure ml workspace portal\" , category = DataCategory . PUBLIC , ) # log list log . metric_list ( name = \"list1\" , value = list1 , category = DataCategory . PUBLIC ) # log table log . metric_table ( name = \"Lists\" , value = { \"list1\" : list1 , \"list2\" : list2 }, category = DataCategory . PUBLIC , ) # log scalar value log . metric ( name = \"sum1\" , value = sum ( list1 ), category = DataCategory . PUBLIC ) log . metric ( name = \"sum2\" , value = sum ( list2 ), category = DataCategory . PUBLIC ) # log image plt . plot ( list1 , list2 ) log . metric_image ( name = \"Sample plot\" , plot = plt , category = DataCategory . PUBLIC ) # log row for i in range ( n ): log . metric_row ( name = \"pairwise-sum\" , description = \"\" , category = DataCategory . PUBLIC , pairwise_sum = list1 [ i ] + list2 [ i ], ) if __name__ == \"__main__\" : parser = argparse . ArgumentParser () parser . add_argument ( \"--prefix\" , default = \"SystemLog:\" ) parser . add_argument ( \"--log_level\" , default = \"INFO\" ) parser . add_argument ( \"--list_length\" , required = False , default = 5 , type = int , help = \"length of test list\" , ) args = parser . parse_args () enable_compliant_logging ( args . prefix , level = args . log_level , format = \" %(prefix)s%(levelname)s : %(name)s : %(message)s \" , use_aml_metrics = True , ) run ( args )","title":"Logging metrics in AML"},{"location":"compliant_logging/aml-metrics-logging/#logging-metrics-in-azure-ml-portal-using-shrike","text":"Logging real-time metrics and sending them to Azure Machine Learning (ML) workspace portal is supported by shrike >= 1.7.0 . This page is on how to use the shrike library to log various types of metrics in AML. For the information on how to use the Azure ML Python SDK for metrics logging, please check out this documentation . This metrics logging feature in shrike is supported in both eyes-on and eyes-off environments, and it also works in the offline runs (e.g., your local laptop or any non-AML virtual machines). For jobs in detonation chambers, please check out this internal note . Before logging any metrics, call shrike.compliant_logging.enable_compliant_logging with the argument use_aml_metrics=True and category=DataCategory.PUBLIC to connect with the workspace portal and set up data-category-aware logging. Then continue to use the standard Python logging functionality as before. There are various metric-logging functions to match various metric types. The API references on all metric-logging functions are availabe on this page . Use the following methods in the logging APIs for different scenarios & metric types. Logged Value Example Code Supported Types Log image log.metric_image(name='food', path='./breadpudding.jpg', plot=None, description='desert', category=DataCategory.PUBLIC) string, matplotlib.pyplot.plot Log an array of numeric values log.metric_list(name=\"Fibonacci\", value=[0, 1, 1, 2, 3, 5, 8], category=DataCategory.PUBLIC) list, tuple Log a single value log.metric_value(name='metric_value', value=1, step=NA, category=DataCategory.PUBLIC)) scalar Log a row with 2 numerical columns log.metric_row(name='Cosine Wave', angle=0, cos=1, category=DataCategory.PUBLIC)) scalar Log a table log.metric_table(name=\"students\", value={\"name\": [\"James\", \"Robert\", \"Michael\"], \"number\": [2, 3, 1, 5]}, category=DataCategory.PUBLIC) dict Log residuals log.metric_residual(name=\"ml_residual\", value=panda.DataFrame([[1.0, 1.1], [2.0, 2.0], [3.0, 3.1]],columns=[\"pred\", \"targ\"]), col_predict=\"pred\", col_target=\"targ\", category=DataCategory.PUBLIC) dict, pandas.DataFrame, vaex.dataframe, spark dataframe Log confusion matrix log.metric_confusion_matrix(name=\"animal_classification\", value=vaex.from_arrays(x=[\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"], y=[\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]), idx_true=\"x\", idx_pred=\"y\",category=DataCategory.PUBLI) dict, pandas.DataFrame, vaex.dataframe, spark dataframe Log accuracy table log.metric_confusion_matrix(name=\"accuracy_table\", value=vaex.from_arrays(x=[0.1, 0.3, 0.7], y=[\"a\", \"b\", \"c\"]), idx_true=\"x\", idx_pred=\"y\",category=DataCategory.PUBLI) dict, pandas.DataFrame, vaex.dataframe, spark dataframe Here is a full-fledged example: # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. import argparse import logging import random import matplotlib.pyplot as plt from shrike.compliant_logging import enable_compliant_logging from shrike.compliant_logging.constants import DataCategory def run ( args ): n = args . list_length list1 = [ random . randint ( 0 , 100 ) for i in range ( n )] list2 = [ random . randint ( 0 , 100 ) for i in range ( n )] log = logging . getLogger ( __name__ ) log . info ( \"Start metric logging in azure ml workspace portal\" , category = DataCategory . PUBLIC , ) # log list log . metric_list ( name = \"list1\" , value = list1 , category = DataCategory . PUBLIC ) # log table log . metric_table ( name = \"Lists\" , value = { \"list1\" : list1 , \"list2\" : list2 }, category = DataCategory . PUBLIC , ) # log scalar value log . metric ( name = \"sum1\" , value = sum ( list1 ), category = DataCategory . PUBLIC ) log . metric ( name = \"sum2\" , value = sum ( list2 ), category = DataCategory . PUBLIC ) # log image plt . plot ( list1 , list2 ) log . metric_image ( name = \"Sample plot\" , plot = plt , category = DataCategory . PUBLIC ) # log row for i in range ( n ): log . metric_row ( name = \"pairwise-sum\" , description = \"\" , category = DataCategory . PUBLIC , pairwise_sum = list1 [ i ] + list2 [ i ], ) if __name__ == \"__main__\" : parser = argparse . ArgumentParser () parser . add_argument ( \"--prefix\" , default = \"SystemLog:\" ) parser . add_argument ( \"--log_level\" , default = \"INFO\" ) parser . add_argument ( \"--list_length\" , required = False , default = 5 , type = int , help = \"length of test list\" , ) args = parser . parse_args () enable_compliant_logging ( args . prefix , level = args . log_level , format = \" %(prefix)s%(levelname)s : %(name)s : %(message)s \" , use_aml_metrics = True , ) run ( args )","title":"Logging metrics in Azure ML Portal using shrike"},{"location":"compliant_logging/constants/","text":"Constants Constant values used by this library. DataCategory Enumeration of data categories in compliant machine learning. Values: - PRIVATE: data which is private. Researchers may not view this. - PUBLIC: data which may safely be viewed by researchers.","title":"constants"},{"location":"compliant_logging/constants/#constants","text":"Constant values used by this library.","title":"Constants"},{"location":"compliant_logging/constants/#shrike.compliant_logging.constants.DataCategory","text":"Enumeration of data categories in compliant machine learning. Values: - PRIVATE: data which is private. Researchers may not view this. - PUBLIC: data which may safely be viewed by researchers.","title":"DataCategory"},{"location":"compliant_logging/exceptions/","text":"Exceptions Decorators and utilities for prefixing exception stack traces while obscuring the exception message itself. PublicArgumentError Argument error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception. PublicFileNotFoundError File not found error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception. PublicIndexError Index error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception. PublicIOError I/O error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception. PublicKeyError Key error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception. PublicNotImplementedError Not implemented error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception. PublicRuntimeError Runtime error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception. PublicTypeError Type error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception. PublicValueError Value error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception. is_exception_allowed ( exception , allow_list ) Check if message is allowed, either by allow_list , or default_allow_list . Parameters: Name Type Description Default exception Union[BaseException, traceback.TracebackException] the exception to test required allow_list list list of regex expressions. If any expression matches the exception name or message, it will be considered allowed. required Returns: Type Description bool bool: True if message is allowed, False otherwise. Source code in shrike/compliant_logging/exceptions.py def is_exception_allowed ( exception : Union [ BaseException , TracebackException ], allow_list : list ) -> bool : \"\"\" Check if message is allowed, either by `allow_list`, or `default_allow_list`. Args: exception (TracebackException): the exception to test allow_list (list): list of regex expressions. If any expression matches the exception name or message, it will be considered allowed. Returns: bool: True if message is allowed, False otherwise. \"\"\" if not isinstance ( exception , TracebackException ): exception = TracebackException . from_exception ( exception ) # empty list means all messages are allowed for expr in allow_list + default_allow_list : if re . search ( expr , getattr ( exception , \"_str\" , \"\" ), re . IGNORECASE ): return True if re . search ( expr , getattr ( exception . exc_type , \"__name__\" , \"\" ), re . IGNORECASE ): return True return False prefix_stack_trace ( file =< _io . TextIOWrapper name = '<stderr>' mode = 'w' encoding = 'UTF-8' > , disable = False , prefix = 'SystemLog:' , scrub_message = '**Exception message scrubbed**' , keep_message = False , allow_list = [], add_timestamp = False ) Decorator which wraps the decorated function and prints the stack trace of exceptions which occur, prefixed with prefix and with exception messages scrubbed (replaced with scrub_message ). To use this, just add @prefix_stack_trace() above your function definition, e.g. @prefix_stack_trace() def foo(x): pass Source code in shrike/compliant_logging/exceptions.py def prefix_stack_trace ( file : TextIO = sys . stderr , disable : bool = bool ( sys . flags . debug ), prefix : str = PREFIX , scrub_message : str = SCRUB_MESSAGE , keep_message : bool = False , allow_list : list = [], add_timestamp : bool = False , ) -> Callable : \"\"\" Decorator which wraps the decorated function and prints the stack trace of exceptions which occur, prefixed with `prefix` and with exception messages scrubbed (replaced with `scrub_message`). To use this, just add `@prefix_stack_trace()` above your function definition, e.g. @prefix_stack_trace() def foo(x): pass \"\"\" return _PrefixStackTraceWrapper ( file , disable , prefix , scrub_message , keep_message , allow_list , add_timestamp ) print_prefixed_stack_trace_and_raise ( file =< _io . TextIOWrapper name = '<stderr>' mode = 'w' encoding = 'UTF-8' > , prefix = 'SystemLog:' , scrub_message = '**Exception message scrubbed**' , keep_message = False , allow_list = [], add_timestamp = False , err = None ) Print the current exception and stack trace to file (usually client standard error), prefixing the stack trace with prefix . Parameters: Name Type Description Default keep_message bool if True, don't scrub message. If false, scrub (unless allowed). False allow_list list exception allow_list. Ignored if keep_message is True. If empty all messages will be srubbed. [] err Optional[BaseException] the error that was thrown. None accepted for backwards compatibility. None Source code in shrike/compliant_logging/exceptions.py def print_prefixed_stack_trace_and_raise ( file : TextIO = sys . stderr , prefix : str = PREFIX , scrub_message : str = SCRUB_MESSAGE , keep_message : bool = False , allow_list : list = [], add_timestamp : bool = False , err : Optional [ BaseException ] = None , ) -> None : \"\"\" Print the current exception and stack trace to `file` (usually client standard error), prefixing the stack trace with `prefix`. Args: keep_message (bool): if True, don't scrub message. If false, scrub (unless allowed). allow_list (list): exception allow_list. Ignored if keep_message is True. If empty all messages will be srubbed. err: the error that was thrown. None accepted for backwards compatibility. \"\"\" if err is None : err = sys . exc_info ()[ 1 ] scrubbed_err = scrub_exception ( err , scrub_message , prefix , keep_message , allow_list ) tb_exception = TracebackException . from_exception ( scrubbed_err ) # type: ignore for execution in tb_exception . format (): if \"return function(*func_args, **func_kwargs)\" in execution : # Do not show the stack trace for our decorator. continue for line in execution . splitlines (): if add_timestamp : current_time = time . strftime ( \"%Y-%m- %d %H:%M:%S\" , time . localtime ()) print ( f \" { prefix } { current_time } { line } \" , file = file ) else : print ( f \" { prefix } { line } \" , file = file ) raise scrubbed_err # type: ignore scrub_exception ( exception , scrub_message , prefix , keep_message , allow_list , _seen = None ) Recursively scrub all potentially private data from an exception, using the logic in _attribute_transformer . Inspired by Dan Schwartz's closed-source implementation: https://dev.azure.com/eemo/TEE/_git/TEEGit?path=%2FOffline%2FFocusedInbox%2FComTriage%2Fcomtriage%2Futils%2Fscrubber.py&version=GBcompliant%2FComTriage&_a=content which is closely based on the CPython implementation of the TracebackException class: https://github.com/python/cpython/blob/master/Lib/traceback.py#L478 Source code in shrike/compliant_logging/exceptions.py def scrub_exception ( exception : Optional [ BaseException ], scrub_message : str , prefix : str , keep_message : bool , allow_list : list , _seen : Optional [ Set [ int ]] = None , ) -> Optional [ BaseException ]: \"\"\" Recursively scrub all potentially private data from an exception, using the logic in `_attribute_transformer`. Inspired by Dan Schwartz's closed-source implementation: https://dev.azure.com/eemo/TEE/_git/TEEGit?path=%2FOffline%2FFocusedInbox%2FComTriage%2Fcomtriage%2Futils%2Fscrubber.py&version=GBcompliant%2FComTriage&_a=content which is closely based on the CPython implementation of the TracebackException class: https://github.com/python/cpython/blob/master/Lib/traceback.py#L478 \"\"\" if not exception : return None # Handle loops in __cause__ or __context__ . if _seen is None : _seen = set () _seen . add ( id ( exception )) # Gracefully handle being called with no type or value. if exception . __cause__ is not None and id ( exception . __cause__ ) not in _seen : exception . __cause__ = scrub_exception ( exception . __cause__ , scrub_message , prefix , keep_message , allow_list , _seen , ) if exception . __context__ is not None and id ( exception . __context__ ) not in _seen : exception . __context__ = scrub_exception ( exception . __context__ , scrub_message , prefix , keep_message , allow_list , _seen , ) keep = keep_message or is_exception_allowed ( exception , allow_list ) transformer = _attribute_transformer ( prefix , scrub_message , keep ) for attr in dir ( exception ): if attr and not attr . startswith ( \"__\" ): try : value = getattr ( exception , attr ) except AttributeError : # In some cases, e.g. FileNotFoundError, there are attributes # which show up in dir(e), but for which an AttributeError is # thrown when attempting to access the value. See, e.g.: # https://stackoverflow.com/q/47775772 . continue try : # If unable to transform or set the attribute, replace the # entire exception since the attribute value is readable, but # we are unable to scrub it. new_value = transformer ( value ) setattr ( exception , attr , new_value ) except BaseException as e : new_exception = PublicRuntimeError ( f \" { prefix } Obtained { type ( e ) . __name__ } when trying to scrub { attr } from { type ( exception ) . __name__ } \" # noqa: E501 ) new_exception . __cause__ = exception . __cause__ new_exception . __context__ = exception . __context__ exception = new_exception break return exception","title":"exceptions"},{"location":"compliant_logging/exceptions/#exceptions","text":"Decorators and utilities for prefixing exception stack traces while obscuring the exception message itself.","title":"Exceptions"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.PublicArgumentError","text":"Argument error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception.","title":"PublicArgumentError"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.PublicFileNotFoundError","text":"File not found error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception.","title":"PublicFileNotFoundError"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.PublicIndexError","text":"Index error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception.","title":"PublicIndexError"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.PublicIOError","text":"I/O error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception.","title":"PublicIOError"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.PublicKeyError","text":"Key error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception.","title":"PublicKeyError"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.PublicNotImplementedError","text":"Not implemented error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception.","title":"PublicNotImplementedError"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.PublicRuntimeError","text":"Runtime error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception.","title":"PublicRuntimeError"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.PublicTypeError","text":"Type error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception.","title":"PublicTypeError"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.PublicValueError","text":"Value error with public message. Exceptions of this type raised under prefix_stack_trace or print_prefixed_stack_trace_and_raise will have the message prefixed with PREFIX in both the printed stack trace and the re-raised exception.","title":"PublicValueError"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.is_exception_allowed","text":"Check if message is allowed, either by allow_list , or default_allow_list . Parameters: Name Type Description Default exception Union[BaseException, traceback.TracebackException] the exception to test required allow_list list list of regex expressions. If any expression matches the exception name or message, it will be considered allowed. required Returns: Type Description bool bool: True if message is allowed, False otherwise. Source code in shrike/compliant_logging/exceptions.py def is_exception_allowed ( exception : Union [ BaseException , TracebackException ], allow_list : list ) -> bool : \"\"\" Check if message is allowed, either by `allow_list`, or `default_allow_list`. Args: exception (TracebackException): the exception to test allow_list (list): list of regex expressions. If any expression matches the exception name or message, it will be considered allowed. Returns: bool: True if message is allowed, False otherwise. \"\"\" if not isinstance ( exception , TracebackException ): exception = TracebackException . from_exception ( exception ) # empty list means all messages are allowed for expr in allow_list + default_allow_list : if re . search ( expr , getattr ( exception , \"_str\" , \"\" ), re . IGNORECASE ): return True if re . search ( expr , getattr ( exception . exc_type , \"__name__\" , \"\" ), re . IGNORECASE ): return True return False","title":"is_exception_allowed()"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.prefix_stack_trace","text":"Decorator which wraps the decorated function and prints the stack trace of exceptions which occur, prefixed with prefix and with exception messages scrubbed (replaced with scrub_message ). To use this, just add @prefix_stack_trace() above your function definition, e.g. @prefix_stack_trace() def foo(x): pass Source code in shrike/compliant_logging/exceptions.py def prefix_stack_trace ( file : TextIO = sys . stderr , disable : bool = bool ( sys . flags . debug ), prefix : str = PREFIX , scrub_message : str = SCRUB_MESSAGE , keep_message : bool = False , allow_list : list = [], add_timestamp : bool = False , ) -> Callable : \"\"\" Decorator which wraps the decorated function and prints the stack trace of exceptions which occur, prefixed with `prefix` and with exception messages scrubbed (replaced with `scrub_message`). To use this, just add `@prefix_stack_trace()` above your function definition, e.g. @prefix_stack_trace() def foo(x): pass \"\"\" return _PrefixStackTraceWrapper ( file , disable , prefix , scrub_message , keep_message , allow_list , add_timestamp )","title":"prefix_stack_trace()"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.print_prefixed_stack_trace_and_raise","text":"Print the current exception and stack trace to file (usually client standard error), prefixing the stack trace with prefix . Parameters: Name Type Description Default keep_message bool if True, don't scrub message. If false, scrub (unless allowed). False allow_list list exception allow_list. Ignored if keep_message is True. If empty all messages will be srubbed. [] err Optional[BaseException] the error that was thrown. None accepted for backwards compatibility. None Source code in shrike/compliant_logging/exceptions.py def print_prefixed_stack_trace_and_raise ( file : TextIO = sys . stderr , prefix : str = PREFIX , scrub_message : str = SCRUB_MESSAGE , keep_message : bool = False , allow_list : list = [], add_timestamp : bool = False , err : Optional [ BaseException ] = None , ) -> None : \"\"\" Print the current exception and stack trace to `file` (usually client standard error), prefixing the stack trace with `prefix`. Args: keep_message (bool): if True, don't scrub message. If false, scrub (unless allowed). allow_list (list): exception allow_list. Ignored if keep_message is True. If empty all messages will be srubbed. err: the error that was thrown. None accepted for backwards compatibility. \"\"\" if err is None : err = sys . exc_info ()[ 1 ] scrubbed_err = scrub_exception ( err , scrub_message , prefix , keep_message , allow_list ) tb_exception = TracebackException . from_exception ( scrubbed_err ) # type: ignore for execution in tb_exception . format (): if \"return function(*func_args, **func_kwargs)\" in execution : # Do not show the stack trace for our decorator. continue for line in execution . splitlines (): if add_timestamp : current_time = time . strftime ( \"%Y-%m- %d %H:%M:%S\" , time . localtime ()) print ( f \" { prefix } { current_time } { line } \" , file = file ) else : print ( f \" { prefix } { line } \" , file = file ) raise scrubbed_err # type: ignore","title":"print_prefixed_stack_trace_and_raise()"},{"location":"compliant_logging/exceptions/#shrike.compliant_logging.exceptions.scrub_exception","text":"Recursively scrub all potentially private data from an exception, using the logic in _attribute_transformer . Inspired by Dan Schwartz's closed-source implementation: https://dev.azure.com/eemo/TEE/_git/TEEGit?path=%2FOffline%2FFocusedInbox%2FComTriage%2Fcomtriage%2Futils%2Fscrubber.py&version=GBcompliant%2FComTriage&_a=content which is closely based on the CPython implementation of the TracebackException class: https://github.com/python/cpython/blob/master/Lib/traceback.py#L478 Source code in shrike/compliant_logging/exceptions.py def scrub_exception ( exception : Optional [ BaseException ], scrub_message : str , prefix : str , keep_message : bool , allow_list : list , _seen : Optional [ Set [ int ]] = None , ) -> Optional [ BaseException ]: \"\"\" Recursively scrub all potentially private data from an exception, using the logic in `_attribute_transformer`. Inspired by Dan Schwartz's closed-source implementation: https://dev.azure.com/eemo/TEE/_git/TEEGit?path=%2FOffline%2FFocusedInbox%2FComTriage%2Fcomtriage%2Futils%2Fscrubber.py&version=GBcompliant%2FComTriage&_a=content which is closely based on the CPython implementation of the TracebackException class: https://github.com/python/cpython/blob/master/Lib/traceback.py#L478 \"\"\" if not exception : return None # Handle loops in __cause__ or __context__ . if _seen is None : _seen = set () _seen . add ( id ( exception )) # Gracefully handle being called with no type or value. if exception . __cause__ is not None and id ( exception . __cause__ ) not in _seen : exception . __cause__ = scrub_exception ( exception . __cause__ , scrub_message , prefix , keep_message , allow_list , _seen , ) if exception . __context__ is not None and id ( exception . __context__ ) not in _seen : exception . __context__ = scrub_exception ( exception . __context__ , scrub_message , prefix , keep_message , allow_list , _seen , ) keep = keep_message or is_exception_allowed ( exception , allow_list ) transformer = _attribute_transformer ( prefix , scrub_message , keep ) for attr in dir ( exception ): if attr and not attr . startswith ( \"__\" ): try : value = getattr ( exception , attr ) except AttributeError : # In some cases, e.g. FileNotFoundError, there are attributes # which show up in dir(e), but for which an AttributeError is # thrown when attempting to access the value. See, e.g.: # https://stackoverflow.com/q/47775772 . continue try : # If unable to transform or set the attribute, replace the # entire exception since the attribute value is readable, but # we are unable to scrub it. new_value = transformer ( value ) setattr ( exception , attr , new_value ) except BaseException as e : new_exception = PublicRuntimeError ( f \" { prefix } Obtained { type ( e ) . __name__ } when trying to scrub { attr } from { type ( exception ) . __name__ } \" # noqa: E501 ) new_exception . __cause__ = exception . __cause__ new_exception . __context__ = exception . __context__ exception = new_exception break return exception","title":"scrub_exception()"},{"location":"compliant_logging/logging/","text":"Logging Utilities around logging data which may or may not contain private content. CompliantLogger Subclass of the default logging class with an explicit category parameter on all logging methods. It will pass an extra param with prefix key (value depending on whether category is public or private) to the handlers. The default value for data category is PRIVATE for all methods. Implementation is inspired by: https://github.com/python/cpython/blob/3.8/Lib/logging/ init .py metric ( self , value , step = None , name = None , description = None , max_rows = 250 , category =< DataCategory . PRIVATE : 1 > ) Converts most datatypes into a metric and logs them to AML Metric for the RunContext (if available) or directly to log Note: Private Data will not be send to metrics! Parameters: Name Type Description Default value Any The value to log (can be vaex/pandas/spark dataframe, numpy array, list, dict, int/float) required step str | int Step value used for single value items. Defaults to None. None name str Name under which the metric should be logged. Defaults to None. None description str Description for the metric provided to the run context. Defaults to None. None max_rows int Defines the number of rows to batch table metrics (only required for table metrics). Defaults to 250. 250 category DataCategory Category of the data (logging to AML requires this to be set to PUBLIC explicitly). Defaults to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Source code in shrike/compliant_logging/logging.py def metric ( self , value , step = None , name = None , description = None , max_rows = 250 , category = DataCategory . PRIVATE , ): \"\"\" Converts most datatypes into a metric and logs them to AML Metric for the RunContext (if available) or directly to log Note: Private Data will not be send to metrics! Args: value (Any): The value to log (can be vaex/pandas/spark dataframe, numpy array, list, dict, int/float) step (str | int, optional): Step value used for single value items. Defaults to None. name (str, optional): Name under which the metric should be logged. Defaults to None. description (str, optional): Description for the metric provided to the run context. Defaults to None. max_rows (int, optional): Defines the number of rows to batch table metrics (only required for table metrics). Defaults to 250. category (DataCategory, optional): Category of the data (logging to AML requires this to be set to PUBLIC explicitly). Defaults to DataCategory.PRIVATE. \"\"\" # check for name if name is None : name = f \"metric_ { self . metric_count } \" self . metric_count += 1 # check for description if description is None : description = \"\" # retrieve AML Context run = self . _get_aml_context () # check if value provided if value is None : self . error ( f \"Value provided for metric { name } is None, skipping (step: { step } )\" ) return # check different data-types if isinstance ( value , ( float , int )): # log the data if run is not None and category == DataCategory . PUBLIC : if step : run . log ( name = name , value = value , description = description , step = step ) else : run . log ( name = name , value = value , description = description ) else : self . info ( f \"NumbericMetric | { name } : { step } | { value } \" , category = category , ) return # collect dataframes if is_vaex_dataframe ( value ): value = collect_vaex_dataframe ( value ) elif is_spark_dataframe ( value ): value = collect_spark_dataframe ( value ) elif is_pandas_dataframe ( value ): value = collect_pandas_dataframe ( value ) # log dictionary data if isinstance ( value , dict ): # check if values are present if len ( value ) == 0 : self . warning ( f \"Dictionary Value for Metric { name } is empty. Skipping.\" ) return # check the value types of the dict type_set = list ( set ([ type ( v ) for v in value . values ()])) # check for mixed types if len ( type_set ) > 1 : pass else : type_set = type_set [ 0 ] # check types if type_set == list : if run is not None and category == DataCategory . PUBLIC : run . log_table ( name , value , description ) else : # log the matrix manually col_names = \" | \" . join ( [ f \" { ( '' if col is None else col ) : 15 } \" for col in value . keys ()] ) header = f \"TableMetric | Index | { col_names } |\" self . info ( f \"TableMetric | { name } \" , category = category ) self . info ( header , category = category ) self . info ( \"-\" * len ( header ), category = category ) # generate the rows max_rows = max ([ len ( value [ col ]) for col in value ]) for i in range ( max_rows ): row_str = f \"TableMetric | { i : 05 } \" for key in value : col = value [ key ] col = col [ i ] if i < len ( col ) and col [ i ] else \"\" row_str += f \" | { str ( col ) : 15 } \" self . info ( row_str , category = category ) elif type_set in [ int , float ]: for key , val in value . items (): key = name + \"/\" + key self . metric ( val , step , key , description , category ) else : self . warning ( ( \"The provided dictionary for metric\" f \" { name } appears to be unstructured!\" ), category = category , ) return # collect list wise datatypes if is_numpy_array ( value ): value = numpy_array_to_list ( value ) if is_pandas_series ( value ): value = pandas_series_to_list ( value ) # log list data if isinstance ( value , ( list , tuple )): value = list ( value ) # check if values are present if len ( value ) == 0 : self . warning ( f \"List Value for Metric { name } is empty. Skipping.\" ) return # log data to run context if run is not None and category == DataCategory . PUBLIC : run . log_list ( name = name , value = value , description = description ) else : self . info ( f \"ListMetric | { name } | { value } \" ) return self . warning ( f \"Value { value } of the provided metric { name } has an unkown type\" , category = category , ) metric_accuracy_table ( self , name , value , description = None , col_predict = None , col_target = None , probability_thresholds = 5 , percentile_thresholds = [ 0.0 , 0.01 , 0.24 , 0.98 , 1.0 ], class_labels = None , category =< DataCategory . PRIVATE : 1 > ) Equivalent of the Run.log_accuracy_table function. Logs the data for an accuracy table to the metrics. In the dataframe case, the col_predict value has to contain the prediction probabilities for the target class! Note: Private Data will not be send to metrics! Parameters: Name Type Description Default value dict | table Either dicationary in AML defined format or table that provides accuracy values. required name str Name of the metric. Defaults to None. required description str Description of the metric. Defaults to None. None col_predict str | int Name or Id of the predicted probabilities for the target class. This is only required if DataFrame is passed. Defaults to None. None col_target str | int Name or id of the target value column. This is only required if DataFrame is passed. Defaults to None. None probability_thresholds list | int Either a list of thresholds or a number of evenly spaced threshold points. Defaults to 5. 5 percentile_thresholds list | int Either a list of thresholds or a number of evenly spaced threshold points. Defaults to a list. [0.0, 0.01, 0.24, 0.98, 1.0] category DataCategory Classification of the data category. Defaults to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Source code in shrike/compliant_logging/logging.py def metric_accuracy_table ( self , name , value , description = None , col_predict = None , col_target = None , probability_thresholds = 5 , percentile_thresholds = [ 0.0 , 0.01 , 0.24 , 0.98 , 1.0 ], class_labels = None , category = DataCategory . PRIVATE , ): \"\"\" Equivalent of the `Run.log_accuracy_table` function. Logs the data for an accuracy table to the metrics. In the dataframe case, the `col_predict` value has to contain the prediction probabilities for the **target** class! Note: Private Data will not be send to metrics! Args: value (dict | table): Either dicationary in AML defined format or table that provides accuracy values. name (str, optional): Name of the metric. Defaults to None. description (str, optional): Description of the metric. Defaults to None. col_predict (str | int, optional): Name or Id of the predicted probabilities for the target class. This is only required if DataFrame is passed. Defaults to None. col_target (str | int, optional): Name or id of the target value column. This is only required if DataFrame is passed. Defaults to None. probability_thresholds (list | int, optional): Either a list of thresholds or a number of evenly spaced threshold points. Defaults to 5. percentile_thresholds (list | int, optional): Either a list of thresholds or a number of evenly spaced threshold points. Defaults to a list. category (DataCategory, optional): Classification of the data category. Defaults to DataCategory.PRIVATE. \"\"\" # retrieve the context run = self . _get_aml_context () # convert data if not already pre-computed if not isinstance ( value , dict ) or \"schema_type\" not in value : # check the data if is_vaex_dataframe ( value ): value = collect_vaex_dataframe ( value ) if is_spark_dataframe ( value ): value = collect_spark_dataframe ( value ) if is_pandas_dataframe ( value ): value = collect_pandas_dataframe ( value ) # check if datatype matches if not isinstance ( value , dict ): raise PublicRuntimeError ( \"Unkown value-type passed to accuracy_table!\" ) # convert the data try : import pandas as pd # create the dataframe df = pd . DataFrame . from_dict ( value ) # column checks if None in [ col_predict , col_target ]: raise PublicRuntimeError ( \"If table is passed to accuracy_table it requires all \" + \"columns to be present!\" ) # check the class list (sort to make sure it is aligned) class_list = list ( df [ col_target ] . unique ()) class_list . sort () if class_labels is None : class_labels = class_list # compute ranges if isinstance ( probability_thresholds , int ): probability_thresholds = floating_range ( probability_thresholds ) if isinstance ( percentile_thresholds , int ): percentile_thresholds = floating_range ( percentile_thresholds ) # compute one-vs-rest labels for the class prob_tables = [] perc_tables = [] for cl in class_list : # compute the thresholds prob_tables . append ( self . _compute_truth_matrix ( df [ col_predict ], df [ col_target ], cl , probability_thresholds ) ) # compute per class percentiles cl_proba = ( df [ col_predict ] * ( df [ col_target ] == cl )) + ( ( 1 - df [ col_predict ]) * ( df [ col_target ] != cl ) ) cl_percentile = list ( cl_proba . quantile ( percentile_thresholds )) perc_tables . append ( self . _compute_truth_matrix ( df [ col_predict ], df [ col_target ], cl , cl_percentile ) ) # generate data value = { \"schema_type\" : \"accuracy_table\" , \"schema_version\" : \"1.0.1\" , \"data\" : { \"probability_tables\" : prob_tables , \"precentile_tables\" : perc_tables , \"probability_thresholds\" : probability_thresholds , \"percentile_thresholds\" : percentile_thresholds , \"class_labels\" : class_labels , }, } except Exception : raise PublicRuntimeError ( \"Unable to import pandas and parse the given data table! \" + \"Make sure that libraries are available and \" + \"correct data is passed.\" ) # log the data if category == DataCategory . PUBLIC and run is not None : run . log_accuracy_table ( name , value , description ) else : self . warning ( \"Logging Accuracy Tables to text is not yet implemented\" ) metric_confusion_matrix ( self , name , value , idx_true = None , idx_pred = None , labels = None , description = None , category =< DataCategory . PRIVATE : 1 > ) Equivalent of the Run.log_confusion_matrix function. Logs or generates a confusion matrix to the AML logs. Note: Private Data will not be send to metrics! Parameters: Name Type Description Default value dict | DataFrame Data to be used for the confusion_matrix required idx_true int | str Name or id of the target column. Defaults to None. None idx_pred int | str Name or id of the prediction column. Defaults to None. None labels list List of labels used for the rows. Defaults to None. None name str Name of the metric. Defaults to None. required description str Description of the metric. Defaults to None. None category DataCategory Classification of the data. Defaults to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Exceptions: Type Description PublicRuntimeError [description] PublicRuntimeError [description] Source code in shrike/compliant_logging/logging.py def metric_confusion_matrix ( self , name , value , idx_true = None , idx_pred = None , labels = None , description = None , category = DataCategory . PRIVATE , ): \"\"\" Equivalent of the `Run.log_confusion_matrix` function. Logs or generates a confusion matrix to the AML logs. Note: Private Data will not be send to metrics! Args: value (dict | DataFrame): Data to be used for the confusion_matrix idx_true (int | str, optional): Name or id of the target column. Defaults to None. idx_pred (int | str, optional): Name or id of the prediction column. Defaults to None. labels (list, optional): List of labels used for the rows. Defaults to None. name (str, optional): Name of the metric. Defaults to None. description (str, optional): Description of the metric. Defaults to None. category (DataCategory, optional): Classification of the data. Defaults to DataCategory.PRIVATE. Raises: PublicRuntimeError: [description] PublicRuntimeError: [description] \"\"\" # retrieve the context run = self . _get_aml_context () # convert data if not already pre-computed if ( not isinstance ( value , dict ) or \"schema_type\" not in value or \"schema_version\" not in value ): # check the data if is_vaex_dataframe ( value ): value = collect_vaex_dataframe ( value ) if is_spark_dataframe ( value ): value = collect_spark_dataframe ( value ) if is_pandas_dataframe ( value ): value = collect_pandas_dataframe ( value ) # check if datatype matches if not isinstance ( value , dict ): raise PublicRuntimeError ( \"Unkown value-type passed to Run.log_confusion_matrix!\" ) # convert the data try : # try to import libs import numpy as np from sklearn.metrics import confusion_matrix # update row names if isinstance ( idx_true , str ): idx_true = list ( value . keys ()) . index ( idx_true ) if isinstance ( idx_pred , str ): idx_pred = list ( value . keys ()) . index ( idx_pred ) # retrieve left right val_true , val_pred = None , None value = np . array ( list ( value . values ())) val_true = value [ idx_true ] val_pred = value [ idx_pred ] # compute matrix mat = confusion_matrix ( val_true , val_pred ) # generate labels as distincts if labels is None : labels = np . unique ( val_true ) # generate the dict value = { \"schema_type\" : \"confusion_matrix\" , \"schema_version\" : \"1.0.0\" , \"data\" : { \"class_labels\" : labels , \"matrix\" : mat }, } except Exception : raise PublicRuntimeError ( \"Unable to import numpy & scikit and parse the given data table! \" + \"Make sure that libraries are available and correct \" + \"data is passed.\" ) # log the data if category == DataCategory . PUBLIC and run is not None : run . log_confusion_matrix ( name , value , description ) else : self . warning ( \"Logging Confusion Matrices to text is not yet implemented\" ) metric_image ( self , name = None , plot = None , path = None , description = None , category =< DataCategory . PRIVATE : 1 > ) Logs an image to the AML Metrics. Note that this is only possible for public data when AML Run context is available Note: Private Data will not be send to metrics! Parameters: Name Type Description Default plot pyplot.Plot The plot that should be logger None path str Optional Path to the image. Defaults to None. None name str Name of the image. Defaults to None. None description str Description of the metric. Defaults to None. None category DataCategory Category under which this image is logged Defaults to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Source code in shrike/compliant_logging/logging.py def metric_image ( self , name = None , plot = None , path = None , description = None , category = DataCategory . PRIVATE , ): \"\"\" Logs an image to the AML Metrics. Note that this is only possible for public data when AML Run context is available Note: Private Data will not be send to metrics! Args: plot (pyplot.Plot): The plot that should be logger path (str, optional): Optional Path to the image. Defaults to None. name (str, optional): Name of the image. Defaults to None. description (str, optional): Description of the metric. Defaults to None. category (DataCategory, optional): Category under which this image is logged Defaults to DataCategory.PRIVATE. \"\"\" # retrieve the run context run = self . _get_aml_context () # check if parameters are correct if category != DataCategory . PUBLIC : self . warning ( f \"Unable to log image metric { name } as private, skipping.\" ) return # check for name if name is None : name = f \"metric_ { self . metric_count } \" self . metric_count += 1 if description is None : description = \"\" # log the image run . log_image ( # type: ignore name = name , path = path , plot = plot , description = description ) metric_list ( self , name , value , description = None , category =< DataCategory . PRIVATE : 1 > ) Equivalent to the Run.log_list . Logs a list of values for a single metric. Note: Private Data will not be send to metrics! Parameters: Name Type Description Default value list List values to log required name str Name of the metric. Defaults to None. required description str Description of the metric. Defaults to None. None category DataCategory DataCategory to log the data as. Defaults to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Source code in shrike/compliant_logging/logging.py def metric_list ( self , name , value , description = None , category = DataCategory . PRIVATE ): \"\"\" Equivalent to the `Run.log_list`. Logs a list of values for a single metric. Note: Private Data will not be send to metrics! Args: value (list): List values to log name (str, optional): Name of the metric. Defaults to None. description (str, optional): Description of the metric. Defaults to None. category (DataCategory, optional): DataCategory to log the data as. Defaults to DataCategory.PRIVATE. \"\"\" self . metric ( value , name = name , description = description , category = category ) metric_predictions ( self , name , value , description = None , col_predict = None , col_target = None , bin_edges = 5 , category =< DataCategory . PRIVATE : 1 > ) Equivalent of Run.log_predictions function. This will log regression prediction histogram from dict or dataframe. Note: Private Data will not be send to metrics! For the dataframe case the prediction error is computed as the absolute difference between prediction and target. Parameters: Name Type Description Default name str Name of the metric required value dict | DataFrame The data to log required description str Description of the metric. Defaults to ''. None col_predict str | int Id or Name of the target column. Defaults to None. None bin_edges list List of edge boundaries for logging. Defaults to None. 5 category DataCategory Privacy Classification of the data. Defaults to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Exceptions: Type Description PublicRuntimeError If the data is not in the right format or required parameters are not passed. Source code in shrike/compliant_logging/logging.py def metric_predictions ( self , name , value , description = None , col_predict = None , col_target = None , bin_edges = 5 , category = DataCategory . PRIVATE , ): \"\"\" Equivalent of `Run.log_predictions` function. This will log regression prediction histogram from dict or dataframe. Note: Private Data will not be send to metrics! For the dataframe case the prediction error is computed as the absolute difference between prediction and target. Args: name (str): Name of the metric value (dict | DataFrame): The data to log description (str, optional): Description of the metric. Defaults to ''. col_predict (str | int, optional): Id or Name of the target column. Defaults to None. bin_edges (list, optional): List of edge boundaries for logging. Defaults to None. category (DataCategory, optional): Privacy Classification of the data. Defaults to DataCategory.PRIVATE. Raises: PublicRuntimeError: If the data is not in the right format or required parameters are not passed. \"\"\" # retrieve the context run = self . _get_aml_context () # convert data if not already pre-computed if ( not isinstance ( value , dict ) or \"schema_type\" not in value or \"schema_version\" not in value ): # check the data if is_vaex_dataframe ( value ): value = collect_vaex_dataframe ( value ) if is_spark_dataframe ( value ): value = collect_spark_dataframe ( value ) if is_pandas_dataframe ( value ): value = collect_pandas_dataframe ( value ) # check if datatype matches if not isinstance ( value , dict ): raise PublicRuntimeError ( \"Unkown value-type passed to predictions!\" ) # convert the data try : import pandas as pd # create the dataframe df = pd . DataFrame . from_dict ( value ) # column checks if None in [ col_predict , col_target ]: raise PublicRuntimeError ( \"The col_predict and col_target columns are both required.\" ) # compute edges automatically if isinstance ( bin_edges , int ): bin_edges = floating_range ( bin_edges ) # compute groupings in bins df [ \"bin\" ] = pd . cut ( df [ col_target ], bin_edges ) df [ \"error\" ] = ( df [ col_predict ] - df [ col_target ]) . abs () # generate data value = { \"schema_type\" : \"predictions\" , \"schema_version\" : \"1.0.0\" , \"data\" : { \"bin_averages\" : list ( df . groupby ( \"bin\" )[ col_target ] . mean ()), \"bin_errors\" : list ( df . groupby ( \"bin\" )[ \"error\" ] . sum ()), \"bin_counts\" : list ( df . groupby ( \"bin\" )[ col_target ] . count ()), \"bin_edges\" : bin_edges , }, } except Exception : raise PublicRuntimeError ( \"Unable to import pandas and parse the given data! \" + \"Make sure that libraries are available and correct \" + \"data is passed.\" ) # log the data if category == DataCategory . PUBLIC and run is not None : run . log_predictions ( name , value , description ) else : self . warning ( \"Logging Predictions to text is not yet implemented\" ) metric_residual ( self , name , value , description = None , col_predict = None , col_target = None , bin_edges = 5 , category =< DataCategory . PRIVATE : 1 > ) Equivalent on the Run.log_residuals functions. Logs residual values for a list of edges Note: Private Data will not be send to metrics! Parameters: Name Type Description Default name str Name of the metric required value dict | DataFrame Values to contain the residuals required description str Description of the dataframe. Defaults to ''. None col_target str Name of the target column (if value is a df). Defaults to None. None bin_edges list | int List of edges towards the bins. Defaults to 5. 5 category DataCategory Privacy Classification of the data. Defaults to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Exceptions: Type Description PublicRuntimeError Thrown when data is in unkown format or required params not provided Source code in shrike/compliant_logging/logging.py def metric_residual ( self , name , value , description = None , col_predict = None , col_target = None , bin_edges = 5 , category = DataCategory . PRIVATE , ): \"\"\" Equivalent on the `Run.log_residuals` functions. Logs residual values for a list of edges Note: Private Data will not be send to metrics! Args: name (str): Name of the metric value (dict | DataFrame): Values to contain the residuals description (str, optional): Description of the dataframe. Defaults to ''. col_target (str, optional): Name of the target column (if value is a df). Defaults to None. bin_edges (list | int, optional): List of edges towards the bins. Defaults to 5. category (DataCategory, optional): Privacy Classification of the data. Defaults to DataCategory.PRIVATE. Raises: PublicRuntimeError: Thrown when data is in unkown format or required params not provided \"\"\" # retrieve the context run = self . _get_aml_context () # convert data if not already pre-computed if ( not isinstance ( value , dict ) or \"schema_type\" not in value or \"schema_version\" not in value ): # check the data if is_vaex_dataframe ( value ): value = collect_vaex_dataframe ( value ) if is_spark_dataframe ( value ): value = collect_spark_dataframe ( value ) if is_pandas_dataframe ( value ): value = collect_pandas_dataframe ( value ) # check if datatype matches if not isinstance ( value , dict ): raise PublicRuntimeError ( \"Unkown value-type passed to Run.log_residuals()!\" ) # convert the data try : import pandas as pd # create the dataframe df = pd . DataFrame . from_dict ( value ) # column checks if None in [ col_predict , col_target ]: raise PublicRuntimeError ( \"The col_predict and col_target columns are both required.\" ) # check if bins should be generated automatically if isinstance ( bin_edges , int ): bin_edges = floating_range ( bin_edges ) # compute the values df [ \"residual\" ] = df [ col_predict ] - df [ col_target ] df [ \"bin\" ] = pd . cut ( df [ col_target ], bin_edges ) # generate data value = { \"schema_type\" : \"residuals\" , \"schema_version\" : \"1.0.0\" , \"data\" : { \"bin_edges\" : bin_edges , \"bin_counts\" : list ( df . groupby ( \"bin\" )[ \"residual\" ] . sum ()), }, } except Exception : raise PublicRuntimeError ( \"Unable to import pandas and parse the given data! \" + \"Make sure that libraries are available and correct \" + \"data is passed.\" ) # log the data if category == DataCategory . PUBLIC and run is not None : run . log_residuals ( name , value , description ) else : self . warning ( \"Logging Residuals to text is not yet implemented\" ) metric_row ( self , name , description = None , category =< DataCategory . PRIVATE : 1 > , ** kwargs ) Equivalent of the Run.log_row function. Logs a single row of a table to the metrics. Note: Private Data will not be send to metrics! Parameters: Name Type Description Default name str Name of the metric. required description str Description of the metric. None category DataCategory Classification of the data. Defaults to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Source code in shrike/compliant_logging/logging.py def metric_row ( self , name , description = None , category = DataCategory . PRIVATE , ** kwargs ): \"\"\" Equivalent of the `Run.log_row` function. Logs a single row of a table to the metrics. Note: Private Data will not be send to metrics! Args: name (str): Name of the metric. description (str): Description of the metric. category (DataCategory, optional): Classification of the data. Defaults to DataCategory.PRIVATE. \"\"\" # check run context run = self . _get_aml_context () # log the data if category == DataCategory . PUBLIC and run is not None : run . log_row ( name = name , description = description , ** kwargs ) else : row_str = f \"RowMetric | { name } | \" row_str += \" | \" . join ([ f \" { r } : { c } \" for r , c in kwargs . items ()]) self . info ( row_str , category = category ) metric_table ( self , name , value , description = None , category =< DataCategory . PRIVATE : 1 > ) Equivalent to the Run.log_table function. Logs a table in dict format {rows: [values]} to metrics. Note: Private Data will not be send to metrics! Parameters: Name Type Description Default name str Name of the metric. required value dict Dictionary representation of the table. required description str Description of the metric. Defaults to None. None category DataCategory Category to log the data. Default to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Source code in shrike/compliant_logging/logging.py def metric_table ( self , name , value , description = None , category = DataCategory . PRIVATE ): \"\"\" Equivalent to the `Run.log_table` function. Logs a table in dict format {rows: [values]} to metrics. Note: Private Data will not be send to metrics! Args: name (str): Name of the metric. value (dict): Dictionary representation of the table. description (str, optional): Description of the metric. Defaults to None. category (DataCategory, optional): Category to log the data. Default to DataCategory.PRIVATE. \"\"\" self . metric ( value = value , name = name , description = description , category = category ) metric_value ( self , name , value , description = None , step = None , category =< DataCategory . PRIVATE : 1 > ) Equivalent to the Run.log function. Logs a single value to a metric Note: Private Data will not be send to metrics! Parameters: Name Type Description Default name str name of the metric required value Any value to log required description str Description of the metric. Defaults to None. None step int Step of the current metric. Defaults to None. None category DataCategory Data category to make sure no data leaks. Defaults to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Source code in shrike/compliant_logging/logging.py def metric_value ( self , name , value , description = None , step = None , category = DataCategory . PRIVATE ): \"\"\" Equivalent to the `Run.log` function. Logs a single value to a metric Note: Private Data will not be send to metrics! Args: name (str): name of the metric value (Any): value to log description (str, optional): Description of the metric. Defaults to None. step (int, optional): Step of the current metric. Defaults to None. category (DataCategory, optional): Data category to make sure no data leaks. Defaults to DataCategory.PRIVATE. \"\"\" self . metric ( value , step , name , description , category = category ) enable_compliant_logging ( prefix = 'SystemLog:' , use_aml_metrics = False , ** kwargs ) The default format is logging.BASIC_FORMAT ( %(levelname)s:%(name)s:%(message)s ). All other kwargs are passed to logging.basicConfig . Sets the default logger class and root logger to be compliant. This means the format string %(prefix) will work. Set the format using the format kwarg. If running in Python >= 3.8, will attempt to add force=True to the kwargs for logging.basicConfig. After calling this method, use the kwarg category to pass in a value of DataCategory to denote data category. The default is PRIVATE . That is, if no changes are made to an existing set of log statements, the log output should be the same. The standard implementation of the logging API is a good reference: https://github.com/python/cpython/blob/3.9/Lib/logging/ init .py Source code in shrike/compliant_logging/logging.py def enable_compliant_logging ( prefix : str = \"SystemLog:\" , use_aml_metrics : bool = False , ** kwargs ) -> None : \"\"\" The default format is `logging.BASIC_FORMAT` (`%(levelname)s:%(name)s:%(message)s`). All other kwargs are passed to `logging.basicConfig`. Sets the default logger class and root logger to be compliant. This means the format string `%(prefix)` will work. Set the format using the `format` kwarg. If running in Python >= 3.8, will attempt to add `force=True` to the kwargs for logging.basicConfig. After calling this method, use the kwarg `category` to pass in a value of `DataCategory` to denote data category. The default is `PRIVATE`. That is, if no changes are made to an existing set of log statements, the log output should be the same. The standard implementation of the logging API is a good reference: https://github.com/python/cpython/blob/3.9/Lib/logging/__init__.py \"\"\" set_prefix ( prefix ) if \"format\" not in kwargs : kwargs [ \"format\" ] = f \"%(prefix)s { logging . BASIC_FORMAT } \" # Ensure that all loggers created via `logging.getLogger` are instances of # the `CompliantLogger` class. logging . setLoggerClass ( CompliantLogger ) if len ( logging . root . handlers ) > 0 : p = get_prefix () for line in _logging_basic_config_set_warning . splitlines (): print ( f \" { p }{ line } \" , file = sys . stderr ) if \"force\" not in kwargs and sys . version_info >= ( 3 , 8 ): kwargs [ \"force\" ] = True old_root = logging . root root = CompliantLogger ( logging . root . name , use_aml_metrics ) root . handlers = old_root . handlers logging . root = root logging . Logger . root = root # type: ignore logging . Logger . manager = logging . Manager ( root ) # type: ignore # https://github.com/kivy/kivy/issues/6733 logging . basicConfig ( ** kwargs ) enable_confidential_logging ( prefix = 'SystemLog:' , use_aml_metrics = False , ** kwargs ) This function is a duplicate of the function enable_compliant_logging . We encourage users to use enable_compliant_logging . Source code in shrike/compliant_logging/logging.py def enable_confidential_logging ( prefix : str = \"SystemLog:\" , use_aml_metrics : bool = False , ** kwargs ) -> None : \"\"\" This function is a duplicate of the function `enable_compliant_logging`. We encourage users to use `enable_compliant_logging`. \"\"\" print ( f \" { prefix } The function enable_confidential_logging() is on the way\" \" to deprecation. Please use enable_compliant_logging() instead.\" , file = sys . stderr , ) enable_compliant_logging ( prefix , use_aml_metrics , ** kwargs ) floating_range ( buckets ) Computes a equal distributed list of bucket thresholds Parameters: Name Type Description Default buckets int Number of buckets required Returns: Type Description List List of bucket thresholds of length buckets Source code in shrike/compliant_logging/logging.py def floating_range ( buckets ): \"\"\" Computes a equal distributed list of bucket thresholds Args: buckets (int): Number of buckets Returns: List: List of bucket thresholds of length buckets \"\"\" return [ x / 100 for x in list ( range ( 0 , 100 , int ( 100 / ( buckets - 1 )))) + [ 100 ]] get_aml_context () Obtains the AML Context Source code in shrike/compliant_logging/logging.py def get_aml_context (): \"\"\" Obtains the AML Context \"\"\" return _AML_RUN get_prefix () Obtain the current global prefix to use when logging public (non-private) data. Source code in shrike/compliant_logging/logging.py def get_prefix () -> Optional [ str ]: \"\"\" Obtain the current global prefix to use when logging public (non-private) data. \"\"\" return _PREFIX set_aml_context () Retrieves the AML Context, should be bundled in a try-catch. Source code in shrike/compliant_logging/logging.py def set_aml_context () -> None : \"\"\" Retrieves the AML Context, should be bundled in a try-catch. \"\"\" global _AML_RUN from azureml.core.run import Run _AML_RUN = Run . get_context () set_prefix ( prefix ) Set the global prefix to use when logging public (non-private) data. This method is thread-safe. Source code in shrike/compliant_logging/logging.py def set_prefix ( prefix : str ) -> None : \"\"\" Set the global prefix to use when logging public (non-private) data. This method is thread-safe. \"\"\" with _LOCK : global _PREFIX _PREFIX = prefix","title":"logging"},{"location":"compliant_logging/logging/#logging","text":"Utilities around logging data which may or may not contain private content.","title":"Logging"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.CompliantLogger","text":"Subclass of the default logging class with an explicit category parameter on all logging methods. It will pass an extra param with prefix key (value depending on whether category is public or private) to the handlers. The default value for data category is PRIVATE for all methods. Implementation is inspired by: https://github.com/python/cpython/blob/3.8/Lib/logging/ init .py","title":"CompliantLogger"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.CompliantLogger.metric","text":"Converts most datatypes into a metric and logs them to AML Metric for the RunContext (if available) or directly to log Note: Private Data will not be send to metrics! Parameters: Name Type Description Default value Any The value to log (can be vaex/pandas/spark dataframe, numpy array, list, dict, int/float) required step str | int Step value used for single value items. Defaults to None. None name str Name under which the metric should be logged. Defaults to None. None description str Description for the metric provided to the run context. Defaults to None. None max_rows int Defines the number of rows to batch table metrics (only required for table metrics). Defaults to 250. 250 category DataCategory Category of the data (logging to AML requires this to be set to PUBLIC explicitly). Defaults to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Source code in shrike/compliant_logging/logging.py def metric ( self , value , step = None , name = None , description = None , max_rows = 250 , category = DataCategory . PRIVATE , ): \"\"\" Converts most datatypes into a metric and logs them to AML Metric for the RunContext (if available) or directly to log Note: Private Data will not be send to metrics! Args: value (Any): The value to log (can be vaex/pandas/spark dataframe, numpy array, list, dict, int/float) step (str | int, optional): Step value used for single value items. Defaults to None. name (str, optional): Name under which the metric should be logged. Defaults to None. description (str, optional): Description for the metric provided to the run context. Defaults to None. max_rows (int, optional): Defines the number of rows to batch table metrics (only required for table metrics). Defaults to 250. category (DataCategory, optional): Category of the data (logging to AML requires this to be set to PUBLIC explicitly). Defaults to DataCategory.PRIVATE. \"\"\" # check for name if name is None : name = f \"metric_ { self . metric_count } \" self . metric_count += 1 # check for description if description is None : description = \"\" # retrieve AML Context run = self . _get_aml_context () # check if value provided if value is None : self . error ( f \"Value provided for metric { name } is None, skipping (step: { step } )\" ) return # check different data-types if isinstance ( value , ( float , int )): # log the data if run is not None and category == DataCategory . PUBLIC : if step : run . log ( name = name , value = value , description = description , step = step ) else : run . log ( name = name , value = value , description = description ) else : self . info ( f \"NumbericMetric | { name } : { step } | { value } \" , category = category , ) return # collect dataframes if is_vaex_dataframe ( value ): value = collect_vaex_dataframe ( value ) elif is_spark_dataframe ( value ): value = collect_spark_dataframe ( value ) elif is_pandas_dataframe ( value ): value = collect_pandas_dataframe ( value ) # log dictionary data if isinstance ( value , dict ): # check if values are present if len ( value ) == 0 : self . warning ( f \"Dictionary Value for Metric { name } is empty. Skipping.\" ) return # check the value types of the dict type_set = list ( set ([ type ( v ) for v in value . values ()])) # check for mixed types if len ( type_set ) > 1 : pass else : type_set = type_set [ 0 ] # check types if type_set == list : if run is not None and category == DataCategory . PUBLIC : run . log_table ( name , value , description ) else : # log the matrix manually col_names = \" | \" . join ( [ f \" { ( '' if col is None else col ) : 15 } \" for col in value . keys ()] ) header = f \"TableMetric | Index | { col_names } |\" self . info ( f \"TableMetric | { name } \" , category = category ) self . info ( header , category = category ) self . info ( \"-\" * len ( header ), category = category ) # generate the rows max_rows = max ([ len ( value [ col ]) for col in value ]) for i in range ( max_rows ): row_str = f \"TableMetric | { i : 05 } \" for key in value : col = value [ key ] col = col [ i ] if i < len ( col ) and col [ i ] else \"\" row_str += f \" | { str ( col ) : 15 } \" self . info ( row_str , category = category ) elif type_set in [ int , float ]: for key , val in value . items (): key = name + \"/\" + key self . metric ( val , step , key , description , category ) else : self . warning ( ( \"The provided dictionary for metric\" f \" { name } appears to be unstructured!\" ), category = category , ) return # collect list wise datatypes if is_numpy_array ( value ): value = numpy_array_to_list ( value ) if is_pandas_series ( value ): value = pandas_series_to_list ( value ) # log list data if isinstance ( value , ( list , tuple )): value = list ( value ) # check if values are present if len ( value ) == 0 : self . warning ( f \"List Value for Metric { name } is empty. Skipping.\" ) return # log data to run context if run is not None and category == DataCategory . PUBLIC : run . log_list ( name = name , value = value , description = description ) else : self . info ( f \"ListMetric | { name } | { value } \" ) return self . warning ( f \"Value { value } of the provided metric { name } has an unkown type\" , category = category , )","title":"metric()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.CompliantLogger.metric_accuracy_table","text":"Equivalent of the Run.log_accuracy_table function. Logs the data for an accuracy table to the metrics. In the dataframe case, the col_predict value has to contain the prediction probabilities for the target class! Note: Private Data will not be send to metrics! Parameters: Name Type Description Default value dict | table Either dicationary in AML defined format or table that provides accuracy values. required name str Name of the metric. Defaults to None. required description str Description of the metric. Defaults to None. None col_predict str | int Name or Id of the predicted probabilities for the target class. This is only required if DataFrame is passed. Defaults to None. None col_target str | int Name or id of the target value column. This is only required if DataFrame is passed. Defaults to None. None probability_thresholds list | int Either a list of thresholds or a number of evenly spaced threshold points. Defaults to 5. 5 percentile_thresholds list | int Either a list of thresholds or a number of evenly spaced threshold points. Defaults to a list. [0.0, 0.01, 0.24, 0.98, 1.0] category DataCategory Classification of the data category. Defaults to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Source code in shrike/compliant_logging/logging.py def metric_accuracy_table ( self , name , value , description = None , col_predict = None , col_target = None , probability_thresholds = 5 , percentile_thresholds = [ 0.0 , 0.01 , 0.24 , 0.98 , 1.0 ], class_labels = None , category = DataCategory . PRIVATE , ): \"\"\" Equivalent of the `Run.log_accuracy_table` function. Logs the data for an accuracy table to the metrics. In the dataframe case, the `col_predict` value has to contain the prediction probabilities for the **target** class! Note: Private Data will not be send to metrics! Args: value (dict | table): Either dicationary in AML defined format or table that provides accuracy values. name (str, optional): Name of the metric. Defaults to None. description (str, optional): Description of the metric. Defaults to None. col_predict (str | int, optional): Name or Id of the predicted probabilities for the target class. This is only required if DataFrame is passed. Defaults to None. col_target (str | int, optional): Name or id of the target value column. This is only required if DataFrame is passed. Defaults to None. probability_thresholds (list | int, optional): Either a list of thresholds or a number of evenly spaced threshold points. Defaults to 5. percentile_thresholds (list | int, optional): Either a list of thresholds or a number of evenly spaced threshold points. Defaults to a list. category (DataCategory, optional): Classification of the data category. Defaults to DataCategory.PRIVATE. \"\"\" # retrieve the context run = self . _get_aml_context () # convert data if not already pre-computed if not isinstance ( value , dict ) or \"schema_type\" not in value : # check the data if is_vaex_dataframe ( value ): value = collect_vaex_dataframe ( value ) if is_spark_dataframe ( value ): value = collect_spark_dataframe ( value ) if is_pandas_dataframe ( value ): value = collect_pandas_dataframe ( value ) # check if datatype matches if not isinstance ( value , dict ): raise PublicRuntimeError ( \"Unkown value-type passed to accuracy_table!\" ) # convert the data try : import pandas as pd # create the dataframe df = pd . DataFrame . from_dict ( value ) # column checks if None in [ col_predict , col_target ]: raise PublicRuntimeError ( \"If table is passed to accuracy_table it requires all \" + \"columns to be present!\" ) # check the class list (sort to make sure it is aligned) class_list = list ( df [ col_target ] . unique ()) class_list . sort () if class_labels is None : class_labels = class_list # compute ranges if isinstance ( probability_thresholds , int ): probability_thresholds = floating_range ( probability_thresholds ) if isinstance ( percentile_thresholds , int ): percentile_thresholds = floating_range ( percentile_thresholds ) # compute one-vs-rest labels for the class prob_tables = [] perc_tables = [] for cl in class_list : # compute the thresholds prob_tables . append ( self . _compute_truth_matrix ( df [ col_predict ], df [ col_target ], cl , probability_thresholds ) ) # compute per class percentiles cl_proba = ( df [ col_predict ] * ( df [ col_target ] == cl )) + ( ( 1 - df [ col_predict ]) * ( df [ col_target ] != cl ) ) cl_percentile = list ( cl_proba . quantile ( percentile_thresholds )) perc_tables . append ( self . _compute_truth_matrix ( df [ col_predict ], df [ col_target ], cl , cl_percentile ) ) # generate data value = { \"schema_type\" : \"accuracy_table\" , \"schema_version\" : \"1.0.1\" , \"data\" : { \"probability_tables\" : prob_tables , \"precentile_tables\" : perc_tables , \"probability_thresholds\" : probability_thresholds , \"percentile_thresholds\" : percentile_thresholds , \"class_labels\" : class_labels , }, } except Exception : raise PublicRuntimeError ( \"Unable to import pandas and parse the given data table! \" + \"Make sure that libraries are available and \" + \"correct data is passed.\" ) # log the data if category == DataCategory . PUBLIC and run is not None : run . log_accuracy_table ( name , value , description ) else : self . warning ( \"Logging Accuracy Tables to text is not yet implemented\" )","title":"metric_accuracy_table()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.CompliantLogger.metric_confusion_matrix","text":"Equivalent of the Run.log_confusion_matrix function. Logs or generates a confusion matrix to the AML logs. Note: Private Data will not be send to metrics! Parameters: Name Type Description Default value dict | DataFrame Data to be used for the confusion_matrix required idx_true int | str Name or id of the target column. Defaults to None. None idx_pred int | str Name or id of the prediction column. Defaults to None. None labels list List of labels used for the rows. Defaults to None. None name str Name of the metric. Defaults to None. required description str Description of the metric. Defaults to None. None category DataCategory Classification of the data. Defaults to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Exceptions: Type Description PublicRuntimeError [description] PublicRuntimeError [description] Source code in shrike/compliant_logging/logging.py def metric_confusion_matrix ( self , name , value , idx_true = None , idx_pred = None , labels = None , description = None , category = DataCategory . PRIVATE , ): \"\"\" Equivalent of the `Run.log_confusion_matrix` function. Logs or generates a confusion matrix to the AML logs. Note: Private Data will not be send to metrics! Args: value (dict | DataFrame): Data to be used for the confusion_matrix idx_true (int | str, optional): Name or id of the target column. Defaults to None. idx_pred (int | str, optional): Name or id of the prediction column. Defaults to None. labels (list, optional): List of labels used for the rows. Defaults to None. name (str, optional): Name of the metric. Defaults to None. description (str, optional): Description of the metric. Defaults to None. category (DataCategory, optional): Classification of the data. Defaults to DataCategory.PRIVATE. Raises: PublicRuntimeError: [description] PublicRuntimeError: [description] \"\"\" # retrieve the context run = self . _get_aml_context () # convert data if not already pre-computed if ( not isinstance ( value , dict ) or \"schema_type\" not in value or \"schema_version\" not in value ): # check the data if is_vaex_dataframe ( value ): value = collect_vaex_dataframe ( value ) if is_spark_dataframe ( value ): value = collect_spark_dataframe ( value ) if is_pandas_dataframe ( value ): value = collect_pandas_dataframe ( value ) # check if datatype matches if not isinstance ( value , dict ): raise PublicRuntimeError ( \"Unkown value-type passed to Run.log_confusion_matrix!\" ) # convert the data try : # try to import libs import numpy as np from sklearn.metrics import confusion_matrix # update row names if isinstance ( idx_true , str ): idx_true = list ( value . keys ()) . index ( idx_true ) if isinstance ( idx_pred , str ): idx_pred = list ( value . keys ()) . index ( idx_pred ) # retrieve left right val_true , val_pred = None , None value = np . array ( list ( value . values ())) val_true = value [ idx_true ] val_pred = value [ idx_pred ] # compute matrix mat = confusion_matrix ( val_true , val_pred ) # generate labels as distincts if labels is None : labels = np . unique ( val_true ) # generate the dict value = { \"schema_type\" : \"confusion_matrix\" , \"schema_version\" : \"1.0.0\" , \"data\" : { \"class_labels\" : labels , \"matrix\" : mat }, } except Exception : raise PublicRuntimeError ( \"Unable to import numpy & scikit and parse the given data table! \" + \"Make sure that libraries are available and correct \" + \"data is passed.\" ) # log the data if category == DataCategory . PUBLIC and run is not None : run . log_confusion_matrix ( name , value , description ) else : self . warning ( \"Logging Confusion Matrices to text is not yet implemented\" )","title":"metric_confusion_matrix()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.CompliantLogger.metric_image","text":"Logs an image to the AML Metrics. Note that this is only possible for public data when AML Run context is available Note: Private Data will not be send to metrics! Parameters: Name Type Description Default plot pyplot.Plot The plot that should be logger None path str Optional Path to the image. Defaults to None. None name str Name of the image. Defaults to None. None description str Description of the metric. Defaults to None. None category DataCategory Category under which this image is logged Defaults to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Source code in shrike/compliant_logging/logging.py def metric_image ( self , name = None , plot = None , path = None , description = None , category = DataCategory . PRIVATE , ): \"\"\" Logs an image to the AML Metrics. Note that this is only possible for public data when AML Run context is available Note: Private Data will not be send to metrics! Args: plot (pyplot.Plot): The plot that should be logger path (str, optional): Optional Path to the image. Defaults to None. name (str, optional): Name of the image. Defaults to None. description (str, optional): Description of the metric. Defaults to None. category (DataCategory, optional): Category under which this image is logged Defaults to DataCategory.PRIVATE. \"\"\" # retrieve the run context run = self . _get_aml_context () # check if parameters are correct if category != DataCategory . PUBLIC : self . warning ( f \"Unable to log image metric { name } as private, skipping.\" ) return # check for name if name is None : name = f \"metric_ { self . metric_count } \" self . metric_count += 1 if description is None : description = \"\" # log the image run . log_image ( # type: ignore name = name , path = path , plot = plot , description = description )","title":"metric_image()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.CompliantLogger.metric_list","text":"Equivalent to the Run.log_list . Logs a list of values for a single metric. Note: Private Data will not be send to metrics! Parameters: Name Type Description Default value list List values to log required name str Name of the metric. Defaults to None. required description str Description of the metric. Defaults to None. None category DataCategory DataCategory to log the data as. Defaults to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Source code in shrike/compliant_logging/logging.py def metric_list ( self , name , value , description = None , category = DataCategory . PRIVATE ): \"\"\" Equivalent to the `Run.log_list`. Logs a list of values for a single metric. Note: Private Data will not be send to metrics! Args: value (list): List values to log name (str, optional): Name of the metric. Defaults to None. description (str, optional): Description of the metric. Defaults to None. category (DataCategory, optional): DataCategory to log the data as. Defaults to DataCategory.PRIVATE. \"\"\" self . metric ( value , name = name , description = description , category = category )","title":"metric_list()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.CompliantLogger.metric_predictions","text":"Equivalent of Run.log_predictions function. This will log regression prediction histogram from dict or dataframe. Note: Private Data will not be send to metrics! For the dataframe case the prediction error is computed as the absolute difference between prediction and target. Parameters: Name Type Description Default name str Name of the metric required value dict | DataFrame The data to log required description str Description of the metric. Defaults to ''. None col_predict str | int Id or Name of the target column. Defaults to None. None bin_edges list List of edge boundaries for logging. Defaults to None. 5 category DataCategory Privacy Classification of the data. Defaults to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Exceptions: Type Description PublicRuntimeError If the data is not in the right format or required parameters are not passed. Source code in shrike/compliant_logging/logging.py def metric_predictions ( self , name , value , description = None , col_predict = None , col_target = None , bin_edges = 5 , category = DataCategory . PRIVATE , ): \"\"\" Equivalent of `Run.log_predictions` function. This will log regression prediction histogram from dict or dataframe. Note: Private Data will not be send to metrics! For the dataframe case the prediction error is computed as the absolute difference between prediction and target. Args: name (str): Name of the metric value (dict | DataFrame): The data to log description (str, optional): Description of the metric. Defaults to ''. col_predict (str | int, optional): Id or Name of the target column. Defaults to None. bin_edges (list, optional): List of edge boundaries for logging. Defaults to None. category (DataCategory, optional): Privacy Classification of the data. Defaults to DataCategory.PRIVATE. Raises: PublicRuntimeError: If the data is not in the right format or required parameters are not passed. \"\"\" # retrieve the context run = self . _get_aml_context () # convert data if not already pre-computed if ( not isinstance ( value , dict ) or \"schema_type\" not in value or \"schema_version\" not in value ): # check the data if is_vaex_dataframe ( value ): value = collect_vaex_dataframe ( value ) if is_spark_dataframe ( value ): value = collect_spark_dataframe ( value ) if is_pandas_dataframe ( value ): value = collect_pandas_dataframe ( value ) # check if datatype matches if not isinstance ( value , dict ): raise PublicRuntimeError ( \"Unkown value-type passed to predictions!\" ) # convert the data try : import pandas as pd # create the dataframe df = pd . DataFrame . from_dict ( value ) # column checks if None in [ col_predict , col_target ]: raise PublicRuntimeError ( \"The col_predict and col_target columns are both required.\" ) # compute edges automatically if isinstance ( bin_edges , int ): bin_edges = floating_range ( bin_edges ) # compute groupings in bins df [ \"bin\" ] = pd . cut ( df [ col_target ], bin_edges ) df [ \"error\" ] = ( df [ col_predict ] - df [ col_target ]) . abs () # generate data value = { \"schema_type\" : \"predictions\" , \"schema_version\" : \"1.0.0\" , \"data\" : { \"bin_averages\" : list ( df . groupby ( \"bin\" )[ col_target ] . mean ()), \"bin_errors\" : list ( df . groupby ( \"bin\" )[ \"error\" ] . sum ()), \"bin_counts\" : list ( df . groupby ( \"bin\" )[ col_target ] . count ()), \"bin_edges\" : bin_edges , }, } except Exception : raise PublicRuntimeError ( \"Unable to import pandas and parse the given data! \" + \"Make sure that libraries are available and correct \" + \"data is passed.\" ) # log the data if category == DataCategory . PUBLIC and run is not None : run . log_predictions ( name , value , description ) else : self . warning ( \"Logging Predictions to text is not yet implemented\" )","title":"metric_predictions()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.CompliantLogger.metric_residual","text":"Equivalent on the Run.log_residuals functions. Logs residual values for a list of edges Note: Private Data will not be send to metrics! Parameters: Name Type Description Default name str Name of the metric required value dict | DataFrame Values to contain the residuals required description str Description of the dataframe. Defaults to ''. None col_target str Name of the target column (if value is a df). Defaults to None. None bin_edges list | int List of edges towards the bins. Defaults to 5. 5 category DataCategory Privacy Classification of the data. Defaults to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Exceptions: Type Description PublicRuntimeError Thrown when data is in unkown format or required params not provided Source code in shrike/compliant_logging/logging.py def metric_residual ( self , name , value , description = None , col_predict = None , col_target = None , bin_edges = 5 , category = DataCategory . PRIVATE , ): \"\"\" Equivalent on the `Run.log_residuals` functions. Logs residual values for a list of edges Note: Private Data will not be send to metrics! Args: name (str): Name of the metric value (dict | DataFrame): Values to contain the residuals description (str, optional): Description of the dataframe. Defaults to ''. col_target (str, optional): Name of the target column (if value is a df). Defaults to None. bin_edges (list | int, optional): List of edges towards the bins. Defaults to 5. category (DataCategory, optional): Privacy Classification of the data. Defaults to DataCategory.PRIVATE. Raises: PublicRuntimeError: Thrown when data is in unkown format or required params not provided \"\"\" # retrieve the context run = self . _get_aml_context () # convert data if not already pre-computed if ( not isinstance ( value , dict ) or \"schema_type\" not in value or \"schema_version\" not in value ): # check the data if is_vaex_dataframe ( value ): value = collect_vaex_dataframe ( value ) if is_spark_dataframe ( value ): value = collect_spark_dataframe ( value ) if is_pandas_dataframe ( value ): value = collect_pandas_dataframe ( value ) # check if datatype matches if not isinstance ( value , dict ): raise PublicRuntimeError ( \"Unkown value-type passed to Run.log_residuals()!\" ) # convert the data try : import pandas as pd # create the dataframe df = pd . DataFrame . from_dict ( value ) # column checks if None in [ col_predict , col_target ]: raise PublicRuntimeError ( \"The col_predict and col_target columns are both required.\" ) # check if bins should be generated automatically if isinstance ( bin_edges , int ): bin_edges = floating_range ( bin_edges ) # compute the values df [ \"residual\" ] = df [ col_predict ] - df [ col_target ] df [ \"bin\" ] = pd . cut ( df [ col_target ], bin_edges ) # generate data value = { \"schema_type\" : \"residuals\" , \"schema_version\" : \"1.0.0\" , \"data\" : { \"bin_edges\" : bin_edges , \"bin_counts\" : list ( df . groupby ( \"bin\" )[ \"residual\" ] . sum ()), }, } except Exception : raise PublicRuntimeError ( \"Unable to import pandas and parse the given data! \" + \"Make sure that libraries are available and correct \" + \"data is passed.\" ) # log the data if category == DataCategory . PUBLIC and run is not None : run . log_residuals ( name , value , description ) else : self . warning ( \"Logging Residuals to text is not yet implemented\" )","title":"metric_residual()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.CompliantLogger.metric_row","text":"Equivalent of the Run.log_row function. Logs a single row of a table to the metrics. Note: Private Data will not be send to metrics! Parameters: Name Type Description Default name str Name of the metric. required description str Description of the metric. None category DataCategory Classification of the data. Defaults to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Source code in shrike/compliant_logging/logging.py def metric_row ( self , name , description = None , category = DataCategory . PRIVATE , ** kwargs ): \"\"\" Equivalent of the `Run.log_row` function. Logs a single row of a table to the metrics. Note: Private Data will not be send to metrics! Args: name (str): Name of the metric. description (str): Description of the metric. category (DataCategory, optional): Classification of the data. Defaults to DataCategory.PRIVATE. \"\"\" # check run context run = self . _get_aml_context () # log the data if category == DataCategory . PUBLIC and run is not None : run . log_row ( name = name , description = description , ** kwargs ) else : row_str = f \"RowMetric | { name } | \" row_str += \" | \" . join ([ f \" { r } : { c } \" for r , c in kwargs . items ()]) self . info ( row_str , category = category )","title":"metric_row()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.CompliantLogger.metric_table","text":"Equivalent to the Run.log_table function. Logs a table in dict format {rows: [values]} to metrics. Note: Private Data will not be send to metrics! Parameters: Name Type Description Default name str Name of the metric. required value dict Dictionary representation of the table. required description str Description of the metric. Defaults to None. None category DataCategory Category to log the data. Default to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Source code in shrike/compliant_logging/logging.py def metric_table ( self , name , value , description = None , category = DataCategory . PRIVATE ): \"\"\" Equivalent to the `Run.log_table` function. Logs a table in dict format {rows: [values]} to metrics. Note: Private Data will not be send to metrics! Args: name (str): Name of the metric. value (dict): Dictionary representation of the table. description (str, optional): Description of the metric. Defaults to None. category (DataCategory, optional): Category to log the data. Default to DataCategory.PRIVATE. \"\"\" self . metric ( value = value , name = name , description = description , category = category )","title":"metric_table()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.CompliantLogger.metric_value","text":"Equivalent to the Run.log function. Logs a single value to a metric Note: Private Data will not be send to metrics! Parameters: Name Type Description Default name str name of the metric required value Any value to log required description str Description of the metric. Defaults to None. None step int Step of the current metric. Defaults to None. None category DataCategory Data category to make sure no data leaks. Defaults to DataCategory.PRIVATE. <DataCategory.PRIVATE: 1> Source code in shrike/compliant_logging/logging.py def metric_value ( self , name , value , description = None , step = None , category = DataCategory . PRIVATE ): \"\"\" Equivalent to the `Run.log` function. Logs a single value to a metric Note: Private Data will not be send to metrics! Args: name (str): name of the metric value (Any): value to log description (str, optional): Description of the metric. Defaults to None. step (int, optional): Step of the current metric. Defaults to None. category (DataCategory, optional): Data category to make sure no data leaks. Defaults to DataCategory.PRIVATE. \"\"\" self . metric ( value , step , name , description , category = category )","title":"metric_value()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.enable_compliant_logging","text":"The default format is logging.BASIC_FORMAT ( %(levelname)s:%(name)s:%(message)s ). All other kwargs are passed to logging.basicConfig . Sets the default logger class and root logger to be compliant. This means the format string %(prefix) will work. Set the format using the format kwarg. If running in Python >= 3.8, will attempt to add force=True to the kwargs for logging.basicConfig. After calling this method, use the kwarg category to pass in a value of DataCategory to denote data category. The default is PRIVATE . That is, if no changes are made to an existing set of log statements, the log output should be the same. The standard implementation of the logging API is a good reference: https://github.com/python/cpython/blob/3.9/Lib/logging/ init .py Source code in shrike/compliant_logging/logging.py def enable_compliant_logging ( prefix : str = \"SystemLog:\" , use_aml_metrics : bool = False , ** kwargs ) -> None : \"\"\" The default format is `logging.BASIC_FORMAT` (`%(levelname)s:%(name)s:%(message)s`). All other kwargs are passed to `logging.basicConfig`. Sets the default logger class and root logger to be compliant. This means the format string `%(prefix)` will work. Set the format using the `format` kwarg. If running in Python >= 3.8, will attempt to add `force=True` to the kwargs for logging.basicConfig. After calling this method, use the kwarg `category` to pass in a value of `DataCategory` to denote data category. The default is `PRIVATE`. That is, if no changes are made to an existing set of log statements, the log output should be the same. The standard implementation of the logging API is a good reference: https://github.com/python/cpython/blob/3.9/Lib/logging/__init__.py \"\"\" set_prefix ( prefix ) if \"format\" not in kwargs : kwargs [ \"format\" ] = f \"%(prefix)s { logging . BASIC_FORMAT } \" # Ensure that all loggers created via `logging.getLogger` are instances of # the `CompliantLogger` class. logging . setLoggerClass ( CompliantLogger ) if len ( logging . root . handlers ) > 0 : p = get_prefix () for line in _logging_basic_config_set_warning . splitlines (): print ( f \" { p }{ line } \" , file = sys . stderr ) if \"force\" not in kwargs and sys . version_info >= ( 3 , 8 ): kwargs [ \"force\" ] = True old_root = logging . root root = CompliantLogger ( logging . root . name , use_aml_metrics ) root . handlers = old_root . handlers logging . root = root logging . Logger . root = root # type: ignore logging . Logger . manager = logging . Manager ( root ) # type: ignore # https://github.com/kivy/kivy/issues/6733 logging . basicConfig ( ** kwargs )","title":"enable_compliant_logging()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.enable_confidential_logging","text":"This function is a duplicate of the function enable_compliant_logging . We encourage users to use enable_compliant_logging . Source code in shrike/compliant_logging/logging.py def enable_confidential_logging ( prefix : str = \"SystemLog:\" , use_aml_metrics : bool = False , ** kwargs ) -> None : \"\"\" This function is a duplicate of the function `enable_compliant_logging`. We encourage users to use `enable_compliant_logging`. \"\"\" print ( f \" { prefix } The function enable_confidential_logging() is on the way\" \" to deprecation. Please use enable_compliant_logging() instead.\" , file = sys . stderr , ) enable_compliant_logging ( prefix , use_aml_metrics , ** kwargs )","title":"enable_confidential_logging()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.floating_range","text":"Computes a equal distributed list of bucket thresholds Parameters: Name Type Description Default buckets int Number of buckets required Returns: Type Description List List of bucket thresholds of length buckets Source code in shrike/compliant_logging/logging.py def floating_range ( buckets ): \"\"\" Computes a equal distributed list of bucket thresholds Args: buckets (int): Number of buckets Returns: List: List of bucket thresholds of length buckets \"\"\" return [ x / 100 for x in list ( range ( 0 , 100 , int ( 100 / ( buckets - 1 )))) + [ 100 ]]","title":"floating_range()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.get_aml_context","text":"Obtains the AML Context Source code in shrike/compliant_logging/logging.py def get_aml_context (): \"\"\" Obtains the AML Context \"\"\" return _AML_RUN","title":"get_aml_context()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.get_prefix","text":"Obtain the current global prefix to use when logging public (non-private) data. Source code in shrike/compliant_logging/logging.py def get_prefix () -> Optional [ str ]: \"\"\" Obtain the current global prefix to use when logging public (non-private) data. \"\"\" return _PREFIX","title":"get_prefix()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.set_aml_context","text":"Retrieves the AML Context, should be bundled in a try-catch. Source code in shrike/compliant_logging/logging.py def set_aml_context () -> None : \"\"\" Retrieves the AML Context, should be bundled in a try-catch. \"\"\" global _AML_RUN from azureml.core.run import Run _AML_RUN = Run . get_context ()","title":"set_aml_context()"},{"location":"compliant_logging/logging/#shrike.compliant_logging.logging.set_prefix","text":"Set the global prefix to use when logging public (non-private) data. This method is thread-safe. Source code in shrike/compliant_logging/logging.py def set_prefix ( prefix : str ) -> None : \"\"\" Set the global prefix to use when logging public (non-private) data. This method is thread-safe. \"\"\" with _LOCK : global _PREFIX _PREFIX = prefix","title":"set_prefix()"},{"location":"compliant_logging/rfc-logging/","text":"Logging in Compliant ML Owner Approvers Participants Daniel Miller Jeff Omhover AML DS Team In many corporations, data scientists are working to build and train machine learning models under extremely strict compliance and privacy requirements. These can include: Being unable to directly access or view the customer data used to train a model. Being unable to run unsigned code against customer data, except possibly in specialized compute clusters with no network access and other hardening in place. All training logs removed, except possibly those starting with some fixed prefix (e.g., SystemLog: ). Machine learning is already a hard problem \u2014 building and training models under these constraints is even more difficult. This RFC ( R equest F or C omment) proposes the code patterns and expected behavior of a to-be-written logging utility for compliant machine learning. It begins by outlining the requirements this utility would need to satisfy, then gives a concrete proposal with sample code. It proceeds to outline some alternatives, along with known risks of the proposal. The following topics are out of scope for this RFC: How to implement the proposed behavior. It considers only usage patterns and the intended behavior of the logging utility library. Compliant argument parsing. Compliant exception handling (how to keep and prefix the stack trace and exception type, while scrubbing exception messages). Languages besides Python. Requirements Every call to log a message should be forced to include information on whether the message is \"safe\" (does not contain any customer or sensitive data). If the message is safe, the log line should be mutated to contain a configurable prefix (e.g., SystemLog: ). The default behavior of this library should be to not add the \"don't scrub\" prefix. It should only add that prefix if a conscious decision has been made, possibly by choosing a different method name or parameter value. This library should not rely on users naming variables \"well\". That is, if a user accidentally names a logger safeLogger , there should be no data leak if that logger was not pre-configured on some special way. Code which consumes this library should have the same \"look and feel\" as code consuming the Python standard library's logging module. :warning: This entire document relies on the assumption that the filtering mechanism looks for a specific prefix in log lines. Proposal I propose that the existing logging paradigm in Python be mostly unchanged, with the following small differences: Calls to getLogger must include a safe_prefix parameter. Open question: should this be more flexible, e.g. a callable or dictionary mapping data categories to prefixes or format strings? Calls to logger.* have a new optional parameter taking enum values. The default value is CONTAINS_PRIVATE_DATA , i.e. if this parameter is not explicitly provided the \"safe\" prefix will not be added. Here is some sample code following this proposal. import argparse from shrike.compliant_logging import logging from shrike.compliant_logging.constants import DataCategory def main ( logger ): # HAS a prefix. logger . warning ( 'safe message' , category = DataCategory . ONLY_PUBLIC_DATA ) # DOES NOT have a prefix. logger . warning ( 'unsafe message' ) if __name__ == '__main__' : parser = argparse . ArgumentParser () parser . add_argument ( '--prefix' , default = 'SystemLog:' ) args = parser . parse_args () # logger will be an instance of a subclass of the logging.Logger class. logger = logging . getLogger ( __name__ , args . prefix ) main ( logger ) Open question: should the compliant logging.getLogger method throw an exception if the non-compliant logging namespace is in scope? Alternatives Global log function Global log function. Not just prefix, more complicated \"mutation\" logic. Hash unsafe log lines Provide option to hash unsafe log lines. Not compliant. Other option, record their length Unsafe log message of length 23 Write public logs to pre-specified location Instead of adding a predetermined prefix to \"public\" logs, depending on the log filtering / scrubbing mechanisms, another alternative would be to write public logs to a specific file. Risks Abuse This logging utility will not prevent malicious abuse. That is, there is no way from the code to stop someone from writing a line like this. logger . info ( 'private data' , category = DataCategory . ONLY_PUBLIC_DATA ) However, this risk is not new, nor does it arise uniquely because of the proposed library. There is already nothing to prevent a malicious actor from writing this line print ( 'SystemLog: private data' ) Compliant machine learning in this context involves an element of trust, i.e. it is not designed or intended to stop malicious actors. We do not attempt to mitigate this risk by filtering out specific types of objects from \"public\" logs. Standard Python types like str and pandas.DataFrame can easily contain sensitive customer data. If we exclude those from logging, we will be excluding nearly all helpful logs.","title":"Logging design"},{"location":"compliant_logging/rfc-logging/#logging-in-compliant-ml","text":"Owner Approvers Participants Daniel Miller Jeff Omhover AML DS Team In many corporations, data scientists are working to build and train machine learning models under extremely strict compliance and privacy requirements. These can include: Being unable to directly access or view the customer data used to train a model. Being unable to run unsigned code against customer data, except possibly in specialized compute clusters with no network access and other hardening in place. All training logs removed, except possibly those starting with some fixed prefix (e.g., SystemLog: ). Machine learning is already a hard problem \u2014 building and training models under these constraints is even more difficult. This RFC ( R equest F or C omment) proposes the code patterns and expected behavior of a to-be-written logging utility for compliant machine learning. It begins by outlining the requirements this utility would need to satisfy, then gives a concrete proposal with sample code. It proceeds to outline some alternatives, along with known risks of the proposal. The following topics are out of scope for this RFC: How to implement the proposed behavior. It considers only usage patterns and the intended behavior of the logging utility library. Compliant argument parsing. Compliant exception handling (how to keep and prefix the stack trace and exception type, while scrubbing exception messages). Languages besides Python.","title":"Logging in Compliant ML"},{"location":"compliant_logging/rfc-logging/#requirements","text":"Every call to log a message should be forced to include information on whether the message is \"safe\" (does not contain any customer or sensitive data). If the message is safe, the log line should be mutated to contain a configurable prefix (e.g., SystemLog: ). The default behavior of this library should be to not add the \"don't scrub\" prefix. It should only add that prefix if a conscious decision has been made, possibly by choosing a different method name or parameter value. This library should not rely on users naming variables \"well\". That is, if a user accidentally names a logger safeLogger , there should be no data leak if that logger was not pre-configured on some special way. Code which consumes this library should have the same \"look and feel\" as code consuming the Python standard library's logging module. :warning: This entire document relies on the assumption that the filtering mechanism looks for a specific prefix in log lines.","title":"Requirements"},{"location":"compliant_logging/rfc-logging/#proposal","text":"I propose that the existing logging paradigm in Python be mostly unchanged, with the following small differences: Calls to getLogger must include a safe_prefix parameter. Open question: should this be more flexible, e.g. a callable or dictionary mapping data categories to prefixes or format strings? Calls to logger.* have a new optional parameter taking enum values. The default value is CONTAINS_PRIVATE_DATA , i.e. if this parameter is not explicitly provided the \"safe\" prefix will not be added. Here is some sample code following this proposal. import argparse from shrike.compliant_logging import logging from shrike.compliant_logging.constants import DataCategory def main ( logger ): # HAS a prefix. logger . warning ( 'safe message' , category = DataCategory . ONLY_PUBLIC_DATA ) # DOES NOT have a prefix. logger . warning ( 'unsafe message' ) if __name__ == '__main__' : parser = argparse . ArgumentParser () parser . add_argument ( '--prefix' , default = 'SystemLog:' ) args = parser . parse_args () # logger will be an instance of a subclass of the logging.Logger class. logger = logging . getLogger ( __name__ , args . prefix ) main ( logger ) Open question: should the compliant logging.getLogger method throw an exception if the non-compliant logging namespace is in scope?","title":"Proposal"},{"location":"compliant_logging/rfc-logging/#alternatives","text":"","title":"Alternatives"},{"location":"compliant_logging/rfc-logging/#global-log-function","text":"Global log function. Not just prefix, more complicated \"mutation\" logic.","title":"Global log function"},{"location":"compliant_logging/rfc-logging/#hash-unsafe-log-lines","text":"Provide option to hash unsafe log lines. Not compliant. Other option, record their length Unsafe log message of length 23","title":"Hash unsafe log lines"},{"location":"compliant_logging/rfc-logging/#write-public-logs-to-pre-specified-location","text":"Instead of adding a predetermined prefix to \"public\" logs, depending on the log filtering / scrubbing mechanisms, another alternative would be to write public logs to a specific file.","title":"Write public logs to pre-specified location"},{"location":"compliant_logging/rfc-logging/#risks","text":"","title":"Risks"},{"location":"compliant_logging/rfc-logging/#abuse","text":"This logging utility will not prevent malicious abuse. That is, there is no way from the code to stop someone from writing a line like this. logger . info ( 'private data' , category = DataCategory . ONLY_PUBLIC_DATA ) However, this risk is not new, nor does it arise uniquely because of the proposed library. There is already nothing to prevent a malicious actor from writing this line print ( 'SystemLog: private data' ) Compliant machine learning in this context involves an element of trust, i.e. it is not designed or intended to stop malicious actors. We do not attempt to mitigate this risk by filtering out specific types of objects from \"public\" logs. Standard Python types like str and pandas.DataFrame can easily contain sensitive customer data. If we exclude those from logging, we will be excluding nearly all helpful logs.","title":"Abuse"},{"location":"pipeline/aml-connect/","text":"AML Connect Helper code for connecting to AzureML and sharing one workspace accross code. add_cli_args ( parser ) Adds parser arguments for connecting to AzureML Parameters: Name Type Description Default parser argparse.ArgumentParser parser to add AzureML arguments to required Returns: Type Description argparse.ArgumentParser that same parser Source code in shrike/pipeline/aml_connect.py def add_cli_args ( parser ): \"\"\"Adds parser arguments for connecting to AzureML Args: parser (argparse.ArgumentParser): parser to add AzureML arguments to Returns: argparse.ArgumentParser: that same parser \"\"\" parser . add_argument ( \"--aml-subscription-id\" , dest = \"aml_subscription_id\" , type = str , required = False , help = \"\" , ) parser . add_argument ( \"--aml-resource-group\" , dest = \"aml_resource_group\" , type = str , required = False , help = \"\" , ) parser . add_argument ( \"--aml-workspace\" , dest = \"aml_workspace_name\" , type = str , required = False , help = \"\" ) parser . add_argument ( \"--aml-config\" , dest = \"aml_config\" , type = str , required = False , help = \"path to aml config.json file\" , ) parser . add_argument ( \"--aml-auth\" , dest = \"aml_auth\" , type = str , choices = [ \"azurecli\" , \"msi\" , \"interactive\" ], default = \"interactive\" , ) parser . add_argument ( \"--aml-tenant\" , dest = \"aml_tenant\" , type = str , default = None , help = \"tenant to use for auth (default: auto)\" , ) parser . add_argument ( \"--aml-force\" , dest = \"aml_force\" , type = lambda x : ( str ( x ) . lower () in [ \"true\" , \"1\" , \"yes\" ] ), # we want to use --aml-force True default = False , help = \"force tenant auth (default: False)\" , ) return parser azureml_connect ( ** kwargs ) Calls azureml_connect_cli with an argparse-like structure based on keyword arguments Source code in shrike/pipeline/aml_connect.py def azureml_connect ( ** kwargs ): \"\"\"Calls azureml_connect_cli with an argparse-like structure based on keyword arguments\"\"\" keys = [ \"aml_subscription_id\" , \"aml_resource_group\" , \"aml_workspace_name\" , \"aml_config\" , \"aml_auth\" , \"aml_tenant\" , \"aml_force\" , ] aml_args = dict ([( k , kwargs . get ( k )) for k in keys ]) azureml_argparse_tuple = namedtuple ( \"AzureMLArguments\" , aml_args ) aml_argparse = azureml_argparse_tuple ( ** aml_args ) return azureml_connect_cli ( aml_argparse ) azureml_connect_cli ( args ) Connects to an AzureML workspace. Parameters: Name Type Description Default args argparse.Namespace arguments to connect to AzureML required Returns: Type Description azureml.core.Workspace AzureML workspace Source code in shrike/pipeline/aml_connect.py def azureml_connect_cli ( args ): \"\"\"Connects to an AzureML workspace. Args: args (argparse.Namespace): arguments to connect to AzureML Returns: azureml.core.Workspace: AzureML workspace \"\"\" if args . aml_auth == \"msi\" : from azureml.core.authentication import MsiAuthentication auth = MsiAuthentication () elif args . aml_auth == \"azurecli\" : from azureml.core.authentication import AzureCliAuthentication auth = AzureCliAuthentication () elif args . aml_auth == \"interactive\" : from azureml.core.authentication import InteractiveLoginAuthentication auth = InteractiveLoginAuthentication ( tenant_id = args . aml_tenant , force = args . aml_force ) else : auth = None if args . aml_config : config_dir = os . path . dirname ( args . aml_config ) config_file_name = os . path . basename ( args . aml_config ) aml_ws = Workspace . from_config ( path = config_dir , _file_name = config_file_name , auth = auth ) else : aml_ws = Workspace . get ( subscription_id = args . aml_subscription_id , name = args . aml_workspace_name , resource_group = args . aml_resource_group , auth = auth , ) log . info ( \"Connected to workspace:\" ) log . info ( f \" \\t subscription: { aml_ws . subscription_id } \" ) log . info ( f \" \\t name: { aml_ws . name } \" ) log . info ( f \" \\t Azure region: { aml_ws . location } \" ) log . info ( f \" \\t resource group: { aml_ws . resource_group } \" ) return current_workspace ( aml_ws ) current_workspace ( workspace = None ) Sets/Gets the current AML workspace used all accross code. Parameters: Name Type Description Default workspace azureml.core.Workspace any given workspace None Returns: Type Description azureml.core.Workspace current (last) workspace given to current_workspace() Source code in shrike/pipeline/aml_connect.py def current_workspace ( workspace = None ): \"\"\"Sets/Gets the current AML workspace used all accross code. Args: workspace (azureml.core.Workspace): any given workspace Returns: azureml.core.Workspace: current (last) workspace given to current_workspace() \"\"\" global CURRENT_AML_WORKSPACE if workspace : CURRENT_AML_WORKSPACE = workspace if not CURRENT_AML_WORKSPACE : raise Exception ( \"You need to initialize current_workspace() with an AML workspace\" ) return CURRENT_AML_WORKSPACE main () Main function (for testing) Source code in shrike/pipeline/aml_connect.py def main (): \"\"\"Main function (for testing)\"\"\" parser = argparse . ArgumentParser ( description = __doc__ ) group = parser . add_argument_group ( \"AzureML connect arguments\" ) add_cli_args ( group ) args , unknown_args = parser . parse_known_args () if unknown_args : log . warning ( f \"You have provided unknown arguments { unknown_args } \" ) return azureml_connect_cli ( args )","title":"aml_connect"},{"location":"pipeline/aml-connect/#aml-connect","text":"Helper code for connecting to AzureML and sharing one workspace accross code.","title":"AML Connect"},{"location":"pipeline/aml-connect/#shrike.pipeline.aml_connect.add_cli_args","text":"Adds parser arguments for connecting to AzureML Parameters: Name Type Description Default parser argparse.ArgumentParser parser to add AzureML arguments to required Returns: Type Description argparse.ArgumentParser that same parser Source code in shrike/pipeline/aml_connect.py def add_cli_args ( parser ): \"\"\"Adds parser arguments for connecting to AzureML Args: parser (argparse.ArgumentParser): parser to add AzureML arguments to Returns: argparse.ArgumentParser: that same parser \"\"\" parser . add_argument ( \"--aml-subscription-id\" , dest = \"aml_subscription_id\" , type = str , required = False , help = \"\" , ) parser . add_argument ( \"--aml-resource-group\" , dest = \"aml_resource_group\" , type = str , required = False , help = \"\" , ) parser . add_argument ( \"--aml-workspace\" , dest = \"aml_workspace_name\" , type = str , required = False , help = \"\" ) parser . add_argument ( \"--aml-config\" , dest = \"aml_config\" , type = str , required = False , help = \"path to aml config.json file\" , ) parser . add_argument ( \"--aml-auth\" , dest = \"aml_auth\" , type = str , choices = [ \"azurecli\" , \"msi\" , \"interactive\" ], default = \"interactive\" , ) parser . add_argument ( \"--aml-tenant\" , dest = \"aml_tenant\" , type = str , default = None , help = \"tenant to use for auth (default: auto)\" , ) parser . add_argument ( \"--aml-force\" , dest = \"aml_force\" , type = lambda x : ( str ( x ) . lower () in [ \"true\" , \"1\" , \"yes\" ] ), # we want to use --aml-force True default = False , help = \"force tenant auth (default: False)\" , ) return parser","title":"add_cli_args()"},{"location":"pipeline/aml-connect/#shrike.pipeline.aml_connect.azureml_connect","text":"Calls azureml_connect_cli with an argparse-like structure based on keyword arguments Source code in shrike/pipeline/aml_connect.py def azureml_connect ( ** kwargs ): \"\"\"Calls azureml_connect_cli with an argparse-like structure based on keyword arguments\"\"\" keys = [ \"aml_subscription_id\" , \"aml_resource_group\" , \"aml_workspace_name\" , \"aml_config\" , \"aml_auth\" , \"aml_tenant\" , \"aml_force\" , ] aml_args = dict ([( k , kwargs . get ( k )) for k in keys ]) azureml_argparse_tuple = namedtuple ( \"AzureMLArguments\" , aml_args ) aml_argparse = azureml_argparse_tuple ( ** aml_args ) return azureml_connect_cli ( aml_argparse )","title":"azureml_connect()"},{"location":"pipeline/aml-connect/#shrike.pipeline.aml_connect.azureml_connect_cli","text":"Connects to an AzureML workspace. Parameters: Name Type Description Default args argparse.Namespace arguments to connect to AzureML required Returns: Type Description azureml.core.Workspace AzureML workspace Source code in shrike/pipeline/aml_connect.py def azureml_connect_cli ( args ): \"\"\"Connects to an AzureML workspace. Args: args (argparse.Namespace): arguments to connect to AzureML Returns: azureml.core.Workspace: AzureML workspace \"\"\" if args . aml_auth == \"msi\" : from azureml.core.authentication import MsiAuthentication auth = MsiAuthentication () elif args . aml_auth == \"azurecli\" : from azureml.core.authentication import AzureCliAuthentication auth = AzureCliAuthentication () elif args . aml_auth == \"interactive\" : from azureml.core.authentication import InteractiveLoginAuthentication auth = InteractiveLoginAuthentication ( tenant_id = args . aml_tenant , force = args . aml_force ) else : auth = None if args . aml_config : config_dir = os . path . dirname ( args . aml_config ) config_file_name = os . path . basename ( args . aml_config ) aml_ws = Workspace . from_config ( path = config_dir , _file_name = config_file_name , auth = auth ) else : aml_ws = Workspace . get ( subscription_id = args . aml_subscription_id , name = args . aml_workspace_name , resource_group = args . aml_resource_group , auth = auth , ) log . info ( \"Connected to workspace:\" ) log . info ( f \" \\t subscription: { aml_ws . subscription_id } \" ) log . info ( f \" \\t name: { aml_ws . name } \" ) log . info ( f \" \\t Azure region: { aml_ws . location } \" ) log . info ( f \" \\t resource group: { aml_ws . resource_group } \" ) return current_workspace ( aml_ws )","title":"azureml_connect_cli()"},{"location":"pipeline/aml-connect/#shrike.pipeline.aml_connect.current_workspace","text":"Sets/Gets the current AML workspace used all accross code. Parameters: Name Type Description Default workspace azureml.core.Workspace any given workspace None Returns: Type Description azureml.core.Workspace current (last) workspace given to current_workspace() Source code in shrike/pipeline/aml_connect.py def current_workspace ( workspace = None ): \"\"\"Sets/Gets the current AML workspace used all accross code. Args: workspace (azureml.core.Workspace): any given workspace Returns: azureml.core.Workspace: current (last) workspace given to current_workspace() \"\"\" global CURRENT_AML_WORKSPACE if workspace : CURRENT_AML_WORKSPACE = workspace if not CURRENT_AML_WORKSPACE : raise Exception ( \"You need to initialize current_workspace() with an AML workspace\" ) return CURRENT_AML_WORKSPACE","title":"current_workspace()"},{"location":"pipeline/aml-connect/#shrike.pipeline.aml_connect.main","text":"Main function (for testing) Source code in shrike/pipeline/aml_connect.py def main (): \"\"\"Main function (for testing)\"\"\" parser = argparse . ArgumentParser ( description = __doc__ ) group = parser . add_argument_group ( \"AzureML connect arguments\" ) add_cli_args ( group ) args , unknown_args = parser . parse_known_args () if unknown_args : log . warning ( f \"You have provided unknown arguments { unknown_args } \" ) return azureml_connect_cli ( args )","title":"main()"},{"location":"pipeline/canary-helper/","text":"Canary helper Canary helper code get_repo_info () [EXPERIMENTAL] Obtains info on the current repo the code is in. Returns: Type Description dict git meta data Source code in shrike/pipeline/canary_helper.py def get_repo_info (): \"\"\"[EXPERIMENTAL] Obtains info on the current repo the code is in. Returns: dict: git meta data\"\"\" try : import git repo = git . Repo ( search_parent_directories = True ) branch = repo . active_branch head = repo . head return { \"git\" : repo . remotes . origin . url , \"branch\" : branch . name , \"commit\" : head . commit . hexsha , \"last_known_author\" : head . commit . author . name , } except : return { \"git\" : \"n/a\" } test_pipeline_step_metrics ( pipeline_run , expected_metrics ) Tests a pipeline run against a set of expected metrics. Parameters: Name Type Description Default pipeline_run PipelineRun the AzureML pipeline run required expected_metrics dict defines the tests to execute required Returns: Type Description List errors collected during tests !!! notes example entries in expected_metrics \"SelectJsonField\" : [{\"row\" : {\"name\" : \"output\", \"key\" : \"size\", \"value\" : 369559}}], tests module \"SelectJsonField\" for a metric row named \"output\", checks key \"size\" must have value 369559 \"tokenizerparallel\" : [{\"metric\" : {\"key\" : \"Failed Items\", \"value\" : 0}}], tests module \"tokenizerparallel\" for a metric named \"Failed Items\", value must be 0 Source code in shrike/pipeline/canary_helper.py def test_pipeline_step_metrics ( pipeline_run , expected_metrics ): \"\"\"Tests a pipeline run against a set of expected metrics. Args: pipeline_run (PipelineRun): the AzureML pipeline run expected_metrics (dict): defines the tests to execute Returns: List: errors collected during tests Notes: example entries in expected_metrics \"SelectJsonField\" : [{\"row\" : {\"name\" : \"output\", \"key\" : \"size\", \"value\" : 369559}}], tests module \"SelectJsonField\" for a metric row named \"output\", checks key \"size\" must have value 369559 \"tokenizerparallel\" : [{\"metric\" : {\"key\" : \"Failed Items\", \"value\" : 0}}], tests module \"tokenizerparallel\" for a metric named \"Failed Items\", value must be 0 \"\"\" errors = [] log . info ( \"Looping through PipelineRun steps to test metrics...\" ) for step in pipeline_run . get_steps (): log . info ( f \"Checking status of step { step . name } ...\" ) observed_metrics = step . get_metrics () log . info ( f \"Step Metrics: { observed_metrics } \" ) status = step . get_status () if status != \"Finished\" : errors . append ( f \"Pipeline step { step . name } status is { status } != Finished\" ) if step . name in expected_metrics : for expected_metric_test in expected_metrics [ step . name ]: if \"row\" in expected_metric_test : log . info ( f \"Checking metrics, looking for { expected_metric_test } \" ) row_key = expected_metric_test [ \"row\" ][ \"name\" ] metric_key = expected_metric_test [ \"row\" ][ \"key\" ] expected_value = expected_metric_test [ \"row\" ][ \"value\" ] if row_key not in observed_metrics : errors . append ( f \"Step { step . name } metric row ' { row_key } ' not available in observed metrics { observed_metrics } \" ) elif metric_key not in observed_metrics [ row_key ]: errors . append ( f \"Step { step . name } metric row ' { row_key } ' does not have a metric ' { metric_key } ' in observed metrics { observed_metrics [ row_key ] } \" ) elif observed_metrics [ row_key ][ metric_key ] != expected_value : errors . append ( f \"Step { step . name } metric row ' { row_key } ' - metric ' { metric_key } ' - does not have expected value { expected_value } in observed metrics { observed_metrics [ row_key ] } \" ) if \"metric\" in expected_metric_test : log . info ( f \"Checking metrics, looking for { expected_metric_test } \" ) metric_key = expected_metric_test [ \"metric\" ][ \"key\" ] expected_value = expected_metric_test [ \"metric\" ][ \"value\" ] if metric_key not in observed_metrics : errors . append ( f \"Step { step . name } metric ' { metric_key } ' not available in observed metrics { observed_metrics } \" ) elif observed_metrics [ metric_key ] != expected_value : errors . append ( f \"Step { step . name } metric row ' { metric_key } ' does not have expected value { expected_value } in observed metrics { observed_metrics [ metric_key ] } \" ) return errors test_pipeline_step_output ( pipeline_run , step_name , output_name , ** kwargs ) Verify a given pipeline output for some basic checks. Parameters: Name Type Description Default pipeline_run PipelineRun the pipeline run required step_name str name of the step to check required output_name str name of the output to check required **kwargs Arbitrary keyword arguments defining the test {} !!! kwargs length (int) : to verify the length Returns: Type Description dict results Source code in shrike/pipeline/canary_helper.py def test_pipeline_step_output ( pipeline_run , step_name , output_name , ** kwargs ): \"\"\"Verify a given pipeline output for some basic checks. Args: pipeline_run (PipelineRun): the pipeline run step_name (str): name of the step to check output_name (str): name of the output to check **kwargs: Arbitrary keyword arguments defining the test Kwargs: length (int) : to verify the length Returns: dict: results \"\"\" pipeline_step = pipeline_run . find_step_run ( step_name ) results = { \"errors\" : []} if not pipeline_step : results [ \"exception\" ] = f \"Could not find step { step_name } in pipeline { pipeline_run . _run_id } .\" return results output_port = pipeline_step [ 0 ] . get_output_data ( output_name ) if not output_port : results [ \"exception\" ] = f \"Could not find output { output_name } in step { step_name } in pipeline { pipeline_run . _run_id } .\" return results data_reference = output_port . _data_reference data_path = DataPath ( datastore = data_reference . datastore , path_on_datastore = data_reference . path_on_datastore , name = data_reference . data_reference_name , ) if kwargs . get ( \"length\" ): expected_length = kwargs . get ( \"length\" ) log . info ( f \"Checking count= { expected_length } of files for step { step_name } output { output_name } ...\" ) data_set = Dataset . File . from_files ( data_path ) files_list = data_set . to_path () if expected_length < 0 : # test any length > 0 results [ \"length\" ] = { \"expected\" : \">0\" , \"observed\" : len ( files_list )} if results [ \"length\" ][ \"observed\" ] == 0 : message = \"\"\"Length mismatch in output {output_name} in step {step_name} in pipeline {run_id} . Expected len {a} found {b} .\"\"\" . format ( output_name = output_name , step_name = step_name , run_id = pipeline_run . _run_id , b = results [ \"length\" ][ \"observed\" ], a = results [ \"length\" ][ \"expected\" ], ) # logging.error(message) results [ \"errors\" ] . append ( message ) else : results [ \"length\" ] = { \"expected\" : expected_length , \"observed\" : len ( files_list ), } if results [ \"length\" ][ \"observed\" ] != results [ \"length\" ][ \"expected\" ]: message = \"\"\"Length mismatch in output {output_name} in step {step_name} in pipeline {run_id} . Expected len {a} found {b} .\"\"\" . format ( output_name = output_name , step_name = step_name , run_id = pipeline_run . _run_id , b = results [ \"length\" ][ \"observed\" ], a = results [ \"length\" ][ \"expected\" ], ) # logging.error(message) results [ \"errors\" ] . append ( message ) return results","title":"canary_helper"},{"location":"pipeline/canary-helper/#canary-helper","text":"Canary helper code","title":"Canary helper"},{"location":"pipeline/canary-helper/#shrike.pipeline.canary_helper.get_repo_info","text":"[EXPERIMENTAL] Obtains info on the current repo the code is in. Returns: Type Description dict git meta data Source code in shrike/pipeline/canary_helper.py def get_repo_info (): \"\"\"[EXPERIMENTAL] Obtains info on the current repo the code is in. Returns: dict: git meta data\"\"\" try : import git repo = git . Repo ( search_parent_directories = True ) branch = repo . active_branch head = repo . head return { \"git\" : repo . remotes . origin . url , \"branch\" : branch . name , \"commit\" : head . commit . hexsha , \"last_known_author\" : head . commit . author . name , } except : return { \"git\" : \"n/a\" }","title":"get_repo_info()"},{"location":"pipeline/canary-helper/#shrike.pipeline.canary_helper.test_pipeline_step_metrics","text":"Tests a pipeline run against a set of expected metrics. Parameters: Name Type Description Default pipeline_run PipelineRun the AzureML pipeline run required expected_metrics dict defines the tests to execute required Returns: Type Description List errors collected during tests !!! notes example entries in expected_metrics \"SelectJsonField\" : [{\"row\" : {\"name\" : \"output\", \"key\" : \"size\", \"value\" : 369559}}], tests module \"SelectJsonField\" for a metric row named \"output\", checks key \"size\" must have value 369559 \"tokenizerparallel\" : [{\"metric\" : {\"key\" : \"Failed Items\", \"value\" : 0}}], tests module \"tokenizerparallel\" for a metric named \"Failed Items\", value must be 0 Source code in shrike/pipeline/canary_helper.py def test_pipeline_step_metrics ( pipeline_run , expected_metrics ): \"\"\"Tests a pipeline run against a set of expected metrics. Args: pipeline_run (PipelineRun): the AzureML pipeline run expected_metrics (dict): defines the tests to execute Returns: List: errors collected during tests Notes: example entries in expected_metrics \"SelectJsonField\" : [{\"row\" : {\"name\" : \"output\", \"key\" : \"size\", \"value\" : 369559}}], tests module \"SelectJsonField\" for a metric row named \"output\", checks key \"size\" must have value 369559 \"tokenizerparallel\" : [{\"metric\" : {\"key\" : \"Failed Items\", \"value\" : 0}}], tests module \"tokenizerparallel\" for a metric named \"Failed Items\", value must be 0 \"\"\" errors = [] log . info ( \"Looping through PipelineRun steps to test metrics...\" ) for step in pipeline_run . get_steps (): log . info ( f \"Checking status of step { step . name } ...\" ) observed_metrics = step . get_metrics () log . info ( f \"Step Metrics: { observed_metrics } \" ) status = step . get_status () if status != \"Finished\" : errors . append ( f \"Pipeline step { step . name } status is { status } != Finished\" ) if step . name in expected_metrics : for expected_metric_test in expected_metrics [ step . name ]: if \"row\" in expected_metric_test : log . info ( f \"Checking metrics, looking for { expected_metric_test } \" ) row_key = expected_metric_test [ \"row\" ][ \"name\" ] metric_key = expected_metric_test [ \"row\" ][ \"key\" ] expected_value = expected_metric_test [ \"row\" ][ \"value\" ] if row_key not in observed_metrics : errors . append ( f \"Step { step . name } metric row ' { row_key } ' not available in observed metrics { observed_metrics } \" ) elif metric_key not in observed_metrics [ row_key ]: errors . append ( f \"Step { step . name } metric row ' { row_key } ' does not have a metric ' { metric_key } ' in observed metrics { observed_metrics [ row_key ] } \" ) elif observed_metrics [ row_key ][ metric_key ] != expected_value : errors . append ( f \"Step { step . name } metric row ' { row_key } ' - metric ' { metric_key } ' - does not have expected value { expected_value } in observed metrics { observed_metrics [ row_key ] } \" ) if \"metric\" in expected_metric_test : log . info ( f \"Checking metrics, looking for { expected_metric_test } \" ) metric_key = expected_metric_test [ \"metric\" ][ \"key\" ] expected_value = expected_metric_test [ \"metric\" ][ \"value\" ] if metric_key not in observed_metrics : errors . append ( f \"Step { step . name } metric ' { metric_key } ' not available in observed metrics { observed_metrics } \" ) elif observed_metrics [ metric_key ] != expected_value : errors . append ( f \"Step { step . name } metric row ' { metric_key } ' does not have expected value { expected_value } in observed metrics { observed_metrics [ metric_key ] } \" ) return errors","title":"test_pipeline_step_metrics()"},{"location":"pipeline/canary-helper/#shrike.pipeline.canary_helper.test_pipeline_step_output","text":"Verify a given pipeline output for some basic checks. Parameters: Name Type Description Default pipeline_run PipelineRun the pipeline run required step_name str name of the step to check required output_name str name of the output to check required **kwargs Arbitrary keyword arguments defining the test {} !!! kwargs length (int) : to verify the length Returns: Type Description dict results Source code in shrike/pipeline/canary_helper.py def test_pipeline_step_output ( pipeline_run , step_name , output_name , ** kwargs ): \"\"\"Verify a given pipeline output for some basic checks. Args: pipeline_run (PipelineRun): the pipeline run step_name (str): name of the step to check output_name (str): name of the output to check **kwargs: Arbitrary keyword arguments defining the test Kwargs: length (int) : to verify the length Returns: dict: results \"\"\" pipeline_step = pipeline_run . find_step_run ( step_name ) results = { \"errors\" : []} if not pipeline_step : results [ \"exception\" ] = f \"Could not find step { step_name } in pipeline { pipeline_run . _run_id } .\" return results output_port = pipeline_step [ 0 ] . get_output_data ( output_name ) if not output_port : results [ \"exception\" ] = f \"Could not find output { output_name } in step { step_name } in pipeline { pipeline_run . _run_id } .\" return results data_reference = output_port . _data_reference data_path = DataPath ( datastore = data_reference . datastore , path_on_datastore = data_reference . path_on_datastore , name = data_reference . data_reference_name , ) if kwargs . get ( \"length\" ): expected_length = kwargs . get ( \"length\" ) log . info ( f \"Checking count= { expected_length } of files for step { step_name } output { output_name } ...\" ) data_set = Dataset . File . from_files ( data_path ) files_list = data_set . to_path () if expected_length < 0 : # test any length > 0 results [ \"length\" ] = { \"expected\" : \">0\" , \"observed\" : len ( files_list )} if results [ \"length\" ][ \"observed\" ] == 0 : message = \"\"\"Length mismatch in output {output_name} in step {step_name} in pipeline {run_id} . Expected len {a} found {b} .\"\"\" . format ( output_name = output_name , step_name = step_name , run_id = pipeline_run . _run_id , b = results [ \"length\" ][ \"observed\" ], a = results [ \"length\" ][ \"expected\" ], ) # logging.error(message) results [ \"errors\" ] . append ( message ) else : results [ \"length\" ] = { \"expected\" : expected_length , \"observed\" : len ( files_list ), } if results [ \"length\" ][ \"observed\" ] != results [ \"length\" ][ \"expected\" ]: message = \"\"\"Length mismatch in output {output_name} in step {step_name} in pipeline {run_id} . Expected len {a} found {b} .\"\"\" . format ( output_name = output_name , step_name = step_name , run_id = pipeline_run . _run_id , b = results [ \"length\" ][ \"observed\" ], a = results [ \"length\" ][ \"expected\" ], ) # logging.error(message) results [ \"errors\" ] . append ( message ) return results","title":"test_pipeline_step_output()"},{"location":"pipeline/configure-aml-pipeline/","text":"How to use pipeline config files when using shrike.pipeline Structure of config files In this page, we perform a detailed review of an example standard config file . This should give you a good idea of how to use config files properly based on your scenarios. For a pipeline, we set up 4 config files under the config directory , which includes 4 sub-folders: experiments , modules , aml , and compute . The demograph_eyeson.yaml file linked above lives in the experiments folder; it is the main config file which specifies the overall pipeline configuration. This main config file refers to three other config files under the config directory: a config file under the aml folder which lets you point at your Azure ML workspace by specifying subscription_id, resource_group, workspace_name, tenant and auth; a config file under the compute folder which specifies configurations such as the compliant data store name, compute targets names, data I/O methods, etc; a config file under the modules folder , which lists all the available components with their properties (key, name, default version, and location of the component specification file). Now we will go through the config file linked above and explain each section. 1. Brief summary section At the beginning of the config file, it is suggested to provide a brief comment explaining which pipeline this config file is used for, and also provide an example command to run the pipeline with this config file. See below for an example: # This YAML file configures the accelerator tutorial pipeline for eyes-on # command for running the pipeline: # python pipelines/experiments/demograph_eyeson.py --config-dir pipelines/config --config-name experiments/demograph_eyeson run.submit=True 2. defaults section The defaults section contains references of the aml resources, pointing to two other config files under the aml and compute folders. It also points to the file listing all available components, which is located in the modules folder. This section looks like below. defaults : - aml : eyeson # default aml references - compute : eyeson # default compute target names - modules : module_defaults # list of modules + versions See below for the contents of the aml config file. You will need to update the info based on your own aml resources. To find your workspace name, subscription Id, and resource group, go to your Azure ML workspace, then click the \"change subscription\" icon in the top right (between the settings and question mark), then \"Download config file\". You will find the 3 values in this file. The Torus TenantId for eyes-off workspaces is cdc5aeea-15c5-4db6-b079-fcadd2505dc2 , whereas the 72f988bf-86f1-41af-91ab-2d7cd011db47 used below is the Microsoft TenantId that you will use for personal workspaces. # @package _group_ subscription_id : 48bbc269-ce89-4f6f-9a12-c6f91fcb772d resource_group : aml1p-rg workspace_name : aml1p-ml-wus2 tenant : 72f988bf-86f1-41af-91ab-2d7cd011db47 auth : \"interactive\" See below for the contents of the compute config file (update the info based on your own aml resources). # @package _group_ # name of default target default_compute_target : \"cpu-cluster\" # where intermediary output is written compliant_datastore : \"workspaceblobstore\" # Linux targets linux_cpu_dc_target : \"cpu-cluster\" linux_cpu_prod_target : \"cpu-cluster\" linux_gpu_dc_target : \"gpu-cluster\" linux_gpu_prod_target : \"gpu-cluster\" # data I/O for linux modules linux_input_mode : \"download\" linux_output_mode : \"upload\" # Windows targets windows_cpu_prod_target : \"cpu-cluster\" # data I/O for windows modules windows_input_mode : \"download\" windows_output_mode : \"upload\" # hdi cluster hdi_prod_target : \"hdi-cluster\" # data transfer cluster datatransfer_target : \"data-factory\" 3. run section In this section, you configure the parameters controlling how to run your experiment. Update the info based on your own pipeline. Parameter names should be self-explanatory. # run parameters are command line arguments for running your experiment run : # params for running pipeline experiment_name : \"demo_graph_eyeson\" # IMPORTANT regenerate_outputs : false continue_on_failure : false verbose : false submit : false resume : false canary : false silent : false wait : false 4. module_loader section This section includes 4 arguments: use_local , force_default_module_version , force_all_module_version , and local_steps_folder . The use_local parameter specifies which components of the pipeline you would like to build from your local code (rather than consuming the remote registered component). Use a comma-separated string to specify the list of components from your local code. If you use \"*\", all components are loaded from local code. For more information, please check out Use component key to run this component locally . The force_default_module_version argument enables you to change the default version of the component in your branch (the default version is the latest version, but this argument allows you to pin it to a given release version if you prefer). The force_all_module_version argument enables you to force all components to consume a fixed version, even if the version is specified otherwise in the pipeline code. The argument local_steps_folder should be clear and self-explanatory: this is the directory where all the component folders are located. # module_loader module_loader : # module loading params # IMPORTANT: if you want to modify a given module, add its key here # see the code for identifying the module key # use comma separation in this string to use multiple local modules use_local : \"DemoComponent\" # fix the version of modules in all subgraphs (if left unspecified) # NOTE: use the latest release version to \"fix\" your branch to a given release # see https://eemo.visualstudio.com/TEE/_release?_a=releases&view=mine&definitionId=76 force_default_module_version : null # forces ALL module versions to this unique value (even if specified otherwise in code) force_all_module_version : null # path to the steps folder, don't modify this one # NOTE: we're working on deprecating this one local_steps_folder : \"../../../components\" # NOTE: run scripts from accelerator-repo 5. Other sections The sections above only defined overall pipeline parameters, not component parameters. We recommend gathering the component parameters into distinct sections, one per component. The example for the eyes-on demo experiment is shown below. # DemoComponent config democomponent : input_data : irisdata # the data we'll be working on input_data_version : \"latest\" # use this to pin a specific version message : \"Hello, World!\" value : 1000 # the size of the sample to analyze","title":"Configure your AML pipeline"},{"location":"pipeline/configure-aml-pipeline/#how-to-use-pipeline-config-files-when-using-shrikepipeline","text":"","title":"How to use pipeline config files when using shrike.pipeline"},{"location":"pipeline/configure-aml-pipeline/#structure-of-config-files","text":"In this page, we perform a detailed review of an example standard config file . This should give you a good idea of how to use config files properly based on your scenarios. For a pipeline, we set up 4 config files under the config directory , which includes 4 sub-folders: experiments , modules , aml , and compute . The demograph_eyeson.yaml file linked above lives in the experiments folder; it is the main config file which specifies the overall pipeline configuration. This main config file refers to three other config files under the config directory: a config file under the aml folder which lets you point at your Azure ML workspace by specifying subscription_id, resource_group, workspace_name, tenant and auth; a config file under the compute folder which specifies configurations such as the compliant data store name, compute targets names, data I/O methods, etc; a config file under the modules folder , which lists all the available components with their properties (key, name, default version, and location of the component specification file). Now we will go through the config file linked above and explain each section.","title":"Structure of config files"},{"location":"pipeline/configure-aml-pipeline/#1-brief-summary-section","text":"At the beginning of the config file, it is suggested to provide a brief comment explaining which pipeline this config file is used for, and also provide an example command to run the pipeline with this config file. See below for an example: # This YAML file configures the accelerator tutorial pipeline for eyes-on # command for running the pipeline: # python pipelines/experiments/demograph_eyeson.py --config-dir pipelines/config --config-name experiments/demograph_eyeson run.submit=True","title":"1. Brief summary section"},{"location":"pipeline/configure-aml-pipeline/#2-defaults-section","text":"The defaults section contains references of the aml resources, pointing to two other config files under the aml and compute folders. It also points to the file listing all available components, which is located in the modules folder. This section looks like below. defaults : - aml : eyeson # default aml references - compute : eyeson # default compute target names - modules : module_defaults # list of modules + versions See below for the contents of the aml config file. You will need to update the info based on your own aml resources. To find your workspace name, subscription Id, and resource group, go to your Azure ML workspace, then click the \"change subscription\" icon in the top right (between the settings and question mark), then \"Download config file\". You will find the 3 values in this file. The Torus TenantId for eyes-off workspaces is cdc5aeea-15c5-4db6-b079-fcadd2505dc2 , whereas the 72f988bf-86f1-41af-91ab-2d7cd011db47 used below is the Microsoft TenantId that you will use for personal workspaces. # @package _group_ subscription_id : 48bbc269-ce89-4f6f-9a12-c6f91fcb772d resource_group : aml1p-rg workspace_name : aml1p-ml-wus2 tenant : 72f988bf-86f1-41af-91ab-2d7cd011db47 auth : \"interactive\" See below for the contents of the compute config file (update the info based on your own aml resources). # @package _group_ # name of default target default_compute_target : \"cpu-cluster\" # where intermediary output is written compliant_datastore : \"workspaceblobstore\" # Linux targets linux_cpu_dc_target : \"cpu-cluster\" linux_cpu_prod_target : \"cpu-cluster\" linux_gpu_dc_target : \"gpu-cluster\" linux_gpu_prod_target : \"gpu-cluster\" # data I/O for linux modules linux_input_mode : \"download\" linux_output_mode : \"upload\" # Windows targets windows_cpu_prod_target : \"cpu-cluster\" # data I/O for windows modules windows_input_mode : \"download\" windows_output_mode : \"upload\" # hdi cluster hdi_prod_target : \"hdi-cluster\" # data transfer cluster datatransfer_target : \"data-factory\"","title":"2. defaults section"},{"location":"pipeline/configure-aml-pipeline/#3-run-section","text":"In this section, you configure the parameters controlling how to run your experiment. Update the info based on your own pipeline. Parameter names should be self-explanatory. # run parameters are command line arguments for running your experiment run : # params for running pipeline experiment_name : \"demo_graph_eyeson\" # IMPORTANT regenerate_outputs : false continue_on_failure : false verbose : false submit : false resume : false canary : false silent : false wait : false","title":"3. run section"},{"location":"pipeline/configure-aml-pipeline/#4-module_loader-section","text":"This section includes 4 arguments: use_local , force_default_module_version , force_all_module_version , and local_steps_folder . The use_local parameter specifies which components of the pipeline you would like to build from your local code (rather than consuming the remote registered component). Use a comma-separated string to specify the list of components from your local code. If you use \"*\", all components are loaded from local code. For more information, please check out Use component key to run this component locally . The force_default_module_version argument enables you to change the default version of the component in your branch (the default version is the latest version, but this argument allows you to pin it to a given release version if you prefer). The force_all_module_version argument enables you to force all components to consume a fixed version, even if the version is specified otherwise in the pipeline code. The argument local_steps_folder should be clear and self-explanatory: this is the directory where all the component folders are located. # module_loader module_loader : # module loading params # IMPORTANT: if you want to modify a given module, add its key here # see the code for identifying the module key # use comma separation in this string to use multiple local modules use_local : \"DemoComponent\" # fix the version of modules in all subgraphs (if left unspecified) # NOTE: use the latest release version to \"fix\" your branch to a given release # see https://eemo.visualstudio.com/TEE/_release?_a=releases&view=mine&definitionId=76 force_default_module_version : null # forces ALL module versions to this unique value (even if specified otherwise in code) force_all_module_version : null # path to the steps folder, don't modify this one # NOTE: we're working on deprecating this one local_steps_folder : \"../../../components\" # NOTE: run scripts from accelerator-repo","title":"4. module_loader section"},{"location":"pipeline/configure-aml-pipeline/#5-other-sections","text":"The sections above only defined overall pipeline parameters, not component parameters. We recommend gathering the component parameters into distinct sections, one per component. The example for the eyes-on demo experiment is shown below. # DemoComponent config democomponent : input_data : irisdata # the data we'll be working on input_data_version : \"latest\" # use this to pin a specific version message : \"Hello, World!\" value : 1000 # the size of the sample to analyze","title":"5. Other sections"},{"location":"pipeline/create-aml-pipeline/","text":"Instructions for creating a reusable AML pipeline using shrike.pipeline To enjoy this doc, you need to: have already setup your python environment with the AML SDK following these instructions and cloned the accelerator repository as described in the \"Set up\" section here ; have access to an AML workspace Motivation The Azure ML pipeline helper class AMLPipelineHelper in the shrike package was developed with the goal of helping data scientists to more easily create reusable pipelines. These instructions explain how to use the Azure ML pipeline helper class. 1. Review an existing Azure ML pipeline created using the Azure ML pipeline helper class The accelerator repository already has examples of pipelines created using the pipeline helper class. We will now have an overview of the structure of the two most important directories ( components and pipelines , under aml-ds/recipes/compliant-experimentation ) and go over the key files defining these pipelines. 1.1 \"components\" directory This is where the components are defined, one folder per component. Each folder contains the following files: component_spec.yaml : this is where the component's inputs, outputs and parameters are defined. This is the Azure ML equivalent to the component manifest in \u00c6ther. component_env.yaml : this is where the component dependencies are listed (not required for HDI components). run.py : this is the python file actually run in Azure ML; in most cases, it is just importing a python file from elsewhere in the repo. Further reading on components is available here . 1.2 \"pipelines\" directory This is where the graphs, a.k.a. pipelines, are defined. Here is what you will find in its subdirectories: The config directory contains the config files which contain the parameter values, organized in four sub-folders: experiments which contains the overall graph configs, then aml and compute which contain auxiliary config files referred to in the graph configs. The modules folder hosts the file where the components are defined (by their key, name, default version, and location of the component specification file). Once you have created new components, you will need to add them to that file. The subgraphs directory contain python files that define graphs that are not meant to be used on their own but as part of larger graphs. There is a demo subgraph available there, which consists of 2 probe components chained after each other. The experiments directory contain the python files whichactually define the graphs. Now let's take a closer look at the definition of a graph in python. We will stick with the demo graph for eyes-off and open the demograph_eyesoff.py file in the experiments folder. The key parts are listed below. - The required_subgraphs() function ( line 37 , also shown below) defines the subgraphs that are used in the graph. # line 37 def required_subgraphs ( cls ): \"\"\"Declare dependencies on other subgraphs to allow AMLPipelineHelper to build them for you. This method should return a dictionary: - Keys will be used in self.subgraph_load(key) to build each required subgraph. - Values are classes inheriting from AMLPipelineHelper Returns: dict[str->AMLPipelineHelper]: dictionary of subgraphs used for building this one. \"\"\" return { \"DemoSubgraph\" : DemoSubgraph } The build() function, well, builds the graph. First, the required subgraph is loaded in line 62 : probe_subgraph = self . subgraph_load ( \"DemoSubgraph\" ) Then we define a pipeline function for the graph starting line 70 . This is where all the components and subgraphs are given their parameters and inputs. Note how the parameter values are read from the config files. To see how the outputs of some components can be used as inputs of the following components see here in the subgraph python file . subgraph_load can take an additional custom_config [DictConfig] argument. All params in this arguments will be added to the pipeline config (overwrite) . This is particularly useful when one wants to manipulate different instances of the subgraph with conditionals and other variables that need to be evaluated at build time. For the time being, we have to manually apply run settings to every component. In the future, this will not be necessary anymore. For the current example, it is also done in the subgraph python file, by calling the apply_recommended_runsettings() function . The pipeline_instance() function creates a runnable instance of the pipeline. The input dataset is defined in lines 104-107 , by calling the dataset_load() function with the name and version values provided in the config file. The pipeline function is then called with the input data as argument. Next, let's open the demograph_eyesoff.yaml config file under the pipelines/config/experiments directory, and note how the other config files are referenced, and how the parameters are organized in sections. We also explain config files in more details in this page: Configure your pipeline . Finally, below is the command to run this existing pipeline (a very basic demo pipeline): python pipelines/experiments/demograph_eyesoff.py --config-dir pipelines/config --config-name experiments/demograph_eyesoff run.submit=True 2. Create your own simple Azure ML pipeline using the pipeline helper class and an already existing component In this section, We will create a pipeline graph consisting of a single component called probe , which is readily available in the accelerator repository. We will pass the parameters through a config file. Procedure: [1] For creating your own pipeline, we invite you to start from an already existing pipeline definition such as demograph_eyeson.py and build from there. Just copy demograph_eyeson.py , rename it as demograph_workshop.py , update the contents accordingly, and put it under the same directory (i.e., pipelines/experiments ). The important parts to modify for this file are those listed in the section on key files above: build() , and pipeline_instance() (since we won't be using a subgraph, we don't need to worry about the required_subgraphs part). [2] To prepare the YAML config file, start from an existing example, such as demograph_eyeson.yaml . Just copy demograph_eyeson.yaml , rename it as demograph_eyeson_workshop.yaml , update the contents accordingly, and put it under the same directory (i.e., pipelines/config/experiments ). The important parts are defining the component parameter values, and declaring that we want to use the local version of the component (argument use_local ) for probe . > Note: you will also need to update two auxiliary config files ( eyesoff.yaml / eyeson.yaml file under directory pipelines/config/aml and eyesoff.yaml / eyeson.yaml under directory pipelines/config/compute ), referenced by this main config file demograph_eyeson.yaml , to point to the Azure ML workspace and compute targets to which you have access. And now you should be able to run your pipeline using the following command: python pipelines/experiments/demograph_eyesoff.py --config-dir pipelines/config --config-name experiments/demograph_eyesoff run.submit=True If you are using an eyes-on workspace, you will also need to update the base image info in component_spec.yaml since only eyes-off workspaces can connect to the polymer prod ACR which hosts the base image. When a parameter is not specified in the config file, you need to use + when overriding directly from command line. Otherwise there'll be errors. For example, if run.submit is not in the config file, you need to use python pipelines/experiments/demograph_eyesoff.py --config-dir pipelines/config --config-name experiments/demograph_eyesoff +run.submit=True . Please refer to Hydra override syntax for more info.","title":"Create an AML pipeline"},{"location":"pipeline/create-aml-pipeline/#instructions-for-creating-a-reusable-aml-pipeline-using-shrikepipeline","text":"To enjoy this doc, you need to: have already setup your python environment with the AML SDK following these instructions and cloned the accelerator repository as described in the \"Set up\" section here ; have access to an AML workspace","title":"Instructions for creating a reusable AML pipeline using shrike.pipeline"},{"location":"pipeline/create-aml-pipeline/#motivation","text":"The Azure ML pipeline helper class AMLPipelineHelper in the shrike package was developed with the goal of helping data scientists to more easily create reusable pipelines. These instructions explain how to use the Azure ML pipeline helper class.","title":"Motivation"},{"location":"pipeline/create-aml-pipeline/#1-review-an-existing-azure-ml-pipeline-created-using-the-azure-ml-pipeline-helper-class","text":"The accelerator repository already has examples of pipelines created using the pipeline helper class. We will now have an overview of the structure of the two most important directories ( components and pipelines , under aml-ds/recipes/compliant-experimentation ) and go over the key files defining these pipelines.","title":"1. Review an existing  Azure ML pipeline created using the Azure ML pipeline helper class"},{"location":"pipeline/create-aml-pipeline/#11-components-directory","text":"This is where the components are defined, one folder per component. Each folder contains the following files: component_spec.yaml : this is where the component's inputs, outputs and parameters are defined. This is the Azure ML equivalent to the component manifest in \u00c6ther. component_env.yaml : this is where the component dependencies are listed (not required for HDI components). run.py : this is the python file actually run in Azure ML; in most cases, it is just importing a python file from elsewhere in the repo. Further reading on components is available here .","title":"1.1 \"components\" directory"},{"location":"pipeline/create-aml-pipeline/#12-pipelines-directory","text":"This is where the graphs, a.k.a. pipelines, are defined. Here is what you will find in its subdirectories: The config directory contains the config files which contain the parameter values, organized in four sub-folders: experiments which contains the overall graph configs, then aml and compute which contain auxiliary config files referred to in the graph configs. The modules folder hosts the file where the components are defined (by their key, name, default version, and location of the component specification file). Once you have created new components, you will need to add them to that file. The subgraphs directory contain python files that define graphs that are not meant to be used on their own but as part of larger graphs. There is a demo subgraph available there, which consists of 2 probe components chained after each other. The experiments directory contain the python files whichactually define the graphs. Now let's take a closer look at the definition of a graph in python. We will stick with the demo graph for eyes-off and open the demograph_eyesoff.py file in the experiments folder. The key parts are listed below. - The required_subgraphs() function ( line 37 , also shown below) defines the subgraphs that are used in the graph. # line 37 def required_subgraphs ( cls ): \"\"\"Declare dependencies on other subgraphs to allow AMLPipelineHelper to build them for you. This method should return a dictionary: - Keys will be used in self.subgraph_load(key) to build each required subgraph. - Values are classes inheriting from AMLPipelineHelper Returns: dict[str->AMLPipelineHelper]: dictionary of subgraphs used for building this one. \"\"\" return { \"DemoSubgraph\" : DemoSubgraph } The build() function, well, builds the graph. First, the required subgraph is loaded in line 62 : probe_subgraph = self . subgraph_load ( \"DemoSubgraph\" ) Then we define a pipeline function for the graph starting line 70 . This is where all the components and subgraphs are given their parameters and inputs. Note how the parameter values are read from the config files. To see how the outputs of some components can be used as inputs of the following components see here in the subgraph python file . subgraph_load can take an additional custom_config [DictConfig] argument. All params in this arguments will be added to the pipeline config (overwrite) . This is particularly useful when one wants to manipulate different instances of the subgraph with conditionals and other variables that need to be evaluated at build time. For the time being, we have to manually apply run settings to every component. In the future, this will not be necessary anymore. For the current example, it is also done in the subgraph python file, by calling the apply_recommended_runsettings() function . The pipeline_instance() function creates a runnable instance of the pipeline. The input dataset is defined in lines 104-107 , by calling the dataset_load() function with the name and version values provided in the config file. The pipeline function is then called with the input data as argument. Next, let's open the demograph_eyesoff.yaml config file under the pipelines/config/experiments directory, and note how the other config files are referenced, and how the parameters are organized in sections. We also explain config files in more details in this page: Configure your pipeline . Finally, below is the command to run this existing pipeline (a very basic demo pipeline): python pipelines/experiments/demograph_eyesoff.py --config-dir pipelines/config --config-name experiments/demograph_eyesoff run.submit=True","title":"1.2 \"pipelines\" directory"},{"location":"pipeline/create-aml-pipeline/#2-create-your-own-simple-azure-ml-pipeline-using-the-pipeline-helper-class-and-an-already-existing-component","text":"In this section, We will create a pipeline graph consisting of a single component called probe , which is readily available in the accelerator repository. We will pass the parameters through a config file. Procedure: [1] For creating your own pipeline, we invite you to start from an already existing pipeline definition such as demograph_eyeson.py and build from there. Just copy demograph_eyeson.py , rename it as demograph_workshop.py , update the contents accordingly, and put it under the same directory (i.e., pipelines/experiments ). The important parts to modify for this file are those listed in the section on key files above: build() , and pipeline_instance() (since we won't be using a subgraph, we don't need to worry about the required_subgraphs part). [2] To prepare the YAML config file, start from an existing example, such as demograph_eyeson.yaml . Just copy demograph_eyeson.yaml , rename it as demograph_eyeson_workshop.yaml , update the contents accordingly, and put it under the same directory (i.e., pipelines/config/experiments ). The important parts are defining the component parameter values, and declaring that we want to use the local version of the component (argument use_local ) for probe . > Note: you will also need to update two auxiliary config files ( eyesoff.yaml / eyeson.yaml file under directory pipelines/config/aml and eyesoff.yaml / eyeson.yaml under directory pipelines/config/compute ), referenced by this main config file demograph_eyeson.yaml , to point to the Azure ML workspace and compute targets to which you have access. And now you should be able to run your pipeline using the following command: python pipelines/experiments/demograph_eyesoff.py --config-dir pipelines/config --config-name experiments/demograph_eyesoff run.submit=True If you are using an eyes-on workspace, you will also need to update the base image info in component_spec.yaml since only eyes-off workspaces can connect to the polymer prod ACR which hosts the base image. When a parameter is not specified in the config file, you need to use + when overriding directly from command line. Otherwise there'll be errors. For example, if run.submit is not in the config file, you need to use python pipelines/experiments/demograph_eyesoff.py --config-dir pipelines/config --config-name experiments/demograph_eyesoff +run.submit=True . Please refer to Hydra override syntax for more info.","title":"2. Create your own simple Azure ML pipeline using the pipeline helper class and an already existing component"},{"location":"pipeline/module-helper/","text":"Module helper Pipeline helper class to create pipelines loading modules from a flexible manifest. AMLModuleLoader Helper class to load modules from within an AMLPipelineHelper. __init__ ( self , config ) special Creates module instances for AMLPipelineHelper. Parameters: Name Type Description Default config DictConfig configuration options required Source code in shrike/pipeline/module_helper.py def __init__ ( self , config ): \"\"\"Creates module instances for AMLPipelineHelper. Args: config (DictConfig): configuration options \"\"\" self . use_local_except_for = ( config . module_loader . use_local_except_for if \"use_local_except_for\" in config . module_loader else None ) if \"use_local\" not in config . module_loader : self . use_local = [] elif config . module_loader . use_local is None : self . use_local = [] elif config . module_loader . use_local == \"*\" : self . use_local = \"*\" elif isinstance ( config . module_loader . use_local , str ): self . use_local = [ x . strip () for x in config . module_loader . use_local . split ( \",\" ) ] if not _check_use_local_syntax_valid ( self . use_local ): raise ValueError ( f 'Invalid value for `use_local`. Please follow one of the four patterns: \\n 1) use_local=\"\", all modules are remote \\n 2) use_local=\"*\", all modules are local \\n 3) use_local=\"MODULE_KEY_1, MODULE_KEY_2\", only MODULE_KEY_1, MODULE_KEY_2 are local, everything else is remote \\n 4) use_local=\"!MODULE_KEY_1, !MODULE_KEY_2\", all except for MODULE_KEY_1, MODULE_KEY_2 are local' ) self . use_local_except_for = self . use_local [ 0 ] . startswith ( \"!\" ) self . force_default_module_version = ( config . module_loader . force_default_module_version if \"force_default_module_version\" in config . module_loader else None ) self . force_all_module_version = ( config . module_loader . force_all_module_version if \"force_all_module_version\" in config . module_loader else None ) self . local_steps_folder = config . module_loader . local_steps_folder self . module_cache = {} # internal manifest built from yaml config self . modules_manifest = {} self . load_config_manifest ( config ) log . info ( f \"AMLModuleLoader initialized (use_local= { self . use_local } , force_default_module_version= { self . force_default_module_version } , force_all_module_version= { self . force_all_module_version } , local_steps_folder= { self . local_steps_folder } , manifest= { list ( self . modules_manifest . keys ()) } )\" ) get_from_cache ( self , module_cache_key ) Gets module class from internal cache (dict) Source code in shrike/pipeline/module_helper.py def get_from_cache ( self , module_cache_key ): \"\"\"Gets module class from internal cache (dict)\"\"\" log . debug ( f \"Using cached module { module_cache_key } \" ) return self . module_cache . get ( module_cache_key , None ) get_module_manifest_entry ( self , module_key , modules_manifest = None ) Gets a particular entry in the module manifest. Parameters: Name Type Description Default module_key str module key from the manifest required modules_manifest dict manifest from required_modules() [DEPRECATED] None Returns: Type Description dict module manifest entry Source code in shrike/pipeline/module_helper.py def get_module_manifest_entry ( self , module_key , modules_manifest = None ): \"\"\"Gets a particular entry in the module manifest. Args: module_key (str): module key from the manifest modules_manifest (dict): manifest from required_modules() [DEPRECATED] Returns: dict: module manifest entry \"\"\" if module_key in self . modules_manifest : module_entry = self . modules_manifest [ module_key ] module_namespace = None elif modules_manifest and module_key in modules_manifest : log . warning ( f \"We highly recommend substituting the `required_modules` method by the modules.manifest configuration.\" ) module_entry = modules_manifest [ module_key ] # map to new format module_entry [ \"yaml\" ] = module_entry [ \"yaml_spec\" ] module_entry [ \"name\" ] = module_entry [ \"remote_module_name\" ] module_namespace = module_entry . get ( \"namespace\" , None ) else : raise Exception ( f \"Module key ' { module_key } ' could not be found in modules.manifest configuration or in required_modules() method.\" ) return module_entry , module_namespace is_local ( self , module_name ) Tests is module is in local list Source code in shrike/pipeline/module_helper.py def is_local ( self , module_name ): \"\"\"Tests is module is in local list\"\"\" if self . use_local == \"*\" : return True if self . use_local_except_for : return \"!\" + module_name not in self . use_local else : return module_name in self . use_local load_config_manifest ( self , config ) Fills the internal module manifest based on config object Source code in shrike/pipeline/module_helper.py def load_config_manifest ( self , config ): \"\"\"Fills the internal module manifest based on config object\"\"\" for entry in config . modules . manifest : if entry . key : module_key = entry . key elif entry . name : module_key = entry . name else : raise Exception ( \"In module manifest, you have to provide at least key or name.\" ) self . modules_manifest [ module_key ] = entry load_local_module ( self , module_spec_path ) Creates one module instance. Parameters: Name Type Description Default module_spec_path str path to local module yaml spec required Returns: Type Description object module class loaded Source code in shrike/pipeline/module_helper.py def load_local_module ( self , module_spec_path ): \"\"\"Creates one module instance. Args: module_spec_path (str): path to local module yaml spec Returns: object: module class loaded \"\"\" module_cache_key = module_spec_path if self . module_in_cache ( module_cache_key ): return self . get_from_cache ( module_cache_key ) log . info ( \"Building module from local code at {} \" . format ( module_spec_path )) if not os . path . isfile ( module_spec_path ): module_spec_path = os . path . join ( self . local_steps_folder , module_spec_path ) loaded_module_class = Component . from_yaml ( current_workspace (), module_spec_path ) self . put_in_cache ( module_cache_key , loaded_module_class ) return loaded_module_class load_module ( self , module_key , modules_manifest = None ) Loads a particular module from the manifest. Parameters: Name Type Description Default module_key str module key from the manifest required modules_manifest dict manifest from required_modules() [DEPRECATED] None Returns: Type Description object module class loaded Source code in shrike/pipeline/module_helper.py def load_module ( self , module_key , modules_manifest = None ): \"\"\"Loads a particular module from the manifest. Args: module_key (str): module key from the manifest modules_manifest (dict): manifest from required_modules() [DEPRECATED] Returns: object: module class loaded \"\"\" module_entry , module_namespace = self . get_module_manifest_entry ( module_key , modules_manifest ) if self . is_local ( module_key ): loaded_module = self . load_local_module ( module_entry [ \"yaml\" ]) else : loaded_module = self . load_prod_module ( module_entry [ \"name\" ], module_entry [ \"version\" ], module_namespace = module_namespace , ) return loaded_module load_modules_manifest ( self , modules_manifest ) Creates module instances from modules_manifest. Parameters: Name Type Description Default modules_manifest dict manifest of modules to load required Returns: Type Description dict modules loaded, keys are taken from module_manifest. Exceptions: Type Description Exception if loading module has an error or manifest is wrong. Source code in shrike/pipeline/module_helper.py def load_modules_manifest ( self , modules_manifest ): \"\"\"Creates module instances from modules_manifest. Args: modules_manifest (dict): manifest of modules to load Returns: dict: modules loaded, keys are taken from module_manifest. Raises: Exception: if loading module has an error or manifest is wrong. \"\"\" log . info ( f \"Loading module manifest (use_local= { self . use_local } )\" ) test_results = self . verify_manifest ( modules_manifest ) if test_results : raise Exception ( \"Loading modules from manifest raised errors: \\n\\n MANIFEST: {} \\n\\n ERRORS: {} \" . format ( modules_manifest , \" \\n \" . join ( test_results ) ) ) loaded_modules = {} for module_key in modules_manifest : log . info ( f \"Loading module { module_key } from manifest\" ) loaded_modules [ module_key ] = self . load_module ( module_key , modules_manifest ) return loaded_modules load_prod_module ( self , module_name , module_version , module_namespace = None ) Creates one module instance. Parameters: Name Type Description Default module_name str) module name required module_version str) module version required Returns: Type Description object module class loaded Source code in shrike/pipeline/module_helper.py def load_prod_module ( self , module_name , module_version , module_namespace = None ): \"\"\"Creates one module instance. Args: module_name (str) : module name module_version (str) : module version Returns: object: module class loaded \"\"\" if self . force_all_module_version : module_version = self . force_all_module_version else : module_version = module_version or self . force_default_module_version module_cache_key = f \" { module_name } : { module_version } \" if self . module_in_cache ( module_cache_key ): return self . get_from_cache ( module_cache_key ) log . info ( f \"Loading remote module { module_cache_key } (name= { module_name } , version= { module_version } , namespace= { module_namespace } )\" ) loading_raised_exception = None try : # try without namespace first loaded_module_class = Component . load ( current_workspace (), name = module_name , version = module_version , ) except BaseException as e : # save the exception to raise it if namespace not provided if not module_namespace : raise e if module_namespace : log . info ( f \" Trying to load module { module_name } with namespace { module_namespace } .\" ) module_name = module_namespace + \"://\" + module_name loaded_module_class = Component . load ( current_workspace (), name = module_name , version = module_version , ) self . put_in_cache ( module_cache_key , loaded_module_class ) return loaded_module_class module_in_cache ( self , module_cache_key ) Tests if module in internal cache (dict) Source code in shrike/pipeline/module_helper.py def module_in_cache ( self , module_cache_key ): \"\"\"Tests if module in internal cache (dict)\"\"\" return module_cache_key in self . module_cache put_in_cache ( self , module_cache_key , module_class ) Puts module class in internal cache (dict) Source code in shrike/pipeline/module_helper.py def put_in_cache ( self , module_cache_key , module_class ): \"\"\"Puts module class in internal cache (dict)\"\"\" self . module_cache [ module_cache_key ] = module_class verify_manifest ( self , modules_manifest ) Tests a module manifest schema Source code in shrike/pipeline/module_helper.py def verify_manifest ( self , modules_manifest ): \"\"\"Tests a module manifest schema\"\"\" errors = [] for ( k , module_entry ) in modules_manifest . items (): # TODO: merge error checking code with processing code so we do all this in one pass if self . is_local ( k ): if \"yaml_spec\" not in module_entry : errors . append ( f \" { k } : You need to specify a yaml_spec for your module to use_local=[' { k } ']\" ) elif not os . path . isfile ( module_entry [ \"yaml_spec\" ] ) and not os . path . isfile ( os . path . join ( self . local_steps_folder , module_entry [ \"yaml_spec\" ]) ): errors . append ( \" {} : Could not find yaml spec {} for use_local=[' {} ']\" . format ( k , module_entry [ \"yaml_spec\" ], k ) ) else : if \"remote_module_name\" not in module_entry : errors . append ( f \" { k } : You need to specify a name for your module to use_local=False\" ) if \"namespace\" not in module_entry : errors . append ( f \" { k } : You need to specify a namespace for your module to use_local=False\" ) if ( \"version\" not in module_entry ) and ( self . force_default_module_version or self . force_all_module_version ): errors . append ( f \" { k } : You need to specify a version for your module to use_local=False, or use either force_default_module_version or force_all_module_version in config\" ) return errors module_loader_config dataclass Config for the AMLModuleLoader class module_manifest dataclass module_manifest(manifest: List[shrike.pipeline.module_helper.module_reference] = ) module_reference dataclass module_reference(key: Union[str, NoneType] = None, name: Union[str, NoneType] = None, source: Union[str, NoneType] = 'registered', yaml: Union[str, NoneType] = None, version: Union[str, NoneType] = None)","title":"module_helper"},{"location":"pipeline/module-helper/#module-helper","text":"Pipeline helper class to create pipelines loading modules from a flexible manifest.","title":"Module helper"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader","text":"Helper class to load modules from within an AMLPipelineHelper.","title":"AMLModuleLoader"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.__init__","text":"Creates module instances for AMLPipelineHelper. Parameters: Name Type Description Default config DictConfig configuration options required Source code in shrike/pipeline/module_helper.py def __init__ ( self , config ): \"\"\"Creates module instances for AMLPipelineHelper. Args: config (DictConfig): configuration options \"\"\" self . use_local_except_for = ( config . module_loader . use_local_except_for if \"use_local_except_for\" in config . module_loader else None ) if \"use_local\" not in config . module_loader : self . use_local = [] elif config . module_loader . use_local is None : self . use_local = [] elif config . module_loader . use_local == \"*\" : self . use_local = \"*\" elif isinstance ( config . module_loader . use_local , str ): self . use_local = [ x . strip () for x in config . module_loader . use_local . split ( \",\" ) ] if not _check_use_local_syntax_valid ( self . use_local ): raise ValueError ( f 'Invalid value for `use_local`. Please follow one of the four patterns: \\n 1) use_local=\"\", all modules are remote \\n 2) use_local=\"*\", all modules are local \\n 3) use_local=\"MODULE_KEY_1, MODULE_KEY_2\", only MODULE_KEY_1, MODULE_KEY_2 are local, everything else is remote \\n 4) use_local=\"!MODULE_KEY_1, !MODULE_KEY_2\", all except for MODULE_KEY_1, MODULE_KEY_2 are local' ) self . use_local_except_for = self . use_local [ 0 ] . startswith ( \"!\" ) self . force_default_module_version = ( config . module_loader . force_default_module_version if \"force_default_module_version\" in config . module_loader else None ) self . force_all_module_version = ( config . module_loader . force_all_module_version if \"force_all_module_version\" in config . module_loader else None ) self . local_steps_folder = config . module_loader . local_steps_folder self . module_cache = {} # internal manifest built from yaml config self . modules_manifest = {} self . load_config_manifest ( config ) log . info ( f \"AMLModuleLoader initialized (use_local= { self . use_local } , force_default_module_version= { self . force_default_module_version } , force_all_module_version= { self . force_all_module_version } , local_steps_folder= { self . local_steps_folder } , manifest= { list ( self . modules_manifest . keys ()) } )\" )","title":"__init__()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.get_from_cache","text":"Gets module class from internal cache (dict) Source code in shrike/pipeline/module_helper.py def get_from_cache ( self , module_cache_key ): \"\"\"Gets module class from internal cache (dict)\"\"\" log . debug ( f \"Using cached module { module_cache_key } \" ) return self . module_cache . get ( module_cache_key , None )","title":"get_from_cache()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.get_module_manifest_entry","text":"Gets a particular entry in the module manifest. Parameters: Name Type Description Default module_key str module key from the manifest required modules_manifest dict manifest from required_modules() [DEPRECATED] None Returns: Type Description dict module manifest entry Source code in shrike/pipeline/module_helper.py def get_module_manifest_entry ( self , module_key , modules_manifest = None ): \"\"\"Gets a particular entry in the module manifest. Args: module_key (str): module key from the manifest modules_manifest (dict): manifest from required_modules() [DEPRECATED] Returns: dict: module manifest entry \"\"\" if module_key in self . modules_manifest : module_entry = self . modules_manifest [ module_key ] module_namespace = None elif modules_manifest and module_key in modules_manifest : log . warning ( f \"We highly recommend substituting the `required_modules` method by the modules.manifest configuration.\" ) module_entry = modules_manifest [ module_key ] # map to new format module_entry [ \"yaml\" ] = module_entry [ \"yaml_spec\" ] module_entry [ \"name\" ] = module_entry [ \"remote_module_name\" ] module_namespace = module_entry . get ( \"namespace\" , None ) else : raise Exception ( f \"Module key ' { module_key } ' could not be found in modules.manifest configuration or in required_modules() method.\" ) return module_entry , module_namespace","title":"get_module_manifest_entry()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.is_local","text":"Tests is module is in local list Source code in shrike/pipeline/module_helper.py def is_local ( self , module_name ): \"\"\"Tests is module is in local list\"\"\" if self . use_local == \"*\" : return True if self . use_local_except_for : return \"!\" + module_name not in self . use_local else : return module_name in self . use_local","title":"is_local()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.load_config_manifest","text":"Fills the internal module manifest based on config object Source code in shrike/pipeline/module_helper.py def load_config_manifest ( self , config ): \"\"\"Fills the internal module manifest based on config object\"\"\" for entry in config . modules . manifest : if entry . key : module_key = entry . key elif entry . name : module_key = entry . name else : raise Exception ( \"In module manifest, you have to provide at least key or name.\" ) self . modules_manifest [ module_key ] = entry","title":"load_config_manifest()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.load_local_module","text":"Creates one module instance. Parameters: Name Type Description Default module_spec_path str path to local module yaml spec required Returns: Type Description object module class loaded Source code in shrike/pipeline/module_helper.py def load_local_module ( self , module_spec_path ): \"\"\"Creates one module instance. Args: module_spec_path (str): path to local module yaml spec Returns: object: module class loaded \"\"\" module_cache_key = module_spec_path if self . module_in_cache ( module_cache_key ): return self . get_from_cache ( module_cache_key ) log . info ( \"Building module from local code at {} \" . format ( module_spec_path )) if not os . path . isfile ( module_spec_path ): module_spec_path = os . path . join ( self . local_steps_folder , module_spec_path ) loaded_module_class = Component . from_yaml ( current_workspace (), module_spec_path ) self . put_in_cache ( module_cache_key , loaded_module_class ) return loaded_module_class","title":"load_local_module()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.load_module","text":"Loads a particular module from the manifest. Parameters: Name Type Description Default module_key str module key from the manifest required modules_manifest dict manifest from required_modules() [DEPRECATED] None Returns: Type Description object module class loaded Source code in shrike/pipeline/module_helper.py def load_module ( self , module_key , modules_manifest = None ): \"\"\"Loads a particular module from the manifest. Args: module_key (str): module key from the manifest modules_manifest (dict): manifest from required_modules() [DEPRECATED] Returns: object: module class loaded \"\"\" module_entry , module_namespace = self . get_module_manifest_entry ( module_key , modules_manifest ) if self . is_local ( module_key ): loaded_module = self . load_local_module ( module_entry [ \"yaml\" ]) else : loaded_module = self . load_prod_module ( module_entry [ \"name\" ], module_entry [ \"version\" ], module_namespace = module_namespace , ) return loaded_module","title":"load_module()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.load_modules_manifest","text":"Creates module instances from modules_manifest. Parameters: Name Type Description Default modules_manifest dict manifest of modules to load required Returns: Type Description dict modules loaded, keys are taken from module_manifest. Exceptions: Type Description Exception if loading module has an error or manifest is wrong. Source code in shrike/pipeline/module_helper.py def load_modules_manifest ( self , modules_manifest ): \"\"\"Creates module instances from modules_manifest. Args: modules_manifest (dict): manifest of modules to load Returns: dict: modules loaded, keys are taken from module_manifest. Raises: Exception: if loading module has an error or manifest is wrong. \"\"\" log . info ( f \"Loading module manifest (use_local= { self . use_local } )\" ) test_results = self . verify_manifest ( modules_manifest ) if test_results : raise Exception ( \"Loading modules from manifest raised errors: \\n\\n MANIFEST: {} \\n\\n ERRORS: {} \" . format ( modules_manifest , \" \\n \" . join ( test_results ) ) ) loaded_modules = {} for module_key in modules_manifest : log . info ( f \"Loading module { module_key } from manifest\" ) loaded_modules [ module_key ] = self . load_module ( module_key , modules_manifest ) return loaded_modules","title":"load_modules_manifest()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.load_prod_module","text":"Creates one module instance. Parameters: Name Type Description Default module_name str) module name required module_version str) module version required Returns: Type Description object module class loaded Source code in shrike/pipeline/module_helper.py def load_prod_module ( self , module_name , module_version , module_namespace = None ): \"\"\"Creates one module instance. Args: module_name (str) : module name module_version (str) : module version Returns: object: module class loaded \"\"\" if self . force_all_module_version : module_version = self . force_all_module_version else : module_version = module_version or self . force_default_module_version module_cache_key = f \" { module_name } : { module_version } \" if self . module_in_cache ( module_cache_key ): return self . get_from_cache ( module_cache_key ) log . info ( f \"Loading remote module { module_cache_key } (name= { module_name } , version= { module_version } , namespace= { module_namespace } )\" ) loading_raised_exception = None try : # try without namespace first loaded_module_class = Component . load ( current_workspace (), name = module_name , version = module_version , ) except BaseException as e : # save the exception to raise it if namespace not provided if not module_namespace : raise e if module_namespace : log . info ( f \" Trying to load module { module_name } with namespace { module_namespace } .\" ) module_name = module_namespace + \"://\" + module_name loaded_module_class = Component . load ( current_workspace (), name = module_name , version = module_version , ) self . put_in_cache ( module_cache_key , loaded_module_class ) return loaded_module_class","title":"load_prod_module()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.module_in_cache","text":"Tests if module in internal cache (dict) Source code in shrike/pipeline/module_helper.py def module_in_cache ( self , module_cache_key ): \"\"\"Tests if module in internal cache (dict)\"\"\" return module_cache_key in self . module_cache","title":"module_in_cache()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.put_in_cache","text":"Puts module class in internal cache (dict) Source code in shrike/pipeline/module_helper.py def put_in_cache ( self , module_cache_key , module_class ): \"\"\"Puts module class in internal cache (dict)\"\"\" self . module_cache [ module_cache_key ] = module_class","title":"put_in_cache()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.AMLModuleLoader.verify_manifest","text":"Tests a module manifest schema Source code in shrike/pipeline/module_helper.py def verify_manifest ( self , modules_manifest ): \"\"\"Tests a module manifest schema\"\"\" errors = [] for ( k , module_entry ) in modules_manifest . items (): # TODO: merge error checking code with processing code so we do all this in one pass if self . is_local ( k ): if \"yaml_spec\" not in module_entry : errors . append ( f \" { k } : You need to specify a yaml_spec for your module to use_local=[' { k } ']\" ) elif not os . path . isfile ( module_entry [ \"yaml_spec\" ] ) and not os . path . isfile ( os . path . join ( self . local_steps_folder , module_entry [ \"yaml_spec\" ]) ): errors . append ( \" {} : Could not find yaml spec {} for use_local=[' {} ']\" . format ( k , module_entry [ \"yaml_spec\" ], k ) ) else : if \"remote_module_name\" not in module_entry : errors . append ( f \" { k } : You need to specify a name for your module to use_local=False\" ) if \"namespace\" not in module_entry : errors . append ( f \" { k } : You need to specify a namespace for your module to use_local=False\" ) if ( \"version\" not in module_entry ) and ( self . force_default_module_version or self . force_all_module_version ): errors . append ( f \" { k } : You need to specify a version for your module to use_local=False, or use either force_default_module_version or force_all_module_version in config\" ) return errors","title":"verify_manifest()"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.module_loader_config","text":"Config for the AMLModuleLoader class","title":"module_loader_config"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.module_manifest","text":"module_manifest(manifest: List[shrike.pipeline.module_helper.module_reference] = )","title":"module_manifest"},{"location":"pipeline/module-helper/#shrike.pipeline.module_helper.module_reference","text":"module_reference(key: Union[str, NoneType] = None, name: Union[str, NoneType] = None, source: Union[str, NoneType] = 'registered', yaml: Union[str, NoneType] = None, version: Union[str, NoneType] = None)","title":"module_reference"},{"location":"pipeline/pipeline-config/","text":"Pipeline config Configuration dataclasses for AMLPipelineHelper aml_connection_config dataclass AML connection configuration pipeline_cli_config dataclass Pipeline config for command line parameters pipeline_compute_config dataclass AML workspace compute targets and I/O modes tenant_override_config dataclass tenant_override_config(allow_override: bool = False, keep_modified_files: bool = False, mapping: Dict[str, Any] = ) default_config_dict () Constructs the config dictionary for the pipeline helper settings Source code in shrike/pipeline/pipeline_config.py def default_config_dict (): \"\"\"Constructs the config dictionary for the pipeline helper settings\"\"\" return { \"aml\" : aml_connection_config , \"run\" : pipeline_cli_config , \"compute\" : pipeline_compute_config , \"module_loader\" : module_loader_config , \"modules\" : module_manifest , \"tenant_overrides\" : tenant_override_config , }","title":"pipeline_config"},{"location":"pipeline/pipeline-config/#pipeline-config","text":"Configuration dataclasses for AMLPipelineHelper","title":"Pipeline config"},{"location":"pipeline/pipeline-config/#shrike.pipeline.pipeline_config.aml_connection_config","text":"AML connection configuration","title":"aml_connection_config"},{"location":"pipeline/pipeline-config/#shrike.pipeline.pipeline_config.pipeline_cli_config","text":"Pipeline config for command line parameters","title":"pipeline_cli_config"},{"location":"pipeline/pipeline-config/#shrike.pipeline.pipeline_config.pipeline_compute_config","text":"AML workspace compute targets and I/O modes","title":"pipeline_compute_config"},{"location":"pipeline/pipeline-config/#shrike.pipeline.pipeline_config.tenant_override_config","text":"tenant_override_config(allow_override: bool = False, keep_modified_files: bool = False, mapping: Dict[str, Any] = )","title":"tenant_override_config"},{"location":"pipeline/pipeline-config/#shrike.pipeline.pipeline_config.default_config_dict","text":"Constructs the config dictionary for the pipeline helper settings Source code in shrike/pipeline/pipeline_config.py def default_config_dict (): \"\"\"Constructs the config dictionary for the pipeline helper settings\"\"\" return { \"aml\" : aml_connection_config , \"run\" : pipeline_cli_config , \"compute\" : pipeline_compute_config , \"module_loader\" : module_loader_config , \"modules\" : module_manifest , \"tenant_overrides\" : tenant_override_config , }","title":"default_config_dict()"},{"location":"pipeline/pipeline-helper/","text":"Pipeline helper Pipeline helper class to create pipelines loading modules from a flexible manifest. AMLPipelineHelper Helper class for building pipelines __init__ ( self , config , module_loader = None ) special Constructs the pipeline helper. Parameters: Name Type Description Default config DictConfig config for this object required module_loader AMLModuleLoader which module loader to (re)use None Source code in shrike/pipeline/pipeline_helper.py def __init__ ( self , config , module_loader = None ): \"\"\"Constructs the pipeline helper. Args: config (DictConfig): config for this object module_loader (AMLModuleLoader): which module loader to (re)use \"\"\" self . config = config if module_loader is None : log . info ( f \"Creating instance of AMLModuleLoader for { self . __class__ . __name__ } \" ) self . module_loader = AMLModuleLoader ( self . config ) else : self . module_loader = module_loader apply_recommended_runsettings ( self , module_name , module_instance , gpu = False , hdi = 'auto' , windows = 'auto' , parallel = 'auto' , mpi = 'auto' , scope = 'auto' , datatransfer = 'auto' , sweep = 'auto' , ** custom_runtime_arguments ) Applies regular settings for a given module. Parameters: Name Type Description Default module_name str name of the module from the module manifest (required_modules() method) required module_instance Module the AML module we need to add settings to required gpu bool is the module using GPU? False hdi bool is the module using HDI/Spark? 'auto' windows bool is the module using Windows compute? 'auto' parallel bool is the module using ParallelRunStep? 'auto' mpi bool is the module using Mpi? 'auto' custom_runtime_arguments dict any additional custom args {} Source code in shrike/pipeline/pipeline_helper.py def apply_recommended_runsettings ( self , module_name , module_instance , gpu = False , # can't autodetect that hdi = \"auto\" , windows = \"auto\" , parallel = \"auto\" , mpi = \"auto\" , scope = \"auto\" , datatransfer = \"auto\" , sweep = \"auto\" , ** custom_runtime_arguments , ): \"\"\"Applies regular settings for a given module. Args: module_name (str): name of the module from the module manifest (required_modules() method) module_instance (Module): the AML module we need to add settings to gpu (bool): is the module using GPU? hdi (bool): is the module using HDI/Spark? windows (bool): is the module using Windows compute? parallel (bool): is the module using ParallelRunStep? mpi (bool): is the module using Mpi? custom_runtime_arguments (dict): any additional custom args \"\"\" # verifies if module_name corresponds to module_instance self . _check_module_runsettings_consistency ( module_name , module_instance ) # Auto detect runsettings if hdi == \"auto\" : hdi = str ( module_instance . type ) == \"HDInsightComponent\" if hdi : log . info ( f \"Module { module_name } detected as HDI: { hdi } \" ) if parallel == \"auto\" : parallel = str ( module_instance . type ) == \"ParallelComponent\" if parallel : log . info ( f \"Module { module_name } detected as PARALLEL: { parallel } \" ) if mpi == \"auto\" : mpi = str ( module_instance . type ) == \"DistributedComponent\" if mpi : log . info ( f \"Module { module_name } detected as MPI: { mpi } \" ) if scope == \"auto\" : scope = str ( module_instance . type ) == \"ScopeComponent\" if scope : log . info ( f \"Module { module_name } detected as SCOPE: { scope } \" ) if sweep == \"auto\" : sweep = str ( module_instance . type ) == \"SweepComponent\" if sweep : log . info ( f \"Module { module_name } detected as SweepComponent: { sweep } \" ) if windows == \"auto\" : if ( str ( module_instance . type ) == \"HDInsightComponent\" or str ( module_instance . type ) == \"ScopeComponent\" or str ( module_instance . type ) == \"DataTransferComponent\" or str ( module_instance . type ) == \"SweepComponent\" ): # HDI/scope/datatransfer/sweep modules might not have that environment object windows = False else : windows = ( module_instance . _definition . environment . os . lower () == \"windows\" ) if windows : log . info ( f \"Module { module_name } detected as WINDOWS: { windows } \" ) if datatransfer == \"auto\" : datatransfer = str ( module_instance . type ) == \"DataTransferComponent\" if datatransfer : log . info ( f \"Module { module_name } detected as DATATRANSFER: { datatransfer } \" ) if parallel : self . _apply_parallel_runsettings ( module_name , module_instance , windows = windows , gpu = gpu , ** custom_runtime_arguments , ) return if sweep : self . _apply_sweep_runsettings ( module_name , module_instance , windows = windows , gpu = gpu , ** custom_runtime_arguments , ) return if windows : # no detonation chamber, we an't use \"use_local\" here self . _apply_windows_runsettings ( module_name , module_instance , mpi = mpi , ** custom_runtime_arguments ) return if hdi : # no detonation chamber, we an't use \"use_local\" here self . _apply_hdi_runsettings ( module_name , module_instance , ** custom_runtime_arguments ) return if scope : self . _apply_scope_runsettings ( module_name , module_instance , ** custom_runtime_arguments ) return if datatransfer : self . _apply_datatransfer_runsettings ( module_name , module_instance , ** custom_runtime_arguments ) return self . _apply_linux_runsettings ( module_name , module_instance , mpi = mpi , gpu = gpu , ** custom_runtime_arguments ) apply_smart_runsettings ( self , component_instance , gpu = False , hdi = 'auto' , windows = 'auto' , parallel = 'auto' , mpi = 'auto' , scope = 'auto' , datatransfer = 'auto' , sweep = 'auto' , ** custom_runtime_arguments ) Applies regular settings for a given component. Parameters: Name Type Description Default component_instance Component the AML component we need to add settings to required gpu bool is the component using GPU? False hdi bool is the component using HDI/Spark? 'auto' windows bool is the component using Windows compute? 'auto' parallel bool is the component using ParallelRunStep? 'auto' mpi bool is the component using Mpi? 'auto' scope bool is the component using scope? 'auto' datatransfer bool is the component using datatransfer? 'auto' sweep bool is the component using sweep? 'auto' custom_runtime_arguments dict any additional custom args {} Source code in shrike/pipeline/pipeline_helper.py def apply_smart_runsettings ( self , component_instance , gpu = False , # can't autodetect that hdi = \"auto\" , windows = \"auto\" , parallel = \"auto\" , mpi = \"auto\" , scope = \"auto\" , datatransfer = \"auto\" , sweep = \"auto\" , ** custom_runtime_arguments , ): \"\"\"Applies regular settings for a given component. Args: component_instance (Component): the AML component we need to add settings to gpu (bool): is the component using GPU? hdi (bool): is the component using HDI/Spark? windows (bool): is the component using Windows compute? parallel (bool): is the component using ParallelRunStep? mpi (bool): is the component using Mpi? scope (bool): is the component using scope? datatransfer (bool): is the component using datatransfer? sweep (bool): is the component using sweep? custom_runtime_arguments (dict): any additional custom args \"\"\" # infer component_name component_name = self . _get_component_name_from_instance ( component_instance ) self . apply_recommended_runsettings ( component_name , component_instance , gpu , hdi , windows , parallel , mpi , scope , datatransfer , sweep , ** custom_runtime_arguments , ) build ( self , config ) Builds a pipeline function for this pipeline. Parameters: Name Type Description Default config DictConfig configuration object (see get_config_class()) required Returns: Type Description pipeline_function the function to create your pipeline Source code in shrike/pipeline/pipeline_helper.py def build ( self , config ): \"\"\"Builds a pipeline function for this pipeline. Args: config (DictConfig): configuration object (see get_config_class()) Returns: pipeline_function: the function to create your pipeline \"\"\" raise NotImplementedError ( \"You need to implement your build() method.\" ) canary ( self , args , experiment , pipeline_run ) Tests the output of the pipeline Source code in shrike/pipeline/pipeline_helper.py def canary ( self , args , experiment , pipeline_run ): \"\"\"Tests the output of the pipeline\"\"\" pass component_load ( self , component_key ) Loads one component from the manifest Source code in shrike/pipeline/pipeline_helper.py def component_load ( self , component_key ) -> Callable [ ... , \"Component\" ]: \"\"\"Loads one component from the manifest\"\"\" return self . module_loader . load_module ( component_key , self . required_modules ()) connect ( self ) Connect to the AML workspace using internal config Source code in shrike/pipeline/pipeline_helper.py def connect ( self ): \"\"\"Connect to the AML workspace using internal config\"\"\" return azureml_connect ( aml_subscription_id = self . config . aml . subscription_id , aml_resource_group = self . config . aml . resource_group , aml_workspace_name = self . config . aml . workspace_name , aml_auth = self . config . aml . auth , aml_tenant = self . config . aml . tenant , aml_force = self . config . aml . force , ) # NOTE: this also stores aml workspace in internal global variable dataset_load ( self , name , version = 'latest' ) Loads a dataset by either id or name. Parameters: Name Type Description Default name str name or uuid of dataset to load required version str if loading by name, used to specify version (default \"latest\") 'latest' NOTE: in AzureML SDK there are 2 different methods for loading dataset one for id, one for name. This method just wraps them up in one. Source code in shrike/pipeline/pipeline_helper.py def dataset_load ( self , name , version = \"latest\" ): \"\"\"Loads a dataset by either id or name. Args: name (str): name or uuid of dataset to load version (str): if loading by name, used to specify version (default \"latest\") NOTE: in AzureML SDK there are 2 different methods for loading dataset one for id, one for name. This method just wraps them up in one.\"\"\" # test if given name is a uuid try : parsed_uuid = uuid . UUID ( name ) log . info ( f \"Getting a dataset handle [id= { name } ]...\" ) return Dataset . get_by_id ( self . workspace (), id = name ) except ValueError : log . info ( f \"Getting a dataset handle [name= { name } version= { version } ]...\" ) return Dataset . get_by_name ( self . workspace (), name = name , version = version ) get_config_class () classmethod Returns a dataclass containing config for this pipeline Source code in shrike/pipeline/pipeline_helper.py @classmethod def get_config_class ( cls ): \"\"\"Returns a dataclass containing config for this pipeline\"\"\" pass main () classmethod Pipeline helper main function, parses arguments and run pipeline. Source code in shrike/pipeline/pipeline_helper.py @classmethod def main ( cls ): \"\"\"Pipeline helper main function, parses arguments and run pipeline.\"\"\" config_dict = cls . _default_config () @hydra . main ( config_name = \"default\" ) def hydra_run ( cfg : DictConfig ): # merge cli config with default config cfg = OmegaConf . merge ( config_dict , cfg ) arg_parser = argparse . ArgumentParser () arg_parser . add_argument ( \"--config-dir\" ) args , _ = arg_parser . parse_known_args () cfg . run . config_dir = os . path . join ( HydraConfig . get () . runtime . cwd , args . config_dir ) log . info ( \"*** CONFIGURATION ***\" ) log . info ( OmegaConf . to_yaml ( cfg )) # create class instance main_instance = cls ( cfg ) # run main_instance . run () hydra_run () return cls . BUILT_PIPELINE # return so we can have some unit tests done module_load ( self , module_key ) Loads one module from the manifest Source code in shrike/pipeline/pipeline_helper.py def module_load ( self , module_key ): \"\"\"Loads one module from the manifest\"\"\" return self . module_loader . load_module ( module_key , self . required_modules ()) pipeline_instance ( self , pipeline_function , config ) Creates an instance of the pipeline using arguments. Parameters: Name Type Description Default pipeline_function function the pipeline function obtained from self.build() required config DictConfig configuration object (see get_config_class()) required Returns: Type Description pipeline the instance constructed using build() function Source code in shrike/pipeline/pipeline_helper.py def pipeline_instance ( self , pipeline_function , config ): \"\"\"Creates an instance of the pipeline using arguments. Args: pipeline_function (function): the pipeline function obtained from self.build() config (DictConfig): configuration object (see get_config_class()) Returns: pipeline: the instance constructed using build() function \"\"\" raise NotImplementedError ( \"You need to implement your pipeline_instance() method.\" ) required_modules () classmethod Dependencies on modules/components Returns: Type Description dict[str, dict] manifest Source code in shrike/pipeline/pipeline_helper.py @classmethod def required_modules ( cls ): \"\"\"Dependencies on modules/components Returns: dict[str, dict]: manifest \"\"\" return {} required_subgraphs () classmethod Dependencies on other subgraphs Returns: Type Description dict[str, AMLPipelineHelper] dictionary of subgraphs used for building this one. keys are whatever string you want for building your graph values should be classes inherinting from AMLPipelineHelper. Source code in shrike/pipeline/pipeline_helper.py @classmethod def required_subgraphs ( cls ): \"\"\"Dependencies on other subgraphs Returns: dict[str, AMLPipelineHelper]: dictionary of subgraphs used for building this one. keys are whatever string you want for building your graph values should be classes inherinting from AMLPipelineHelper. \"\"\" return {} run ( self ) Run pipeline using arguments Source code in shrike/pipeline/pipeline_helper.py def run ( self ): \"\"\"Run pipeline using arguments\"\"\" # Log the telemetry information in the Azure Application Insights telemetry_logger = TelemetryLogger ( enable_telemetry = not self . config . run . disable_telemetry ) telemetry_logger . log_trace ( message = f \"shrike.pipeline== { __version__ } \" , properties = { \"custom_dimensions\" : { \"configuration\" : str ( self . config )}}, ) # Check whether the experiment name is valid self . validate_experiment_name ( self . config . run . experiment_name ) self . repository_info = get_repo_info () log . info ( f \"Running from repository: { self . repository_info } \" ) log . info ( f \"azureml.core.VERSION = { azureml . core . VERSION } \" ) self . connect () if self . config . run . verbose : logging . getLogger () . setLevel ( logging . DEBUG ) pipeline_run = None if self . config . run . resume : if not self . config . run . pipeline_run_id : raise Exception ( \"To be able to use --resume you need to provide both --experiment-name and --run-id.\" ) log . info ( f \"Resuming Experiment { self . config . run . experiment_name } ...\" ) experiment = Experiment ( current_workspace (), self . config . run . experiment_name ) log . info ( f \"Resuming PipelineRun { self . config . run . pipeline_run_id } ...\" ) # pipeline_run is of the class \"azureml.pipeline.core.PipelineRun\" pipeline_run = PipelineRun ( experiment , self . config . run . pipeline_run_id ) else : keep_modified_files , override = False , False yaml_to_be_recovered = [] if self . config . tenant_overrides . allow_override : log . info ( \"Check if tenant is consistent with spec yaml\" ) override , mapping = self . _check_if_spec_yaml_override_is_needed () if override : try : tenant = self . config . aml . tenant log . info ( f \"Performing spec yaml override to adapt to tenant: { tenant } .\" ) yaml_to_be_recovered = self . _override_spec_yaml ( mapping ) keep_modified_files = ( self . config . tenant_overrides . keep_modified_files ) except BaseException as e : log . error ( f \"An error occured, override is not successful: { e } \" ) pipeline_run = self . build_and_submit_new_pipeline () if override and yaml_to_be_recovered : try : self . _recover_spec_yaml ( yaml_to_be_recovered , keep_modified_files ) except BaseException as e : log . error ( f \"An error occured, recovery is not successful: { e } \" ) if not pipeline_run : # not submitting code, exit now return # launch the pipeline execution log . info ( f \"Pipeline Run Id: { pipeline_run . id } \" ) log . info ( f \"\"\" ################################# ################################# ################################# Follow link below to access your pipeline run directly: ------------------------------------------------------- { pipeline_run . get_portal_url () } ################################# ################################# ################################# \"\"\" ) if self . config . run . canary : log . info ( \"*** CANARY MODE *** \\n ----------------------------------------------------------\" ) pipeline_run . wait_for_completion ( show_output = True ) # azureml.pipeline.core.PipelineRun.get_status(): [\"Running\", \"Finished\", \"Failed\"] # azureml.core.run.get_status(): [\"Running\", \"Completed\", \"Failed\"] if pipeline_run . get_status () in [ \"Finished\" , \"Completed\" ]: log . info ( \"*** PIPELINE FINISHED, TESTING WITH canary() METHOD ***\" ) self . canary ( self . config , pipeline_run . experiment , pipeline_run ) log . info ( \"OK\" ) elif pipeline_run . get_status () == \"Failed\" : log . info ( \"*** PIPELINE FAILED ***\" ) raise Exception ( \"Pipeline failed.\" ) else : log . info ( \"*** PIPELINE STATUS {} UNKNOWN ***\" ) raise Exception ( \"Pipeline status is unknown.\" ) else : if not self . config . run . silent : webbrowser . open ( url = pipeline_run . get_portal_url ()) # This will wait for the completion of the pipeline execution # and show the full logs in the meantime if self . config . run . resume or self . config . run . wait : log . info ( \"Below are the raw debug logs from your pipeline execution: \\n ----------------------------------------------------------\" ) pipeline_run . wait_for_completion ( show_output = True ) subgraph_load ( self , subgraph_key , custom_config = {}) Loads one subgraph from the manifest Parameters: Name Type Description Default subgraph_key str subgraph identifier that is used in the required_subgraphs() method required custom_config DictConfig custom configuration object, this custom object witll be {} Source code in shrike/pipeline/pipeline_helper.py def subgraph_load ( self , subgraph_key , custom_config = OmegaConf . create ()) -> Callable : \"\"\"Loads one subgraph from the manifest Args: subgraph_key (str): subgraph identifier that is used in the required_subgraphs() method custom_config (DictConfig): custom configuration object, this custom object witll be added to the pipeline config \"\"\" subgraph_class = self . required_subgraphs ()[ subgraph_key ] subgraph_config = self . config . copy () if custom_config : with open_dict ( subgraph_config ): for key in custom_config : subgraph_config [ key ] = custom_config [ key ] log . info ( f \"Building subgraph [ { subgraph_key } as { subgraph_class . __name__ } ]...\" ) # NOTE: below creates subgraph with updated pipeline_config subgraph_instance = subgraph_class ( config = subgraph_config , module_loader = self . module_loader ) # subgraph_instance.setup(self.pipeline_config) return subgraph_instance . build ( subgraph_config ) validate_experiment_name ( name ) staticmethod Check whether the experiment name is valid. It's required that experiment names must be between 1 to 250 characters, start\u202fwith letters or numbers. Valid characters are letters, numbers, \"_\", and the \"-\" character. Source code in shrike/pipeline/pipeline_helper.py @staticmethod def validate_experiment_name ( name ): \"\"\" Check whether the experiment name is valid. It's required that experiment names must be between 1 to 250 characters, start\u202fwith letters or numbers. Valid characters are letters, numbers, \"_\", and the \"-\" character. \"\"\" if len ( name ) < 1 or len ( name ) > 250 : raise ValueError ( \"Experiment names must be between 1 to 250 characters!\" ) if not re . match ( \"^[a-zA-Z0-9]$\" , name [ 0 ]): raise ValueError ( \"Experiment names must start\u202fwith letters or numbers!\" ) if not re . match ( \"^[a-zA-Z0-9_-]*$\" , name ): raise ValueError ( \"Valid experiment names must only contain letters, numbers, underscore and dash!\" ) return True workspace ( self ) Gets the current workspace Source code in shrike/pipeline/pipeline_helper.py def workspace ( self ): \"\"\"Gets the current workspace\"\"\" return current_workspace ()","title":"pipeline_helper"},{"location":"pipeline/pipeline-helper/#pipeline-helper","text":"Pipeline helper class to create pipelines loading modules from a flexible manifest.","title":"Pipeline helper"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper","text":"Helper class for building pipelines","title":"AMLPipelineHelper"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.__init__","text":"Constructs the pipeline helper. Parameters: Name Type Description Default config DictConfig config for this object required module_loader AMLModuleLoader which module loader to (re)use None Source code in shrike/pipeline/pipeline_helper.py def __init__ ( self , config , module_loader = None ): \"\"\"Constructs the pipeline helper. Args: config (DictConfig): config for this object module_loader (AMLModuleLoader): which module loader to (re)use \"\"\" self . config = config if module_loader is None : log . info ( f \"Creating instance of AMLModuleLoader for { self . __class__ . __name__ } \" ) self . module_loader = AMLModuleLoader ( self . config ) else : self . module_loader = module_loader","title":"__init__()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.apply_recommended_runsettings","text":"Applies regular settings for a given module. Parameters: Name Type Description Default module_name str name of the module from the module manifest (required_modules() method) required module_instance Module the AML module we need to add settings to required gpu bool is the module using GPU? False hdi bool is the module using HDI/Spark? 'auto' windows bool is the module using Windows compute? 'auto' parallel bool is the module using ParallelRunStep? 'auto' mpi bool is the module using Mpi? 'auto' custom_runtime_arguments dict any additional custom args {} Source code in shrike/pipeline/pipeline_helper.py def apply_recommended_runsettings ( self , module_name , module_instance , gpu = False , # can't autodetect that hdi = \"auto\" , windows = \"auto\" , parallel = \"auto\" , mpi = \"auto\" , scope = \"auto\" , datatransfer = \"auto\" , sweep = \"auto\" , ** custom_runtime_arguments , ): \"\"\"Applies regular settings for a given module. Args: module_name (str): name of the module from the module manifest (required_modules() method) module_instance (Module): the AML module we need to add settings to gpu (bool): is the module using GPU? hdi (bool): is the module using HDI/Spark? windows (bool): is the module using Windows compute? parallel (bool): is the module using ParallelRunStep? mpi (bool): is the module using Mpi? custom_runtime_arguments (dict): any additional custom args \"\"\" # verifies if module_name corresponds to module_instance self . _check_module_runsettings_consistency ( module_name , module_instance ) # Auto detect runsettings if hdi == \"auto\" : hdi = str ( module_instance . type ) == \"HDInsightComponent\" if hdi : log . info ( f \"Module { module_name } detected as HDI: { hdi } \" ) if parallel == \"auto\" : parallel = str ( module_instance . type ) == \"ParallelComponent\" if parallel : log . info ( f \"Module { module_name } detected as PARALLEL: { parallel } \" ) if mpi == \"auto\" : mpi = str ( module_instance . type ) == \"DistributedComponent\" if mpi : log . info ( f \"Module { module_name } detected as MPI: { mpi } \" ) if scope == \"auto\" : scope = str ( module_instance . type ) == \"ScopeComponent\" if scope : log . info ( f \"Module { module_name } detected as SCOPE: { scope } \" ) if sweep == \"auto\" : sweep = str ( module_instance . type ) == \"SweepComponent\" if sweep : log . info ( f \"Module { module_name } detected as SweepComponent: { sweep } \" ) if windows == \"auto\" : if ( str ( module_instance . type ) == \"HDInsightComponent\" or str ( module_instance . type ) == \"ScopeComponent\" or str ( module_instance . type ) == \"DataTransferComponent\" or str ( module_instance . type ) == \"SweepComponent\" ): # HDI/scope/datatransfer/sweep modules might not have that environment object windows = False else : windows = ( module_instance . _definition . environment . os . lower () == \"windows\" ) if windows : log . info ( f \"Module { module_name } detected as WINDOWS: { windows } \" ) if datatransfer == \"auto\" : datatransfer = str ( module_instance . type ) == \"DataTransferComponent\" if datatransfer : log . info ( f \"Module { module_name } detected as DATATRANSFER: { datatransfer } \" ) if parallel : self . _apply_parallel_runsettings ( module_name , module_instance , windows = windows , gpu = gpu , ** custom_runtime_arguments , ) return if sweep : self . _apply_sweep_runsettings ( module_name , module_instance , windows = windows , gpu = gpu , ** custom_runtime_arguments , ) return if windows : # no detonation chamber, we an't use \"use_local\" here self . _apply_windows_runsettings ( module_name , module_instance , mpi = mpi , ** custom_runtime_arguments ) return if hdi : # no detonation chamber, we an't use \"use_local\" here self . _apply_hdi_runsettings ( module_name , module_instance , ** custom_runtime_arguments ) return if scope : self . _apply_scope_runsettings ( module_name , module_instance , ** custom_runtime_arguments ) return if datatransfer : self . _apply_datatransfer_runsettings ( module_name , module_instance , ** custom_runtime_arguments ) return self . _apply_linux_runsettings ( module_name , module_instance , mpi = mpi , gpu = gpu , ** custom_runtime_arguments )","title":"apply_recommended_runsettings()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.apply_smart_runsettings","text":"Applies regular settings for a given component. Parameters: Name Type Description Default component_instance Component the AML component we need to add settings to required gpu bool is the component using GPU? False hdi bool is the component using HDI/Spark? 'auto' windows bool is the component using Windows compute? 'auto' parallel bool is the component using ParallelRunStep? 'auto' mpi bool is the component using Mpi? 'auto' scope bool is the component using scope? 'auto' datatransfer bool is the component using datatransfer? 'auto' sweep bool is the component using sweep? 'auto' custom_runtime_arguments dict any additional custom args {} Source code in shrike/pipeline/pipeline_helper.py def apply_smart_runsettings ( self , component_instance , gpu = False , # can't autodetect that hdi = \"auto\" , windows = \"auto\" , parallel = \"auto\" , mpi = \"auto\" , scope = \"auto\" , datatransfer = \"auto\" , sweep = \"auto\" , ** custom_runtime_arguments , ): \"\"\"Applies regular settings for a given component. Args: component_instance (Component): the AML component we need to add settings to gpu (bool): is the component using GPU? hdi (bool): is the component using HDI/Spark? windows (bool): is the component using Windows compute? parallel (bool): is the component using ParallelRunStep? mpi (bool): is the component using Mpi? scope (bool): is the component using scope? datatransfer (bool): is the component using datatransfer? sweep (bool): is the component using sweep? custom_runtime_arguments (dict): any additional custom args \"\"\" # infer component_name component_name = self . _get_component_name_from_instance ( component_instance ) self . apply_recommended_runsettings ( component_name , component_instance , gpu , hdi , windows , parallel , mpi , scope , datatransfer , sweep , ** custom_runtime_arguments , )","title":"apply_smart_runsettings()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.build","text":"Builds a pipeline function for this pipeline. Parameters: Name Type Description Default config DictConfig configuration object (see get_config_class()) required Returns: Type Description pipeline_function the function to create your pipeline Source code in shrike/pipeline/pipeline_helper.py def build ( self , config ): \"\"\"Builds a pipeline function for this pipeline. Args: config (DictConfig): configuration object (see get_config_class()) Returns: pipeline_function: the function to create your pipeline \"\"\" raise NotImplementedError ( \"You need to implement your build() method.\" )","title":"build()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.canary","text":"Tests the output of the pipeline Source code in shrike/pipeline/pipeline_helper.py def canary ( self , args , experiment , pipeline_run ): \"\"\"Tests the output of the pipeline\"\"\" pass","title":"canary()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.component_load","text":"Loads one component from the manifest Source code in shrike/pipeline/pipeline_helper.py def component_load ( self , component_key ) -> Callable [ ... , \"Component\" ]: \"\"\"Loads one component from the manifest\"\"\" return self . module_loader . load_module ( component_key , self . required_modules ())","title":"component_load()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.connect","text":"Connect to the AML workspace using internal config Source code in shrike/pipeline/pipeline_helper.py def connect ( self ): \"\"\"Connect to the AML workspace using internal config\"\"\" return azureml_connect ( aml_subscription_id = self . config . aml . subscription_id , aml_resource_group = self . config . aml . resource_group , aml_workspace_name = self . config . aml . workspace_name , aml_auth = self . config . aml . auth , aml_tenant = self . config . aml . tenant , aml_force = self . config . aml . force , ) # NOTE: this also stores aml workspace in internal global variable","title":"connect()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.dataset_load","text":"Loads a dataset by either id or name. Parameters: Name Type Description Default name str name or uuid of dataset to load required version str if loading by name, used to specify version (default \"latest\") 'latest' NOTE: in AzureML SDK there are 2 different methods for loading dataset one for id, one for name. This method just wraps them up in one. Source code in shrike/pipeline/pipeline_helper.py def dataset_load ( self , name , version = \"latest\" ): \"\"\"Loads a dataset by either id or name. Args: name (str): name or uuid of dataset to load version (str): if loading by name, used to specify version (default \"latest\") NOTE: in AzureML SDK there are 2 different methods for loading dataset one for id, one for name. This method just wraps them up in one.\"\"\" # test if given name is a uuid try : parsed_uuid = uuid . UUID ( name ) log . info ( f \"Getting a dataset handle [id= { name } ]...\" ) return Dataset . get_by_id ( self . workspace (), id = name ) except ValueError : log . info ( f \"Getting a dataset handle [name= { name } version= { version } ]...\" ) return Dataset . get_by_name ( self . workspace (), name = name , version = version )","title":"dataset_load()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.get_config_class","text":"Returns a dataclass containing config for this pipeline Source code in shrike/pipeline/pipeline_helper.py @classmethod def get_config_class ( cls ): \"\"\"Returns a dataclass containing config for this pipeline\"\"\" pass","title":"get_config_class()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.main","text":"Pipeline helper main function, parses arguments and run pipeline. Source code in shrike/pipeline/pipeline_helper.py @classmethod def main ( cls ): \"\"\"Pipeline helper main function, parses arguments and run pipeline.\"\"\" config_dict = cls . _default_config () @hydra . main ( config_name = \"default\" ) def hydra_run ( cfg : DictConfig ): # merge cli config with default config cfg = OmegaConf . merge ( config_dict , cfg ) arg_parser = argparse . ArgumentParser () arg_parser . add_argument ( \"--config-dir\" ) args , _ = arg_parser . parse_known_args () cfg . run . config_dir = os . path . join ( HydraConfig . get () . runtime . cwd , args . config_dir ) log . info ( \"*** CONFIGURATION ***\" ) log . info ( OmegaConf . to_yaml ( cfg )) # create class instance main_instance = cls ( cfg ) # run main_instance . run () hydra_run () return cls . BUILT_PIPELINE # return so we can have some unit tests done","title":"main()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.module_load","text":"Loads one module from the manifest Source code in shrike/pipeline/pipeline_helper.py def module_load ( self , module_key ): \"\"\"Loads one module from the manifest\"\"\" return self . module_loader . load_module ( module_key , self . required_modules ())","title":"module_load()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.pipeline_instance","text":"Creates an instance of the pipeline using arguments. Parameters: Name Type Description Default pipeline_function function the pipeline function obtained from self.build() required config DictConfig configuration object (see get_config_class()) required Returns: Type Description pipeline the instance constructed using build() function Source code in shrike/pipeline/pipeline_helper.py def pipeline_instance ( self , pipeline_function , config ): \"\"\"Creates an instance of the pipeline using arguments. Args: pipeline_function (function): the pipeline function obtained from self.build() config (DictConfig): configuration object (see get_config_class()) Returns: pipeline: the instance constructed using build() function \"\"\" raise NotImplementedError ( \"You need to implement your pipeline_instance() method.\" )","title":"pipeline_instance()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.required_modules","text":"Dependencies on modules/components Returns: Type Description dict[str, dict] manifest Source code in shrike/pipeline/pipeline_helper.py @classmethod def required_modules ( cls ): \"\"\"Dependencies on modules/components Returns: dict[str, dict]: manifest \"\"\" return {}","title":"required_modules()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.required_subgraphs","text":"Dependencies on other subgraphs Returns: Type Description dict[str, AMLPipelineHelper] dictionary of subgraphs used for building this one. keys are whatever string you want for building your graph values should be classes inherinting from AMLPipelineHelper. Source code in shrike/pipeline/pipeline_helper.py @classmethod def required_subgraphs ( cls ): \"\"\"Dependencies on other subgraphs Returns: dict[str, AMLPipelineHelper]: dictionary of subgraphs used for building this one. keys are whatever string you want for building your graph values should be classes inherinting from AMLPipelineHelper. \"\"\" return {}","title":"required_subgraphs()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.run","text":"Run pipeline using arguments Source code in shrike/pipeline/pipeline_helper.py def run ( self ): \"\"\"Run pipeline using arguments\"\"\" # Log the telemetry information in the Azure Application Insights telemetry_logger = TelemetryLogger ( enable_telemetry = not self . config . run . disable_telemetry ) telemetry_logger . log_trace ( message = f \"shrike.pipeline== { __version__ } \" , properties = { \"custom_dimensions\" : { \"configuration\" : str ( self . config )}}, ) # Check whether the experiment name is valid self . validate_experiment_name ( self . config . run . experiment_name ) self . repository_info = get_repo_info () log . info ( f \"Running from repository: { self . repository_info } \" ) log . info ( f \"azureml.core.VERSION = { azureml . core . VERSION } \" ) self . connect () if self . config . run . verbose : logging . getLogger () . setLevel ( logging . DEBUG ) pipeline_run = None if self . config . run . resume : if not self . config . run . pipeline_run_id : raise Exception ( \"To be able to use --resume you need to provide both --experiment-name and --run-id.\" ) log . info ( f \"Resuming Experiment { self . config . run . experiment_name } ...\" ) experiment = Experiment ( current_workspace (), self . config . run . experiment_name ) log . info ( f \"Resuming PipelineRun { self . config . run . pipeline_run_id } ...\" ) # pipeline_run is of the class \"azureml.pipeline.core.PipelineRun\" pipeline_run = PipelineRun ( experiment , self . config . run . pipeline_run_id ) else : keep_modified_files , override = False , False yaml_to_be_recovered = [] if self . config . tenant_overrides . allow_override : log . info ( \"Check if tenant is consistent with spec yaml\" ) override , mapping = self . _check_if_spec_yaml_override_is_needed () if override : try : tenant = self . config . aml . tenant log . info ( f \"Performing spec yaml override to adapt to tenant: { tenant } .\" ) yaml_to_be_recovered = self . _override_spec_yaml ( mapping ) keep_modified_files = ( self . config . tenant_overrides . keep_modified_files ) except BaseException as e : log . error ( f \"An error occured, override is not successful: { e } \" ) pipeline_run = self . build_and_submit_new_pipeline () if override and yaml_to_be_recovered : try : self . _recover_spec_yaml ( yaml_to_be_recovered , keep_modified_files ) except BaseException as e : log . error ( f \"An error occured, recovery is not successful: { e } \" ) if not pipeline_run : # not submitting code, exit now return # launch the pipeline execution log . info ( f \"Pipeline Run Id: { pipeline_run . id } \" ) log . info ( f \"\"\" ################################# ################################# ################################# Follow link below to access your pipeline run directly: ------------------------------------------------------- { pipeline_run . get_portal_url () } ################################# ################################# ################################# \"\"\" ) if self . config . run . canary : log . info ( \"*** CANARY MODE *** \\n ----------------------------------------------------------\" ) pipeline_run . wait_for_completion ( show_output = True ) # azureml.pipeline.core.PipelineRun.get_status(): [\"Running\", \"Finished\", \"Failed\"] # azureml.core.run.get_status(): [\"Running\", \"Completed\", \"Failed\"] if pipeline_run . get_status () in [ \"Finished\" , \"Completed\" ]: log . info ( \"*** PIPELINE FINISHED, TESTING WITH canary() METHOD ***\" ) self . canary ( self . config , pipeline_run . experiment , pipeline_run ) log . info ( \"OK\" ) elif pipeline_run . get_status () == \"Failed\" : log . info ( \"*** PIPELINE FAILED ***\" ) raise Exception ( \"Pipeline failed.\" ) else : log . info ( \"*** PIPELINE STATUS {} UNKNOWN ***\" ) raise Exception ( \"Pipeline status is unknown.\" ) else : if not self . config . run . silent : webbrowser . open ( url = pipeline_run . get_portal_url ()) # This will wait for the completion of the pipeline execution # and show the full logs in the meantime if self . config . run . resume or self . config . run . wait : log . info ( \"Below are the raw debug logs from your pipeline execution: \\n ----------------------------------------------------------\" ) pipeline_run . wait_for_completion ( show_output = True )","title":"run()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.subgraph_load","text":"Loads one subgraph from the manifest Parameters: Name Type Description Default subgraph_key str subgraph identifier that is used in the required_subgraphs() method required custom_config DictConfig custom configuration object, this custom object witll be {} Source code in shrike/pipeline/pipeline_helper.py def subgraph_load ( self , subgraph_key , custom_config = OmegaConf . create ()) -> Callable : \"\"\"Loads one subgraph from the manifest Args: subgraph_key (str): subgraph identifier that is used in the required_subgraphs() method custom_config (DictConfig): custom configuration object, this custom object witll be added to the pipeline config \"\"\" subgraph_class = self . required_subgraphs ()[ subgraph_key ] subgraph_config = self . config . copy () if custom_config : with open_dict ( subgraph_config ): for key in custom_config : subgraph_config [ key ] = custom_config [ key ] log . info ( f \"Building subgraph [ { subgraph_key } as { subgraph_class . __name__ } ]...\" ) # NOTE: below creates subgraph with updated pipeline_config subgraph_instance = subgraph_class ( config = subgraph_config , module_loader = self . module_loader ) # subgraph_instance.setup(self.pipeline_config) return subgraph_instance . build ( subgraph_config )","title":"subgraph_load()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.validate_experiment_name","text":"Check whether the experiment name is valid. It's required that experiment names must be between 1 to 250 characters, start\u202fwith letters or numbers. Valid characters are letters, numbers, \"_\", and the \"-\" character. Source code in shrike/pipeline/pipeline_helper.py @staticmethod def validate_experiment_name ( name ): \"\"\" Check whether the experiment name is valid. It's required that experiment names must be between 1 to 250 characters, start\u202fwith letters or numbers. Valid characters are letters, numbers, \"_\", and the \"-\" character. \"\"\" if len ( name ) < 1 or len ( name ) > 250 : raise ValueError ( \"Experiment names must be between 1 to 250 characters!\" ) if not re . match ( \"^[a-zA-Z0-9]$\" , name [ 0 ]): raise ValueError ( \"Experiment names must start\u202fwith letters or numbers!\" ) if not re . match ( \"^[a-zA-Z0-9_-]*$\" , name ): raise ValueError ( \"Valid experiment names must only contain letters, numbers, underscore and dash!\" ) return True","title":"validate_experiment_name()"},{"location":"pipeline/pipeline-helper/#shrike.pipeline.pipeline_helper.AMLPipelineHelper.workspace","text":"Gets the current workspace Source code in shrike/pipeline/pipeline_helper.py def workspace ( self ): \"\"\"Gets the current workspace\"\"\" return current_workspace ()","title":"workspace()"},{"location":"pipeline/reuse-aml-pipeline/","text":"General process for reusing and configuring an existing AML experiment Motivations Essentially, an experiment is made of: - a runnable python script, - a configuration file. They are checked in a repository, making it reusable and shareable in your team. In many cases, it will be easy to copy an existing experiment to make it your own by just modifying the configuration file. In some instances, you'll want to modify the python script as well to have a more in-depth change on the structure of the graph. 1. Branch from an existing experiment In this section, we'll assume you have identified an existing runnable script ( e.g. : demograph_eyesoff.py here in the accelerator repo) and a configuration file ( e.g. demograph_eyesoff.yaml there in the accelerator repo) 1. Create a new branch in your team's repository (to avoid conflicts). In the config/experiments/ folder , identify a config file you want to start from. Copy this file into a new configuration file of your own. If this configuration file has a module manifest specified in the defaults section, as an example: defaults : - aml : eyesoff # default aml references - compute : eyesoff # default compute target names - modules : module_defaults # list of modules + versions, see config/modules/ Copy the file under config/experiments/demograph_eyesoff.yaml to a file of your own (ex: demograph_eyesoff_modelX.yaml ), and rename the name under defaults in your experiment config accordingly. defaults : - aml : eyesoff # default aml references - compute : eyesoff # default compute target names - modules : modules_modelX # <<< modify here In the following sections, you will be able to pin each component in this manifest to a particular version number, different from your colleagues version numbers. In the run section, modify your experiment name: run : # params for running pipeline experiment_name : \"modelX\" # <<< modify here Note: we recommend to use a different experiment for each project you work on. Experiments are essentially just a folder to put all your runs in order. That's it, at this point we invite you to try running the script for validating the graph (see below as an example): python pipelines / experiments / demograph_eyesoff . py - -config-dir pipelines / config - -config-name experiments / demograph_eyesoff_modelX This will try to build the graph, but will not submit it as an experiment. You're ready to modify the experiment now. 2. Modifying the code (best practices) Here's a couple recommendations depending on what you want to do. Your experiment consists in modifying components only If your experiment consists in modifying one or several components only (not the structure of the graph). work in a branch containing your component code use local code to experiment create a specific configuration file for your experiment that shows which component versions to use when satisfied, merge in to main/master and register the new versions for your team to share. Your experiment consists in modifying the graph structure work in a branch containing your graph identify conflicts of versions with your team and either create a new graph or modify the existing one with options to switch on/off your changes create a specific configuration file for your experiment that shows which component versions to use when satisfied, merge in to main/master so that the team can reuse the new graph 3. Experiment with local code If you want to modify a particular component for your experiment, we recommend to iterate on the component code using detonation chamber. IMPORTANT : this is NOT possible for HDI components. If you want to modify HDI components, we recommend to test those first in eyes-on, or to register new versions of those HDI components in parallel of your experiment branch (create a specific branch for your new component versions). To use the local code for a given component: Identify the component key Identify the component key , this is the key used to map to the component in the graphs/subgraphs code. Go to the graph or subgraph you want to modify, check in the build() function to identify the component load key used. For instance below we want to modify VocabGenerator : def build ( self , config ): # ... vocab_generator = self . module_load ( \"VocabGenerator\" ) # ... If your pipeline uses the required_modules() method, this key will match with an entry in the required modules dictionary: @classmethod def required_modules ( cls ): return { \"VocabGenerator\" :{ # references of remote module \"remote_module_name\" : \"SparkVocabGenerator\" , \"namespace\" : \"microsoft.com/office/smartcompose\" , \"version\" : None , # references of local module \"yaml_spec\" : \"spark_vocab_generator/module_spec.yaml\" }, The key here is VocabGenerator , which is not the component name, but its key in the required modules dictionary. If your pipeline uses a module manifest in YAML (recommended!), this key will map to an entry in the modules manifest file config/modules/modules_modelX.yaml : manifest : # ... - key : \"VocabGenerator\" name : \"microsoft.com/office/smartcompose://SparkVocabGenerator\" version : null yaml : \"spark_vocab_generator/module_spec.yaml\" # ... The key here is VocabGenerator , which is not the component name, but its key in the required modules dictionary. Note: if no key is specified, the name is used as key. Use component key to run this component locally Use the use_local command with that key. You can either add it to the command line: python pipelines / experiments / demograph_eyesoff . py - -config-dir pipelines / config - -config-name experiments / demograph_eyesoff_modelX module_loader . use_local = \"VocabGenerator\" Or you can write it in your configuration file: # module_loader module_loader : # module loading params # IMPORTANT: if you want to modify a given module, add its key here # see the code for identifying the module key # use comma separation in this string to use multiple local modules use_local : \"VocabGenerator\" Note: this is example for illustrative purposesonly; the accelerator repo and the demo eyes-off graph do not really have a VocabGenerator component. From shrike v1.4.0 , you could also specify \"all local except for a few\", using the syntax: # module_loader module_loader : use_local=\"!KEY1, !KEY2\" or from command line override: python < your_pipeline_and_config > module_loader . use_local = \"'!KEY1, !KEY2'\" where only components KEY1 and KEY2 are remote, and all the other components are local. In summary, four patterns are allowed - use_local = \"\", all components are remote - use_local = \"*\", all components are local - use_local = \"KEY1, KEY2\", components KEY1 and KEY2 are local - use_local = \"!KEY1, !KEY2\", all components except for KEY1 and KEY2 are local :warning: \"Mix\" (e.g., use_local=\"KEY1, !KEY2\") is not allowed. 2. When running the experiment, watch-out in the logs for a line that will indicate this component has loaded from local code: Building subgraph [Encoding as EncodingHdiPipeline]... --- Building module from local code at spark_vocab_generator/module_spec.yaml 4. Experiment with different versions of (registered) components The way the helper code decides which version to use for a given component M is (in order): if module_loader.force_all_module_version is set, use it as version for component M (and all others) if a version is set under component M in modules.manifest , use it if a version is hardcoded in required_modules() for component M, use it if module_loader.force_default_module_version is set, use it as version for component M (and all others non specified versions) else, use default version registered in Azure ML (usually, the latest). Version management for your experiment components can have multiple use cases. Use a specific version for all unspecified ( force_default_module_version ) If all your components versions are synchronized in the registration build, you can use this to use a single version number accross all the graph for all components that have unspecified versions ( version:null ). If you want to pin down a specific version number outside of this, you can add a specific version in your module manifest, or in the required_modules() method. Use a specific version for all components If all your components versions are synchronized in the registration build, you can use this to use a single version number accross all the graph for all components. This will give you an exact replication of the components at a particular point in time. This will override all other version settings. Use specific versions for some components If you want to pin down a specific version number for some particular components, specify this version in the module manifest: # @package _group_ manifest : # ... - key : \"LMPytorchTrainer\" name : \"[SC] [AML] PyTorch LM Trainer\" version : null # <<< HERE yaml : \"pytorch_trainer/module_spec.yaml\" # ... or hardcode it (not recommended) in your required_modules() method: @classmethod def required_modules ( cls ): return { \"LMPytorchTrainer\" :{ \"remote_module_name\" : \"[SC] [AML] PyTorch LM Trainer\" , \"namespace\" : \"microsoft.com/office/smartcompose\" , \"version\" : None , # <<< HERE \"yaml_spec\" : \"pytorch_trainer/module_spec.yaml\" } }","title":"Reuse your AML pipeline"},{"location":"pipeline/reuse-aml-pipeline/#general-process-for-reusing-and-configuring-an-existing-aml-experiment","text":"","title":"General process for reusing and configuring an existing AML experiment"},{"location":"pipeline/reuse-aml-pipeline/#motivations","text":"Essentially, an experiment is made of: - a runnable python script, - a configuration file. They are checked in a repository, making it reusable and shareable in your team. In many cases, it will be easy to copy an existing experiment to make it your own by just modifying the configuration file. In some instances, you'll want to modify the python script as well to have a more in-depth change on the structure of the graph.","title":"Motivations"},{"location":"pipeline/reuse-aml-pipeline/#1-branch-from-an-existing-experiment","text":"In this section, we'll assume you have identified an existing runnable script ( e.g. : demograph_eyesoff.py here in the accelerator repo) and a configuration file ( e.g. demograph_eyesoff.yaml there in the accelerator repo) 1. Create a new branch in your team's repository (to avoid conflicts). In the config/experiments/ folder , identify a config file you want to start from. Copy this file into a new configuration file of your own. If this configuration file has a module manifest specified in the defaults section, as an example: defaults : - aml : eyesoff # default aml references - compute : eyesoff # default compute target names - modules : module_defaults # list of modules + versions, see config/modules/ Copy the file under config/experiments/demograph_eyesoff.yaml to a file of your own (ex: demograph_eyesoff_modelX.yaml ), and rename the name under defaults in your experiment config accordingly. defaults : - aml : eyesoff # default aml references - compute : eyesoff # default compute target names - modules : modules_modelX # <<< modify here In the following sections, you will be able to pin each component in this manifest to a particular version number, different from your colleagues version numbers. In the run section, modify your experiment name: run : # params for running pipeline experiment_name : \"modelX\" # <<< modify here Note: we recommend to use a different experiment for each project you work on. Experiments are essentially just a folder to put all your runs in order. That's it, at this point we invite you to try running the script for validating the graph (see below as an example): python pipelines / experiments / demograph_eyesoff . py - -config-dir pipelines / config - -config-name experiments / demograph_eyesoff_modelX This will try to build the graph, but will not submit it as an experiment. You're ready to modify the experiment now.","title":"1. Branch from an existing experiment"},{"location":"pipeline/reuse-aml-pipeline/#2-modifying-the-code-best-practices","text":"Here's a couple recommendations depending on what you want to do.","title":"2. Modifying the code (best practices)"},{"location":"pipeline/reuse-aml-pipeline/#your-experiment-consists-in-modifying-components-only","text":"If your experiment consists in modifying one or several components only (not the structure of the graph). work in a branch containing your component code use local code to experiment create a specific configuration file for your experiment that shows which component versions to use when satisfied, merge in to main/master and register the new versions for your team to share.","title":"Your experiment consists in modifying components only"},{"location":"pipeline/reuse-aml-pipeline/#your-experiment-consists-in-modifying-the-graph-structure","text":"work in a branch containing your graph identify conflicts of versions with your team and either create a new graph or modify the existing one with options to switch on/off your changes create a specific configuration file for your experiment that shows which component versions to use when satisfied, merge in to main/master so that the team can reuse the new graph","title":"Your experiment consists in modifying the graph structure"},{"location":"pipeline/reuse-aml-pipeline/#3-experiment-with-local-code","text":"If you want to modify a particular component for your experiment, we recommend to iterate on the component code using detonation chamber. IMPORTANT : this is NOT possible for HDI components. If you want to modify HDI components, we recommend to test those first in eyes-on, or to register new versions of those HDI components in parallel of your experiment branch (create a specific branch for your new component versions). To use the local code for a given component:","title":"3. Experiment with local code"},{"location":"pipeline/reuse-aml-pipeline/#identify-the-component-key","text":"Identify the component key , this is the key used to map to the component in the graphs/subgraphs code. Go to the graph or subgraph you want to modify, check in the build() function to identify the component load key used. For instance below we want to modify VocabGenerator : def build ( self , config ): # ... vocab_generator = self . module_load ( \"VocabGenerator\" ) # ... If your pipeline uses the required_modules() method, this key will match with an entry in the required modules dictionary: @classmethod def required_modules ( cls ): return { \"VocabGenerator\" :{ # references of remote module \"remote_module_name\" : \"SparkVocabGenerator\" , \"namespace\" : \"microsoft.com/office/smartcompose\" , \"version\" : None , # references of local module \"yaml_spec\" : \"spark_vocab_generator/module_spec.yaml\" }, The key here is VocabGenerator , which is not the component name, but its key in the required modules dictionary. If your pipeline uses a module manifest in YAML (recommended!), this key will map to an entry in the modules manifest file config/modules/modules_modelX.yaml : manifest : # ... - key : \"VocabGenerator\" name : \"microsoft.com/office/smartcompose://SparkVocabGenerator\" version : null yaml : \"spark_vocab_generator/module_spec.yaml\" # ... The key here is VocabGenerator , which is not the component name, but its key in the required modules dictionary. Note: if no key is specified, the name is used as key.","title":"Identify the component key"},{"location":"pipeline/reuse-aml-pipeline/#use-component-key-to-run-this-component-locally","text":"Use the use_local command with that key. You can either add it to the command line: python pipelines / experiments / demograph_eyesoff . py - -config-dir pipelines / config - -config-name experiments / demograph_eyesoff_modelX module_loader . use_local = \"VocabGenerator\" Or you can write it in your configuration file: # module_loader module_loader : # module loading params # IMPORTANT: if you want to modify a given module, add its key here # see the code for identifying the module key # use comma separation in this string to use multiple local modules use_local : \"VocabGenerator\" Note: this is example for illustrative purposesonly; the accelerator repo and the demo eyes-off graph do not really have a VocabGenerator component. From shrike v1.4.0 , you could also specify \"all local except for a few\", using the syntax: # module_loader module_loader : use_local=\"!KEY1, !KEY2\" or from command line override: python < your_pipeline_and_config > module_loader . use_local = \"'!KEY1, !KEY2'\" where only components KEY1 and KEY2 are remote, and all the other components are local. In summary, four patterns are allowed - use_local = \"\", all components are remote - use_local = \"*\", all components are local - use_local = \"KEY1, KEY2\", components KEY1 and KEY2 are local - use_local = \"!KEY1, !KEY2\", all components except for KEY1 and KEY2 are local :warning: \"Mix\" (e.g., use_local=\"KEY1, !KEY2\") is not allowed. 2. When running the experiment, watch-out in the logs for a line that will indicate this component has loaded from local code: Building subgraph [Encoding as EncodingHdiPipeline]... --- Building module from local code at spark_vocab_generator/module_spec.yaml","title":"Use component key to run this component locally"},{"location":"pipeline/reuse-aml-pipeline/#4-experiment-with-different-versions-of-registered-components","text":"The way the helper code decides which version to use for a given component M is (in order): if module_loader.force_all_module_version is set, use it as version for component M (and all others) if a version is set under component M in modules.manifest , use it if a version is hardcoded in required_modules() for component M, use it if module_loader.force_default_module_version is set, use it as version for component M (and all others non specified versions) else, use default version registered in Azure ML (usually, the latest). Version management for your experiment components can have multiple use cases.","title":"4. Experiment with different versions of (registered) components"},{"location":"pipeline/reuse-aml-pipeline/#use-a-specific-version-for-all-unspecified-force_default_module_version","text":"If all your components versions are synchronized in the registration build, you can use this to use a single version number accross all the graph for all components that have unspecified versions ( version:null ). If you want to pin down a specific version number outside of this, you can add a specific version in your module manifest, or in the required_modules() method.","title":"Use a specific version for all unspecified (force_default_module_version)"},{"location":"pipeline/reuse-aml-pipeline/#use-a-specific-version-for-all-components","text":"If all your components versions are synchronized in the registration build, you can use this to use a single version number accross all the graph for all components. This will give you an exact replication of the components at a particular point in time. This will override all other version settings.","title":"Use a specific version for all components"},{"location":"pipeline/reuse-aml-pipeline/#use-specific-versions-for-some-components","text":"If you want to pin down a specific version number for some particular components, specify this version in the module manifest: # @package _group_ manifest : # ... - key : \"LMPytorchTrainer\" name : \"[SC] [AML] PyTorch LM Trainer\" version : null # <<< HERE yaml : \"pytorch_trainer/module_spec.yaml\" # ... or hardcode it (not recommended) in your required_modules() method: @classmethod def required_modules ( cls ): return { \"LMPytorchTrainer\" :{ \"remote_module_name\" : \"[SC] [AML] PyTorch LM Trainer\" , \"namespace\" : \"microsoft.com/office/smartcompose\" , \"version\" : None , # <<< HERE \"yaml_spec\" : \"pytorch_trainer/module_spec.yaml\" } }","title":"Use specific versions for some components"},{"location":"pipeline/submission-time-override/","text":"Submission-time override Motivation A common scenario is when running experiments in different workspaces under different tenants, some fields need to changed accordingly, e.g. tags and docker base image. Internally at Microsoft, non-Heron workspaces do not have access to Polymer Python Package Index , and for Heron workspaces, base images are stored in different registries ( polymerprod.azurecr.io , polymerppe.azurecr.io , polymerdev.azurecr.io ) for different workspaces (e.g., PROD , PPE , public eyes-on ). Users have to modify those fields manually when submitting experiments to a different workspace, which is not an optimal data science experience, and could easily lead to typos and errors. Prerequisites To enjoy this documentation, you should be familiar with how to create an AML pipeline and how to configure your AML pipeline . Submission-time override To allow components to operate across multiple workspaces, we now support submission time override, by which the corresponding fields are modified as user defined and reverted back after submission. The intial RFC ( R equest F or C omment) can be found here . Controlling which components to override Only components using the \"local\" copy will be modified at submission time, so you need to define those components in module_loader.use_local or set it to \"*\" . Adding tenant_overrides section to your pipeline yaml With the additional code block tenant_overrides , when you submit an experiment, the shrike.pipeline package will check which tenant is being used, and performs the corresponding override. defaults : - aml : eyesoff - compute : eyesoff - modules : module_defaults run : experiment_name : test tags : accelerator_repo : test module_loader : local_steps_folder : ../../../components use_local : \"*\" ## Adding the following section for this new feature tenant_overrides : allow_override : true # optional, default = false keep_modified_files : false # optional, default = false mapping : # MSIT tenant 72f988bf-86f1-41af-91ab-2d7cd011db47 : remove_polymer_pkg_idx : true # optional, default = false environment.docker.image : polymerprod.azurecr.io/polymercd/prod_official/azureml_base_gpu_openmpi312cuda101cudnn7 : mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04 polymerprod.azurecr.io/polymercd/prod_official/fake_image : mcr.microsoft.com/azureml/fake_image tags : workspace : eyeson personal : # using file name to specify tenant instead of id environment.docker.image : 'polymerprod.azurecr.io/polymercd/prod_official/(.+)' : 'polymerdev.azurecr.io/polymercd/dev_official/\\g<1>' Under tenant_overrides , you could specify three fields: - allow_override : optional, set to False by default. This boolean controls whether the submission-time override functionality will be executed or not. - keep_modified_files : optional, set to False by default. If True , then the modified files ( spec.yaml , env.yaml , etc.) will be saved and renamed as <filename>_<tenant_id>.<extension> . - mapping : (nested) dictionary-style definition. If this tenant is being used with allow_override = True , then all local components will be scanned and the matching fields defined in this mapping section will be changed. - Keys: tenant_id (e.g.: 72f988bf-86f1-41af-91ab-2d7cd011db47 ) or \"aml configuration\" filename in <config-dir>/aml (which is also used as in defaults: aml in this yaml file). - Values: (nested) dictionaries, e.g. environment.docker.image . You could define the override for any field in component schema . - For string-type fields such as environment.docker.image , the override pattern is \"old_value: new_value\". For dict-type fields such as tags , the pattern is \"key: new_value\". In the example above, when running under MSIT tenant, the original spec.yaml with tags : workspace : eyesoff will be overriden as tags : workspace : eyeson - The added code should be interpreted as a JSONPath expression, and both \"naive\" string substitution and regex-based substitution are allowed, e.g.: 'polymerprod.azurecr.io/polymercd/prod_official/(.+)' : 'polymerdev.azurecr.io/polymercd/dev_official/\\g<1>' - remove_polymer_pkg_idx : in addition to fields defined in component schema, you could also define this boolean where the default value is False . If set to True , the index url \"https://o365exchange.pkgs.visualstudio.com/_packaging/PolymerPythonPackages/pypi/simple/\" will be removed from environment.conda.conda_dependencies or conda_dependencies_file . Summary In short, to allow submission-time override, Add component keys or \"*\" to module_loader.use_local Add tenant_overrides section to pipeline yaml file, with allow_override: true nested dictionary mapping with tenant_id or \"aml configuration\" filename as keys Submit your pipeline as usual. In your \"snapshot\" of the experiment on Azure ML portal, component_spec.yaml should be updated, while the original copies are renamed as <filename>.not_used (temporarily). The local filenames will be reverted. Example: Fig 1: original code structure on local machine Fig 2: Azure ML portal Fig 3: modified code after submission on local machine with keep_modified_files set to False (if False , it will be same as before submission)","title":"Submission-time override"},{"location":"pipeline/submission-time-override/#submission-time-override","text":"","title":"Submission-time override"},{"location":"pipeline/submission-time-override/#motivation","text":"A common scenario is when running experiments in different workspaces under different tenants, some fields need to changed accordingly, e.g. tags and docker base image. Internally at Microsoft, non-Heron workspaces do not have access to Polymer Python Package Index , and for Heron workspaces, base images are stored in different registries ( polymerprod.azurecr.io , polymerppe.azurecr.io , polymerdev.azurecr.io ) for different workspaces (e.g., PROD , PPE , public eyes-on ). Users have to modify those fields manually when submitting experiments to a different workspace, which is not an optimal data science experience, and could easily lead to typos and errors.","title":"Motivation"},{"location":"pipeline/submission-time-override/#prerequisites","text":"To enjoy this documentation, you should be familiar with how to create an AML pipeline and how to configure your AML pipeline .","title":"Prerequisites"},{"location":"pipeline/submission-time-override/#submission-time-override_1","text":"To allow components to operate across multiple workspaces, we now support submission time override, by which the corresponding fields are modified as user defined and reverted back after submission. The intial RFC ( R equest F or C omment) can be found here .","title":"Submission-time override"},{"location":"pipeline/submission-time-override/#controlling-which-components-to-override","text":"Only components using the \"local\" copy will be modified at submission time, so you need to define those components in module_loader.use_local or set it to \"*\" .","title":"Controlling which components to override"},{"location":"pipeline/submission-time-override/#adding-tenant_overrides-section-to-your-pipeline-yaml","text":"With the additional code block tenant_overrides , when you submit an experiment, the shrike.pipeline package will check which tenant is being used, and performs the corresponding override. defaults : - aml : eyesoff - compute : eyesoff - modules : module_defaults run : experiment_name : test tags : accelerator_repo : test module_loader : local_steps_folder : ../../../components use_local : \"*\" ## Adding the following section for this new feature tenant_overrides : allow_override : true # optional, default = false keep_modified_files : false # optional, default = false mapping : # MSIT tenant 72f988bf-86f1-41af-91ab-2d7cd011db47 : remove_polymer_pkg_idx : true # optional, default = false environment.docker.image : polymerprod.azurecr.io/polymercd/prod_official/azureml_base_gpu_openmpi312cuda101cudnn7 : mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04 polymerprod.azurecr.io/polymercd/prod_official/fake_image : mcr.microsoft.com/azureml/fake_image tags : workspace : eyeson personal : # using file name to specify tenant instead of id environment.docker.image : 'polymerprod.azurecr.io/polymercd/prod_official/(.+)' : 'polymerdev.azurecr.io/polymercd/dev_official/\\g<1>' Under tenant_overrides , you could specify three fields: - allow_override : optional, set to False by default. This boolean controls whether the submission-time override functionality will be executed or not. - keep_modified_files : optional, set to False by default. If True , then the modified files ( spec.yaml , env.yaml , etc.) will be saved and renamed as <filename>_<tenant_id>.<extension> . - mapping : (nested) dictionary-style definition. If this tenant is being used with allow_override = True , then all local components will be scanned and the matching fields defined in this mapping section will be changed. - Keys: tenant_id (e.g.: 72f988bf-86f1-41af-91ab-2d7cd011db47 ) or \"aml configuration\" filename in <config-dir>/aml (which is also used as in defaults: aml in this yaml file). - Values: (nested) dictionaries, e.g. environment.docker.image . You could define the override for any field in component schema . - For string-type fields such as environment.docker.image , the override pattern is \"old_value: new_value\". For dict-type fields such as tags , the pattern is \"key: new_value\". In the example above, when running under MSIT tenant, the original spec.yaml with tags : workspace : eyesoff will be overriden as tags : workspace : eyeson - The added code should be interpreted as a JSONPath expression, and both \"naive\" string substitution and regex-based substitution are allowed, e.g.: 'polymerprod.azurecr.io/polymercd/prod_official/(.+)' : 'polymerdev.azurecr.io/polymercd/dev_official/\\g<1>' - remove_polymer_pkg_idx : in addition to fields defined in component schema, you could also define this boolean where the default value is False . If set to True , the index url \"https://o365exchange.pkgs.visualstudio.com/_packaging/PolymerPythonPackages/pypi/simple/\" will be removed from environment.conda.conda_dependencies or conda_dependencies_file .","title":"Adding tenant_overrides section to your pipeline yaml"},{"location":"pipeline/submission-time-override/#summary","text":"In short, to allow submission-time override, Add component keys or \"*\" to module_loader.use_local Add tenant_overrides section to pipeline yaml file, with allow_override: true nested dictionary mapping with tenant_id or \"aml configuration\" filename as keys Submit your pipeline as usual. In your \"snapshot\" of the experiment on Azure ML portal, component_spec.yaml should be updated, while the original copies are renamed as <filename>.not_used (temporarily). The local filenames will be reverted. Example: Fig 1: original code structure on local machine Fig 2: Azure ML portal Fig 3: modified code after submission on local machine with keep_modified_files set to False (if False , it will be same as before submission)","title":"Summary"},{"location":"pipeline/testing-components/","text":"PyTest suite for testing if run.py is aligned with module specification: Status: this code relates to the recipe and is a proposition component_run_get_arg_parser ( component_spec_path ) Tests is module run.py has function get_arg_parser(parser) Source code in shrike/pipeline/testing/components.py def component_run_get_arg_parser ( component_spec_path ): \"\"\"Tests is module run.py has function get_arg_parser(parser)\"\"\" definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) run_py_command , definition_command = find_run_py_in_command ( definition , use_component_sdk ) component_import_path = os . path . dirname ( component_spec_path ) run_py_absdir = os . path . join ( component_import_path , run_py_command ) assert os . path . isfile ( run_py_absdir ), \"Component {} has command {} using a python script {} that cannot be found\" . format ( component_spec_path , definition_command , run_py_command ) if component_import_path not in sys . path : sys . path . insert ( 0 , component_import_path ) try : assert os . path . isfile ( run_py_absdir ), f \"module command { run_py_absdir } should exist\" get_arg_parser_func = import_and_test_class ( run_py_absdir , \"get_arg_parser\" ) except : assert ( False ), \"importing {} function get_arg_parser() resulted in an exception: {} \" . format ( run_py_absdir , traceback . format_exc () ) try : returned_parser = get_arg_parser_func () except : assert ( False ), \"Component script {} .get_arg_parser() should be able to run on argparse.ArgumentParser, but raised an exception: {} \" . format ( run_py_absdir , traceback . format_exc () ) assert ( returned_parser is not None ), \"component script {} .get_arg_parser() is supposed to return a parser when provided with None, please add 'return parser' at the end of the function.\" . format ( run_py_absdir ) try : parser = argparse . ArgumentParser () returned_parser = get_arg_parser_func ( parser ) except : assert ( False ), \"Component script {} .get_arg_parser() should be able to run on argparse.ArgumentParser, but raised an exception: {} \" . format ( run_py_absdir , traceback . format_exc () ) assert ( returned_parser is not None ), \"Component script {} .get_arg_parser() is not supposed to return None, please add 'return parser' at the end of the function.\" . format ( run_py_absdir ) # test object equality assert ( returned_parser is parser ), \"Component script {} .get_arg_parser() is supposed to return the parser it was provided, please do not create a new instance if provided with a parser.\" . format ( run_py_absdir ) return parser component_run_py_import ( component_spec_path ) Try importing run.py, just to check if basic script passes syntax/imports checks Source code in shrike/pipeline/testing/components.py def component_run_py_import ( component_spec_path ): \"\"\"Try importing run.py, just to check if basic script passes syntax/imports checks\"\"\" definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) run_py_command , definition_command = find_run_py_in_command ( definition , use_component_sdk ) component_import_path = os . path . dirname ( component_spec_path ) run_py_absdir = os . path . join ( component_import_path , run_py_command ) assert os . path . isfile ( run_py_absdir ), \"Component {} has command {} using a python script {} that cannot be found\" . format ( component_spec_path , definition_command , run_py_command ) if component_import_path not in sys . path : sys . path . insert ( 0 , component_import_path ) try : spec , mod = dynamic_import_module ( run_py_absdir ) except : assert False , \"importing {} resulted in an exception: {} \" . format ( run_py_absdir , traceback . format_exc () ) component_spec_yaml_exists_and_is_parsable ( component_spec_path ) Checks component spec file Source code in shrike/pipeline/testing/components.py def component_spec_yaml_exists_and_is_parsable ( component_spec_path ): \"\"\"Checks component spec file\"\"\" assert os . path . isfile ( component_spec_path ), f \"Component spec file under path { component_spec_path } could not be found\" # opens file for testing schema with open ( component_spec_path , \"r\" ) as ifile : component_spec_content = ifile . read () if \"$schema: http://azureml/\" in component_spec_content : use_component_sdk = True else : use_component_sdk = False # Block unit tests from working with module sdk if not enabled if not os . environ . get ( \"MODULE_SDK_ENABLE\" ): assert ( use_component_sdk ), \"These unit tests are intentionnally blocked from support Module SDK, which is DEPRECATED. To bypass, create env variable MODULE_SDK_ENABLE.\" if use_component_sdk : try : definition = ComponentDefinition . load ( component_spec_path ) except BaseException as e : assert ( False ), \"Failed: failed to load (sdk 2.0) component yaml %r , exception= %r \" % ( component_spec_path , e , ) else : try : with open ( component_spec_path , \"r\" ) as ifile : definition = yaml . safe_load ( ifile ) except BaseException as e : assert ( False ), \"Failed: failed to load (old style) module yaml %r , exception= %r \" % ( component_spec_path , e , ) return definition , use_component_sdk component_uses_private_acr ( component_spec_path , acr_url ) Tests base image in private ACR Source code in shrike/pipeline/testing/components.py def component_uses_private_acr ( component_spec_path , acr_url ): \"\"\"Tests base image in private ACR\"\"\" definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) if use_component_sdk : component_uses_private_acr_componentsdk ( component_spec_path , definition , acr_url ) else : component_uses_private_acr_modulesdk ( component_spec_path , definition , acr_url ) component_uses_private_acr_componentsdk ( component_spec_path , definition , acr_url ) Tests base image in private ACR Source code in shrike/pipeline/testing/components.py def component_uses_private_acr_componentsdk ( component_spec_path , definition , acr_url ): \"\"\"Tests base image in private ACR\"\"\" definition_type = definition . type if definition_type in [ ComponentType . HDInsightComponent , ComponentType . ScopeComponent , ComponentType . DataTransferComponent , ]: return try : base_image_url = definition . environment . docker . image except KeyError : base_image_url = None pass if base_image_url is not None : assert base_image_url . startswith ( acr_url ), \"Component {} baseImage should be drawn from polymerprod, instead found url {} \" . format ( component_spec_path , base_image_url ) component_uses_private_acr_modulesdk ( component_spec_path , definition , acr_url ) Tests base image in private ACR Source code in shrike/pipeline/testing/components.py def component_uses_private_acr_modulesdk ( component_spec_path , definition , acr_url ): \"\"\"Tests base image in private ACR\"\"\" try : base_image_url = definition [ \"implementation\" ][ \"container\" ][ \"amlEnvironment\" ][ \"docker\" ][ \"baseImage\" ] except KeyError : base_image_url = None pass if base_image_url is not None : assert base_image_url . startswith ( acr_url ), \"Component(1.5) {} baseImage should be drawn from polymerprod, instead found url {} \" . format ( component_spec_path , base_image_url ) component_uses_private_python_feed ( component_spec_path , feed_url ) Tests private python feed referenced in conda Source code in shrike/pipeline/testing/components.py def component_uses_private_python_feed ( component_spec_path , feed_url ): \"\"\"Tests private python feed referenced in conda\"\"\" definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) if use_component_sdk : if definition . type in [ ComponentType . HDInsightComponent , ComponentType . ScopeComponent , ComponentType . DataTransferComponent , ]: return try : conda_deps_yaml = definition . environment . conda . conda_dependencies . _to_dict () except KeyError : conda_deps_yaml = None pass else : job_type = str ( definition [ \"jobType\" ]) . lower () if job_type in [ \"hdinsight\" , \"scopecomponent\" , \"datatransfercomponent\" ]: # hdi/scope/datatransfer jobs don't have python feed return if job_type == \"parallel\" : try : conda_deps_path = definition [ \"implementation\" ][ \"parallel\" ][ \"amlEnvironment\" ][ \"python\" ][ \"condaDependenciesFile\" ] except KeyError : conda_deps_path = None pass else : try : conda_deps_path = definition [ \"implementation\" ][ \"container\" ][ \"amlEnvironment\" ][ \"python\" ][ \"condaDependenciesFile\" ] except KeyError : conda_deps_path = None pass if conda_deps_path is None : # no conda yaml provided, nothing to do here return conda_deps_abspath = os . path . join ( os . path . dirname ( component_spec_path ), conda_deps_path ) assert os . path . isfile ( conda_deps_abspath ), \"Component {} specified a conda_dependencies_file {} that cannot be found (abspath: {} )\" . format ( component_spec_path , conda_deps_path , conda_deps_abspath ) try : with open ( conda_deps_abspath , \"r\" ) as ifile : conda_deps_yaml = yaml . safe_load ( ifile ) except : assert ( False ), \"Component {} conda_dependencies_file under path {} should be yaml parsable, but loading it raised an exception: {} \" . format ( component_spec_path , conda_deps_abspath , traceback . format_exc () ) if conda_deps_yaml is None : # no conda yaml provided, nothing to do here return if \"channels\" in conda_deps_yaml : assert conda_deps_yaml [ \"channels\" ] == [ \".\" ], \"In conda deps, no channels must be specified, or use . as channel\" if \"dependencies\" in conda_deps_yaml : for entry in conda_deps_yaml [ \"dependencies\" ]: if \"pip\" in entry and isinstance ( entry , dict ): assert ( f \"--index-url { feed_url } \" in entry [ \"pip\" ] ), \"conda deps must reference private python feed under pip dependencies.\" find_run_py_in_command ( definition , use_component_sdk ) Finds runnable python script in command Source code in shrike/pipeline/testing/components.py def find_run_py_in_command ( definition , use_component_sdk ): \"\"\"Finds runnable python script in command\"\"\" run_py_command , definition_command = None , None if use_component_sdk : definition_type = definition . type if definition_type == ComponentType . HDInsightComponent : run_py_command = definition . file definition_command = definition . args elif definition_type == ComponentType . DistributedComponent : # run_py_command not provided, we need to find it definition_command = definition . launcher . additional_arguments elif definition_type == ComponentType . ParallelComponent : run_py_command = definition . entry definition_command = definition . args elif definition_type == ComponentType . CommandComponent : # run_py_command not provided, we need to find it definition_command = definition . command elif definition_type not in [ ComponentType . ScopeComponent , ComponentType . DataTransferComponent , ]: raise Exception ( f \"Component type { definition_type } is not supported in the helper code unit tests (yet).\" ) if ( run_py_command is None and definition . type != ComponentType . ScopeComponent and definition . type != ComponentType . DataTransferComponent ): # search for python script for entry in definition_command . split ( \" \" ): if entry . endswith ( \".py\" ): run_py_command = entry break else : assert ( False ), \"Could not find any script name like *.py in component command {} \" . format ( definition_command ) else : job_type = str ( definition [ \"jobType\" ]) . lower () if job_type == \"hdinsight\" : run_py_command = definition [ \"implementation\" ][ \"hdinsight\" ][ \"file\" ] definition_command = run_py_command elif job_type == \"parallel\" : run_py_command = definition [ \"implementation\" ][ \"parallel\" ][ \"entry\" ] definition_command = run_py_command elif job_type not in [ \"scopecomponent\" , \"datatransfercomponent\" ]: definition_command = definition [ \"implementation\" ][ \"container\" ][ \"command\" ] for entry in definition_command : if entry . endswith ( \".py\" ): run_py_command = entry break else : assert ( False ), \"Could not find any script name like *.py in component command {} \" . format ( definition_command . split ( \" \" ) ) return run_py_command , definition_command generate_component_arguments_componentsdk ( component_spec , arg , output_script_arguments ) Recursively generate fake arguments to test script argparse. Parameters: Name Type Description Default component_spec dict module specification in yaml required arg list or str or dict) argument specification required output_script_arguments list) output required Returns: Type Description list output_script_arguments Source code in shrike/pipeline/testing/components.py def generate_component_arguments_componentsdk ( component_spec , arg , output_script_arguments ): \"\"\"Recursively generate fake arguments to test script argparse. Args: component_spec (dict): module specification in yaml arg (list or str or dict) : argument specification output_script_arguments (list) : output Returns: list: output_script_arguments \"\"\" log . info ( f \"generate_component_arguments(spec, { arg } , ...)\" ) if isinstance ( arg , list ): # optional argument or root list for entry in arg : generate_component_arguments_componentsdk ( component_spec , entry , output_script_arguments ) elif isinstance ( arg , str ) and arg . startswith ( \"{\" ): io_key = arg . lstrip ( \"{\" ) . rstrip ( \"}\" ) if io_key . startswith ( \"inputs.\" ): input_key = io_key [ 7 :] log . info ( \"inputs keys: \" + \" \" . join ([ key for key in component_spec . inputs ])) log . info ( \"parameter keys: \" + \" \" . join ([ key for key in component_spec . parameters ]) ) if input_key in component_spec . inputs : output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( component_spec . inputs [ input_key ] ) ) ) elif input_key in component_spec . parameters : output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( component_spec . parameters [ input_key ] ) ) ) else : raise Exception ( f \"Input key { input_key } is neither an input or a parameter\" ) elif io_key . startswith ( \"outputs.\" ): output_key = io_key [ 8 :] log . info ( \"outputs keys: \" + \" \" . join ([ key for key in component_spec . outputs ]) ) output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( component_spec . outputs [ output_key ] ) ) ) else : raise NotImplementedError ( \"In argument spec {} , I/O key arg spec is not supported {} \" . format ( arg , io_key ) ) elif isinstance ( arg , str ): output_script_arguments . append ( arg ) elif isinstance ( arg , dict ): # for old module def if \"inputValue\" in arg : # find in inputs for i_spec in component_spec . inputs : if i_spec [ \"name\" ] == arg [ \"inputValue\" ]: output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( i_spec )) ) elif \"inputPath\" in arg : # find in inputs for i_spec in component_spec . inputs : if i_spec [ \"name\" ] == arg [ \"inputPath\" ]: output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( i_spec )) ) elif \"outputPath\" in arg : # find in outputs output_script_arguments . append ( \"/mnt/fakeoutputpath\" ) return output_script_arguments generate_component_arguments_modulesdk ( module_spec , arg , output_script_arguments ) Recursively generate fake arguments to test script argparse. Parameters: Name Type Description Default component_spec dict module specification in yaml required arg list or str or dict) argument specification required output_script_arguments list) output required Returns: Type Description list output_script_arguments Source code in shrike/pipeline/testing/components.py def generate_component_arguments_modulesdk ( module_spec , arg , output_script_arguments ): \"\"\"Recursively generate fake arguments to test script argparse. Args: component_spec (dict): module specification in yaml arg (list or str or dict) : argument specification output_script_arguments (list) : output Returns: list: output_script_arguments \"\"\" if isinstance ( arg , list ): # optional argument or root list for entry in arg : generate_component_arguments_modulesdk ( module_spec , entry , output_script_arguments ) elif isinstance ( arg , str ): output_script_arguments . append ( arg ) elif isinstance ( arg , dict ): if \"inputValue\" in arg : # find in inputs for i_spec in module_spec [ \"inputs\" ]: if i_spec [ \"name\" ] == arg [ \"inputValue\" ]: output_script_arguments . append ( str ( _generate_fake_input_arg_modulesdk ( i_spec )) ) elif \"inputPath\" in arg : # find in inputs for i_spec in module_spec [ \"inputs\" ]: if i_spec [ \"name\" ] == arg [ \"inputPath\" ]: output_script_arguments . append ( str ( _generate_fake_input_arg_modulesdk ( i_spec )) ) elif \"outputPath\" in arg : # find in outputs output_script_arguments . append ( \"/mnt/fakeoutputpath\" ) return output_script_arguments if_arguments_from_component_spec_match_script_argparse ( component_spec_path ) Tests alignment between spec arguments and script parser arguments Source code in shrike/pipeline/testing/components.py def if_arguments_from_component_spec_match_script_argparse ( component_spec_path ): \"\"\"Tests alignment between spec arguments and script parser arguments\"\"\" # assuming we have a yaml spec file that is loadable definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) # assuming we can import the get_arg_parser() function parser = component_run_get_arg_parser ( component_spec_path ) run_py_command , definition_command = find_run_py_in_command ( definition , use_component_sdk ) if use_component_sdk : arguments_spec = [ entry . lstrip ( \"[\" ) . rstrip ( \"]\" ) for entry in definition_command . split ( \" \" ) ] if arguments_spec [ 0 ] . startswith ( \"python\" ): arguments_spec . pop ( 0 ) if arguments_spec [ 0 ] . endswith ( \".py\" ): arguments_spec . pop ( 0 ) script_arguments = [] generate_component_arguments_componentsdk ( definition , arguments_spec , script_arguments ) else : job_type = str ( definition [ \"jobType\" ]) . lower () if job_type == \"hdinsight\" : arguments_spec = definition [ \"implementation\" ][ \"hdinsight\" ][ \"args\" ] elif job_type == \"parallel\" : arguments_spec = definition [ \"implementation\" ][ \"parallel\" ][ \"args\" ] elif job_type not in [ \"scopecomponent\" , \"datatransfercomponent\" ]: arguments_spec = definition [ \"implementation\" ][ \"container\" ][ \"args\" ] script_arguments = [] generate_component_arguments_modulesdk ( definition , arguments_spec , script_arguments ) try : _ , unknown_args = parser . parse_known_args ( script_arguments ) except : assert ( False ), \"Component {} , in run.py, parse_known_args() should be able to parse {} , instead raised an exception: {} \" . format ( component_spec_path , script_arguments , traceback . format_exc () ) assert ( len ( unknown_args ) == 0 ), \"Component {} , while calling run.py with args {} , parsing arguments from module spec should not return unknown args, instead we observed unknown args : {} \" . format ( component_spec_path , script_arguments , unknown_args ) script_main_with_synthetic_arguments ( module , mocker ) Try importing run.py, just to check if basic script passes syntax/imports checks Source code in shrike/pipeline/testing/components.py def script_main_with_synthetic_arguments ( module , mocker ): \"\"\"Try importing run.py, just to check if basic script passes syntax/imports checks\"\"\" paths = _get_module_paths ( module ) # assuming we have a yaml spec file that is loadable module_spec = module_spec_yaml_exists_and_is_parsable ( module ) # import module to get main() function if paths . module_spec_absdir not in sys . path : sys . path . insert ( 0 , paths . module_spec_absdir ) try : spec , mod = dynamic_import_module ( paths . module_import_path ) except : assert False , \"importing {} resulted in an exception: {} \" . format ( paths . module_import_path , traceback . format_exc () ) if module_spec [ \"jobType\" ] . lower () == \"hdinsight\" : arguments_spec = module_spec [ \"implementation\" ][ \"hdinsight\" ][ \"args\" ] elif ( module_spec [ \"jobType\" ] . lower () != \"scopecomponent\" and module_spec [ \"jobType\" ] . lower () != \"datatransfercomponent\" ): arguments_spec = module_spec [ \"implementation\" ][ \"container\" ][ \"args\" ] script_arguments = [] generate_argument ( module_spec , arguments_spec , script_arguments ) log . info ( script_arguments ) # https://medium.com/python-pandemonium/testing-sys-exit-with-pytest-10c6e5f7726f with pytest . raises ( SystemExit ) as pytest_wrapped_e : mod . main ( script_arguments + [ \"-h\" ]) assert pytest_wrapped_e . type == SystemExit assert pytest_wrapped_e . value . code == 0","title":"testing.componets"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.component_run_get_arg_parser","text":"Tests is module run.py has function get_arg_parser(parser) Source code in shrike/pipeline/testing/components.py def component_run_get_arg_parser ( component_spec_path ): \"\"\"Tests is module run.py has function get_arg_parser(parser)\"\"\" definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) run_py_command , definition_command = find_run_py_in_command ( definition , use_component_sdk ) component_import_path = os . path . dirname ( component_spec_path ) run_py_absdir = os . path . join ( component_import_path , run_py_command ) assert os . path . isfile ( run_py_absdir ), \"Component {} has command {} using a python script {} that cannot be found\" . format ( component_spec_path , definition_command , run_py_command ) if component_import_path not in sys . path : sys . path . insert ( 0 , component_import_path ) try : assert os . path . isfile ( run_py_absdir ), f \"module command { run_py_absdir } should exist\" get_arg_parser_func = import_and_test_class ( run_py_absdir , \"get_arg_parser\" ) except : assert ( False ), \"importing {} function get_arg_parser() resulted in an exception: {} \" . format ( run_py_absdir , traceback . format_exc () ) try : returned_parser = get_arg_parser_func () except : assert ( False ), \"Component script {} .get_arg_parser() should be able to run on argparse.ArgumentParser, but raised an exception: {} \" . format ( run_py_absdir , traceback . format_exc () ) assert ( returned_parser is not None ), \"component script {} .get_arg_parser() is supposed to return a parser when provided with None, please add 'return parser' at the end of the function.\" . format ( run_py_absdir ) try : parser = argparse . ArgumentParser () returned_parser = get_arg_parser_func ( parser ) except : assert ( False ), \"Component script {} .get_arg_parser() should be able to run on argparse.ArgumentParser, but raised an exception: {} \" . format ( run_py_absdir , traceback . format_exc () ) assert ( returned_parser is not None ), \"Component script {} .get_arg_parser() is not supposed to return None, please add 'return parser' at the end of the function.\" . format ( run_py_absdir ) # test object equality assert ( returned_parser is parser ), \"Component script {} .get_arg_parser() is supposed to return the parser it was provided, please do not create a new instance if provided with a parser.\" . format ( run_py_absdir ) return parser","title":"component_run_get_arg_parser()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.component_run_py_import","text":"Try importing run.py, just to check if basic script passes syntax/imports checks Source code in shrike/pipeline/testing/components.py def component_run_py_import ( component_spec_path ): \"\"\"Try importing run.py, just to check if basic script passes syntax/imports checks\"\"\" definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) run_py_command , definition_command = find_run_py_in_command ( definition , use_component_sdk ) component_import_path = os . path . dirname ( component_spec_path ) run_py_absdir = os . path . join ( component_import_path , run_py_command ) assert os . path . isfile ( run_py_absdir ), \"Component {} has command {} using a python script {} that cannot be found\" . format ( component_spec_path , definition_command , run_py_command ) if component_import_path not in sys . path : sys . path . insert ( 0 , component_import_path ) try : spec , mod = dynamic_import_module ( run_py_absdir ) except : assert False , \"importing {} resulted in an exception: {} \" . format ( run_py_absdir , traceback . format_exc () )","title":"component_run_py_import()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.component_spec_yaml_exists_and_is_parsable","text":"Checks component spec file Source code in shrike/pipeline/testing/components.py def component_spec_yaml_exists_and_is_parsable ( component_spec_path ): \"\"\"Checks component spec file\"\"\" assert os . path . isfile ( component_spec_path ), f \"Component spec file under path { component_spec_path } could not be found\" # opens file for testing schema with open ( component_spec_path , \"r\" ) as ifile : component_spec_content = ifile . read () if \"$schema: http://azureml/\" in component_spec_content : use_component_sdk = True else : use_component_sdk = False # Block unit tests from working with module sdk if not enabled if not os . environ . get ( \"MODULE_SDK_ENABLE\" ): assert ( use_component_sdk ), \"These unit tests are intentionnally blocked from support Module SDK, which is DEPRECATED. To bypass, create env variable MODULE_SDK_ENABLE.\" if use_component_sdk : try : definition = ComponentDefinition . load ( component_spec_path ) except BaseException as e : assert ( False ), \"Failed: failed to load (sdk 2.0) component yaml %r , exception= %r \" % ( component_spec_path , e , ) else : try : with open ( component_spec_path , \"r\" ) as ifile : definition = yaml . safe_load ( ifile ) except BaseException as e : assert ( False ), \"Failed: failed to load (old style) module yaml %r , exception= %r \" % ( component_spec_path , e , ) return definition , use_component_sdk","title":"component_spec_yaml_exists_and_is_parsable()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.component_uses_private_acr","text":"Tests base image in private ACR Source code in shrike/pipeline/testing/components.py def component_uses_private_acr ( component_spec_path , acr_url ): \"\"\"Tests base image in private ACR\"\"\" definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) if use_component_sdk : component_uses_private_acr_componentsdk ( component_spec_path , definition , acr_url ) else : component_uses_private_acr_modulesdk ( component_spec_path , definition , acr_url )","title":"component_uses_private_acr()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.component_uses_private_acr_componentsdk","text":"Tests base image in private ACR Source code in shrike/pipeline/testing/components.py def component_uses_private_acr_componentsdk ( component_spec_path , definition , acr_url ): \"\"\"Tests base image in private ACR\"\"\" definition_type = definition . type if definition_type in [ ComponentType . HDInsightComponent , ComponentType . ScopeComponent , ComponentType . DataTransferComponent , ]: return try : base_image_url = definition . environment . docker . image except KeyError : base_image_url = None pass if base_image_url is not None : assert base_image_url . startswith ( acr_url ), \"Component {} baseImage should be drawn from polymerprod, instead found url {} \" . format ( component_spec_path , base_image_url )","title":"component_uses_private_acr_componentsdk()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.component_uses_private_acr_modulesdk","text":"Tests base image in private ACR Source code in shrike/pipeline/testing/components.py def component_uses_private_acr_modulesdk ( component_spec_path , definition , acr_url ): \"\"\"Tests base image in private ACR\"\"\" try : base_image_url = definition [ \"implementation\" ][ \"container\" ][ \"amlEnvironment\" ][ \"docker\" ][ \"baseImage\" ] except KeyError : base_image_url = None pass if base_image_url is not None : assert base_image_url . startswith ( acr_url ), \"Component(1.5) {} baseImage should be drawn from polymerprod, instead found url {} \" . format ( component_spec_path , base_image_url )","title":"component_uses_private_acr_modulesdk()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.component_uses_private_python_feed","text":"Tests private python feed referenced in conda Source code in shrike/pipeline/testing/components.py def component_uses_private_python_feed ( component_spec_path , feed_url ): \"\"\"Tests private python feed referenced in conda\"\"\" definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) if use_component_sdk : if definition . type in [ ComponentType . HDInsightComponent , ComponentType . ScopeComponent , ComponentType . DataTransferComponent , ]: return try : conda_deps_yaml = definition . environment . conda . conda_dependencies . _to_dict () except KeyError : conda_deps_yaml = None pass else : job_type = str ( definition [ \"jobType\" ]) . lower () if job_type in [ \"hdinsight\" , \"scopecomponent\" , \"datatransfercomponent\" ]: # hdi/scope/datatransfer jobs don't have python feed return if job_type == \"parallel\" : try : conda_deps_path = definition [ \"implementation\" ][ \"parallel\" ][ \"amlEnvironment\" ][ \"python\" ][ \"condaDependenciesFile\" ] except KeyError : conda_deps_path = None pass else : try : conda_deps_path = definition [ \"implementation\" ][ \"container\" ][ \"amlEnvironment\" ][ \"python\" ][ \"condaDependenciesFile\" ] except KeyError : conda_deps_path = None pass if conda_deps_path is None : # no conda yaml provided, nothing to do here return conda_deps_abspath = os . path . join ( os . path . dirname ( component_spec_path ), conda_deps_path ) assert os . path . isfile ( conda_deps_abspath ), \"Component {} specified a conda_dependencies_file {} that cannot be found (abspath: {} )\" . format ( component_spec_path , conda_deps_path , conda_deps_abspath ) try : with open ( conda_deps_abspath , \"r\" ) as ifile : conda_deps_yaml = yaml . safe_load ( ifile ) except : assert ( False ), \"Component {} conda_dependencies_file under path {} should be yaml parsable, but loading it raised an exception: {} \" . format ( component_spec_path , conda_deps_abspath , traceback . format_exc () ) if conda_deps_yaml is None : # no conda yaml provided, nothing to do here return if \"channels\" in conda_deps_yaml : assert conda_deps_yaml [ \"channels\" ] == [ \".\" ], \"In conda deps, no channels must be specified, or use . as channel\" if \"dependencies\" in conda_deps_yaml : for entry in conda_deps_yaml [ \"dependencies\" ]: if \"pip\" in entry and isinstance ( entry , dict ): assert ( f \"--index-url { feed_url } \" in entry [ \"pip\" ] ), \"conda deps must reference private python feed under pip dependencies.\"","title":"component_uses_private_python_feed()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.find_run_py_in_command","text":"Finds runnable python script in command Source code in shrike/pipeline/testing/components.py def find_run_py_in_command ( definition , use_component_sdk ): \"\"\"Finds runnable python script in command\"\"\" run_py_command , definition_command = None , None if use_component_sdk : definition_type = definition . type if definition_type == ComponentType . HDInsightComponent : run_py_command = definition . file definition_command = definition . args elif definition_type == ComponentType . DistributedComponent : # run_py_command not provided, we need to find it definition_command = definition . launcher . additional_arguments elif definition_type == ComponentType . ParallelComponent : run_py_command = definition . entry definition_command = definition . args elif definition_type == ComponentType . CommandComponent : # run_py_command not provided, we need to find it definition_command = definition . command elif definition_type not in [ ComponentType . ScopeComponent , ComponentType . DataTransferComponent , ]: raise Exception ( f \"Component type { definition_type } is not supported in the helper code unit tests (yet).\" ) if ( run_py_command is None and definition . type != ComponentType . ScopeComponent and definition . type != ComponentType . DataTransferComponent ): # search for python script for entry in definition_command . split ( \" \" ): if entry . endswith ( \".py\" ): run_py_command = entry break else : assert ( False ), \"Could not find any script name like *.py in component command {} \" . format ( definition_command ) else : job_type = str ( definition [ \"jobType\" ]) . lower () if job_type == \"hdinsight\" : run_py_command = definition [ \"implementation\" ][ \"hdinsight\" ][ \"file\" ] definition_command = run_py_command elif job_type == \"parallel\" : run_py_command = definition [ \"implementation\" ][ \"parallel\" ][ \"entry\" ] definition_command = run_py_command elif job_type not in [ \"scopecomponent\" , \"datatransfercomponent\" ]: definition_command = definition [ \"implementation\" ][ \"container\" ][ \"command\" ] for entry in definition_command : if entry . endswith ( \".py\" ): run_py_command = entry break else : assert ( False ), \"Could not find any script name like *.py in component command {} \" . format ( definition_command . split ( \" \" ) ) return run_py_command , definition_command","title":"find_run_py_in_command()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.generate_component_arguments_componentsdk","text":"Recursively generate fake arguments to test script argparse. Parameters: Name Type Description Default component_spec dict module specification in yaml required arg list or str or dict) argument specification required output_script_arguments list) output required Returns: Type Description list output_script_arguments Source code in shrike/pipeline/testing/components.py def generate_component_arguments_componentsdk ( component_spec , arg , output_script_arguments ): \"\"\"Recursively generate fake arguments to test script argparse. Args: component_spec (dict): module specification in yaml arg (list or str or dict) : argument specification output_script_arguments (list) : output Returns: list: output_script_arguments \"\"\" log . info ( f \"generate_component_arguments(spec, { arg } , ...)\" ) if isinstance ( arg , list ): # optional argument or root list for entry in arg : generate_component_arguments_componentsdk ( component_spec , entry , output_script_arguments ) elif isinstance ( arg , str ) and arg . startswith ( \"{\" ): io_key = arg . lstrip ( \"{\" ) . rstrip ( \"}\" ) if io_key . startswith ( \"inputs.\" ): input_key = io_key [ 7 :] log . info ( \"inputs keys: \" + \" \" . join ([ key for key in component_spec . inputs ])) log . info ( \"parameter keys: \" + \" \" . join ([ key for key in component_spec . parameters ]) ) if input_key in component_spec . inputs : output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( component_spec . inputs [ input_key ] ) ) ) elif input_key in component_spec . parameters : output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( component_spec . parameters [ input_key ] ) ) ) else : raise Exception ( f \"Input key { input_key } is neither an input or a parameter\" ) elif io_key . startswith ( \"outputs.\" ): output_key = io_key [ 8 :] log . info ( \"outputs keys: \" + \" \" . join ([ key for key in component_spec . outputs ]) ) output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( component_spec . outputs [ output_key ] ) ) ) else : raise NotImplementedError ( \"In argument spec {} , I/O key arg spec is not supported {} \" . format ( arg , io_key ) ) elif isinstance ( arg , str ): output_script_arguments . append ( arg ) elif isinstance ( arg , dict ): # for old module def if \"inputValue\" in arg : # find in inputs for i_spec in component_spec . inputs : if i_spec [ \"name\" ] == arg [ \"inputValue\" ]: output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( i_spec )) ) elif \"inputPath\" in arg : # find in inputs for i_spec in component_spec . inputs : if i_spec [ \"name\" ] == arg [ \"inputPath\" ]: output_script_arguments . append ( str ( _generate_fake_input_arg_componentsdk ( i_spec )) ) elif \"outputPath\" in arg : # find in outputs output_script_arguments . append ( \"/mnt/fakeoutputpath\" ) return output_script_arguments","title":"generate_component_arguments_componentsdk()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.generate_component_arguments_modulesdk","text":"Recursively generate fake arguments to test script argparse. Parameters: Name Type Description Default component_spec dict module specification in yaml required arg list or str or dict) argument specification required output_script_arguments list) output required Returns: Type Description list output_script_arguments Source code in shrike/pipeline/testing/components.py def generate_component_arguments_modulesdk ( module_spec , arg , output_script_arguments ): \"\"\"Recursively generate fake arguments to test script argparse. Args: component_spec (dict): module specification in yaml arg (list or str or dict) : argument specification output_script_arguments (list) : output Returns: list: output_script_arguments \"\"\" if isinstance ( arg , list ): # optional argument or root list for entry in arg : generate_component_arguments_modulesdk ( module_spec , entry , output_script_arguments ) elif isinstance ( arg , str ): output_script_arguments . append ( arg ) elif isinstance ( arg , dict ): if \"inputValue\" in arg : # find in inputs for i_spec in module_spec [ \"inputs\" ]: if i_spec [ \"name\" ] == arg [ \"inputValue\" ]: output_script_arguments . append ( str ( _generate_fake_input_arg_modulesdk ( i_spec )) ) elif \"inputPath\" in arg : # find in inputs for i_spec in module_spec [ \"inputs\" ]: if i_spec [ \"name\" ] == arg [ \"inputPath\" ]: output_script_arguments . append ( str ( _generate_fake_input_arg_modulesdk ( i_spec )) ) elif \"outputPath\" in arg : # find in outputs output_script_arguments . append ( \"/mnt/fakeoutputpath\" ) return output_script_arguments","title":"generate_component_arguments_modulesdk()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.if_arguments_from_component_spec_match_script_argparse","text":"Tests alignment between spec arguments and script parser arguments Source code in shrike/pipeline/testing/components.py def if_arguments_from_component_spec_match_script_argparse ( component_spec_path ): \"\"\"Tests alignment between spec arguments and script parser arguments\"\"\" # assuming we have a yaml spec file that is loadable definition , use_component_sdk = component_spec_yaml_exists_and_is_parsable ( component_spec_path ) # assuming we can import the get_arg_parser() function parser = component_run_get_arg_parser ( component_spec_path ) run_py_command , definition_command = find_run_py_in_command ( definition , use_component_sdk ) if use_component_sdk : arguments_spec = [ entry . lstrip ( \"[\" ) . rstrip ( \"]\" ) for entry in definition_command . split ( \" \" ) ] if arguments_spec [ 0 ] . startswith ( \"python\" ): arguments_spec . pop ( 0 ) if arguments_spec [ 0 ] . endswith ( \".py\" ): arguments_spec . pop ( 0 ) script_arguments = [] generate_component_arguments_componentsdk ( definition , arguments_spec , script_arguments ) else : job_type = str ( definition [ \"jobType\" ]) . lower () if job_type == \"hdinsight\" : arguments_spec = definition [ \"implementation\" ][ \"hdinsight\" ][ \"args\" ] elif job_type == \"parallel\" : arguments_spec = definition [ \"implementation\" ][ \"parallel\" ][ \"args\" ] elif job_type not in [ \"scopecomponent\" , \"datatransfercomponent\" ]: arguments_spec = definition [ \"implementation\" ][ \"container\" ][ \"args\" ] script_arguments = [] generate_component_arguments_modulesdk ( definition , arguments_spec , script_arguments ) try : _ , unknown_args = parser . parse_known_args ( script_arguments ) except : assert ( False ), \"Component {} , in run.py, parse_known_args() should be able to parse {} , instead raised an exception: {} \" . format ( component_spec_path , script_arguments , traceback . format_exc () ) assert ( len ( unknown_args ) == 0 ), \"Component {} , while calling run.py with args {} , parsing arguments from module spec should not return unknown args, instead we observed unknown args : {} \" . format ( component_spec_path , script_arguments , unknown_args )","title":"if_arguments_from_component_spec_match_script_argparse()"},{"location":"pipeline/testing-components/#shrike.pipeline.testing.components.script_main_with_synthetic_arguments","text":"Try importing run.py, just to check if basic script passes syntax/imports checks Source code in shrike/pipeline/testing/components.py def script_main_with_synthetic_arguments ( module , mocker ): \"\"\"Try importing run.py, just to check if basic script passes syntax/imports checks\"\"\" paths = _get_module_paths ( module ) # assuming we have a yaml spec file that is loadable module_spec = module_spec_yaml_exists_and_is_parsable ( module ) # import module to get main() function if paths . module_spec_absdir not in sys . path : sys . path . insert ( 0 , paths . module_spec_absdir ) try : spec , mod = dynamic_import_module ( paths . module_import_path ) except : assert False , \"importing {} resulted in an exception: {} \" . format ( paths . module_import_path , traceback . format_exc () ) if module_spec [ \"jobType\" ] . lower () == \"hdinsight\" : arguments_spec = module_spec [ \"implementation\" ][ \"hdinsight\" ][ \"args\" ] elif ( module_spec [ \"jobType\" ] . lower () != \"scopecomponent\" and module_spec [ \"jobType\" ] . lower () != \"datatransfercomponent\" ): arguments_spec = module_spec [ \"implementation\" ][ \"container\" ][ \"args\" ] script_arguments = [] generate_argument ( module_spec , arguments_spec , script_arguments ) log . info ( script_arguments ) # https://medium.com/python-pandemonium/testing-sys-exit-with-pytest-10c6e5f7726f with pytest . raises ( SystemExit ) as pytest_wrapped_e : mod . main ( script_arguments + [ \"-h\" ]) assert pytest_wrapped_e . type == SystemExit assert pytest_wrapped_e . value . code == 0","title":"script_main_with_synthetic_arguments()"},{"location":"pipeline/testing-importer/","text":"Importer script Can import a function from a given script from file path dynamic_import_class ( module_path , class_name ) Dynamically imports some class/function from a python file Parameters: Name Type Description Default module_path str) path to python file (ex: ./bla/foo.py) required class_name str) name of class/function to import from it required Returns: Type Description class_attr (class) object imported from module Source code in shrike/pipeline/testing/importer.py def dynamic_import_class ( module_path , class_name ): \"\"\"Dynamically imports some class/function from a python file Args: module_path (str) : path to python file (ex: ./bla/foo.py) class_name (str) : name of class/function to import from it Returns: class_attr (class) : object imported from module \"\"\" spec = importlib . util . spec_from_file_location ( \"dynimportmodulename\" , module_path ) mod = importlib . util . module_from_spec ( spec ) spec . loader . exec_module ( mod ) class_attr = getattr ( mod , class_name ) return class_attr dynamic_import_module ( module_path ) Dynamically imports some class/function from a python file Parameters: Name Type Description Default module_path str) path to python file (ex: ./bla/foo.py) required Returns: Type Description mod, spec Source code in shrike/pipeline/testing/importer.py def dynamic_import_module ( module_path ): \"\"\"Dynamically imports some class/function from a python file Args: module_path (str) : path to python file (ex: ./bla/foo.py) Returns: mod, spec \"\"\" spec = importlib . util . spec_from_file_location ( \"dynimportmodulename\" , module_path ) mod = importlib . util . module_from_spec ( spec ) spec . loader . exec_module ( mod ) return spec , mod import_and_test_class ( module_path , class_name ) Tests importing some class/function from a python file Parameters: Name Type Description Default module_path str) path to python file (ex: ./bla/foo.py) required class_name str) name of class/function to import from it required Returns: Type Description class_attr (class) object imported from module Source code in shrike/pipeline/testing/importer.py def import_and_test_class ( module_path , class_name ): \"\"\"Tests importing some class/function from a python file Args: module_path (str) : path to python file (ex: ./bla/foo.py) class_name (str) : name of class/function to import from it Returns: class_attr (class) : object imported from module \"\"\" # WORK IN PROGRESS # test if module_path can be found in path # test if class_name exists in module_path # test return attr if class import_success = True message = None try : imported_class = dynamic_import_class ( module_path , class_name ) except : import_success = False message = traceback . format_exc () assert import_success , \"\"\" Importing class ' {} ' from module path ' {} ' did not succeed. Current python path is {} . Traceback from exception: {} \"\"\" . format ( class_name , module_path , sys . path , message ) return imported_class","title":"testing.importer"},{"location":"pipeline/testing-importer/#shrike.pipeline.testing.importer.dynamic_import_class","text":"Dynamically imports some class/function from a python file Parameters: Name Type Description Default module_path str) path to python file (ex: ./bla/foo.py) required class_name str) name of class/function to import from it required Returns: Type Description class_attr (class) object imported from module Source code in shrike/pipeline/testing/importer.py def dynamic_import_class ( module_path , class_name ): \"\"\"Dynamically imports some class/function from a python file Args: module_path (str) : path to python file (ex: ./bla/foo.py) class_name (str) : name of class/function to import from it Returns: class_attr (class) : object imported from module \"\"\" spec = importlib . util . spec_from_file_location ( \"dynimportmodulename\" , module_path ) mod = importlib . util . module_from_spec ( spec ) spec . loader . exec_module ( mod ) class_attr = getattr ( mod , class_name ) return class_attr","title":"dynamic_import_class()"},{"location":"pipeline/testing-importer/#shrike.pipeline.testing.importer.dynamic_import_module","text":"Dynamically imports some class/function from a python file Parameters: Name Type Description Default module_path str) path to python file (ex: ./bla/foo.py) required Returns: Type Description mod, spec Source code in shrike/pipeline/testing/importer.py def dynamic_import_module ( module_path ): \"\"\"Dynamically imports some class/function from a python file Args: module_path (str) : path to python file (ex: ./bla/foo.py) Returns: mod, spec \"\"\" spec = importlib . util . spec_from_file_location ( \"dynimportmodulename\" , module_path ) mod = importlib . util . module_from_spec ( spec ) spec . loader . exec_module ( mod ) return spec , mod","title":"dynamic_import_module()"},{"location":"pipeline/testing-importer/#shrike.pipeline.testing.importer.import_and_test_class","text":"Tests importing some class/function from a python file Parameters: Name Type Description Default module_path str) path to python file (ex: ./bla/foo.py) required class_name str) name of class/function to import from it required Returns: Type Description class_attr (class) object imported from module Source code in shrike/pipeline/testing/importer.py def import_and_test_class ( module_path , class_name ): \"\"\"Tests importing some class/function from a python file Args: module_path (str) : path to python file (ex: ./bla/foo.py) class_name (str) : name of class/function to import from it Returns: class_attr (class) : object imported from module \"\"\" # WORK IN PROGRESS # test if module_path can be found in path # test if class_name exists in module_path # test return attr if class import_success = True message = None try : imported_class = dynamic_import_class ( module_path , class_name ) except : import_success = False message = traceback . format_exc () assert import_success , \"\"\" Importing class ' {} ' from module path ' {} ' did not succeed. Current python path is {} . Traceback from exception: {} \"\"\" . format ( class_name , module_path , sys . path , message ) return imported_class","title":"import_and_test_class()"},{"location":"pipeline/testing-module-run-tests/","text":"PyTest suite for testing if run.py is aligned with module specification: if_arguments_from_module_spec_match_script_argparse ( module ) Tests alignment between module_spec arguments and script parser arguments Source code in shrike/pipeline/testing/module_run_tests.py def if_arguments_from_module_spec_match_script_argparse ( module ): \"\"\"Tests alignment between module_spec arguments and script parser arguments\"\"\" if_arguments_from_component_spec_match_script_argparse ( os . path . join ( module , \"module_spec.yaml\" ) ) module_run_get_arg_parser ( module ) Tests is module run.py has function get_arg_parser(parser) Source code in shrike/pipeline/testing/module_run_tests.py def module_run_get_arg_parser ( module ): \"\"\"Tests is module run.py has function get_arg_parser(parser)\"\"\" component_run_get_arg_parser ( os . path . join ( module , \"module_spec.yaml\" )) module_spec_yaml_exists_and_is_parsable ( module ) Tests the presence of module specifications in yaml (and return it) Source code in shrike/pipeline/testing/module_run_tests.py def module_spec_yaml_exists_and_is_parsable ( module ): \"\"\"Tests the presence of module specifications in yaml (and return it)\"\"\" return component_spec_yaml_exists_and_is_parsable ( os . path . join ( module , \"module_spec.yaml\" ) ) module_uses_private_acr ( module , acr_url ) Tests base image in private ACR Source code in shrike/pipeline/testing/module_run_tests.py def module_uses_private_acr ( module , acr_url ): \"\"\"Tests base image in private ACR\"\"\" component_uses_private_acr ( os . path . join ( module , \"module_spec.yaml\" ), acr_url ) module_uses_private_python_feed ( module , feed_url ) Tests private python feed referenced in conda Source code in shrike/pipeline/testing/module_run_tests.py def module_uses_private_python_feed ( module , feed_url ): \"\"\"Tests private python feed referenced in conda\"\"\" component_uses_private_python_feed ( os . path . join ( module , \"module_spec.yaml\" ), feed_url ) run_py_import ( module ) Try importing run.py, just to check if basic script passes syntax/imports checks Source code in shrike/pipeline/testing/module_run_tests.py def run_py_import ( module ): \"\"\"Try importing run.py, just to check if basic script passes syntax/imports checks\"\"\" component_run_py_import ( os . path . join ( module , \"module_spec.yaml\" ))","title":"testing.module_run_tests"},{"location":"pipeline/testing-module-run-tests/#shrike.pipeline.testing.module_run_tests.if_arguments_from_module_spec_match_script_argparse","text":"Tests alignment between module_spec arguments and script parser arguments Source code in shrike/pipeline/testing/module_run_tests.py def if_arguments_from_module_spec_match_script_argparse ( module ): \"\"\"Tests alignment between module_spec arguments and script parser arguments\"\"\" if_arguments_from_component_spec_match_script_argparse ( os . path . join ( module , \"module_spec.yaml\" ) )","title":"if_arguments_from_module_spec_match_script_argparse()"},{"location":"pipeline/testing-module-run-tests/#shrike.pipeline.testing.module_run_tests.module_run_get_arg_parser","text":"Tests is module run.py has function get_arg_parser(parser) Source code in shrike/pipeline/testing/module_run_tests.py def module_run_get_arg_parser ( module ): \"\"\"Tests is module run.py has function get_arg_parser(parser)\"\"\" component_run_get_arg_parser ( os . path . join ( module , \"module_spec.yaml\" ))","title":"module_run_get_arg_parser()"},{"location":"pipeline/testing-module-run-tests/#shrike.pipeline.testing.module_run_tests.module_spec_yaml_exists_and_is_parsable","text":"Tests the presence of module specifications in yaml (and return it) Source code in shrike/pipeline/testing/module_run_tests.py def module_spec_yaml_exists_and_is_parsable ( module ): \"\"\"Tests the presence of module specifications in yaml (and return it)\"\"\" return component_spec_yaml_exists_and_is_parsable ( os . path . join ( module , \"module_spec.yaml\" ) )","title":"module_spec_yaml_exists_and_is_parsable()"},{"location":"pipeline/testing-module-run-tests/#shrike.pipeline.testing.module_run_tests.module_uses_private_acr","text":"Tests base image in private ACR Source code in shrike/pipeline/testing/module_run_tests.py def module_uses_private_acr ( module , acr_url ): \"\"\"Tests base image in private ACR\"\"\" component_uses_private_acr ( os . path . join ( module , \"module_spec.yaml\" ), acr_url )","title":"module_uses_private_acr()"},{"location":"pipeline/testing-module-run-tests/#shrike.pipeline.testing.module_run_tests.module_uses_private_python_feed","text":"Tests private python feed referenced in conda Source code in shrike/pipeline/testing/module_run_tests.py def module_uses_private_python_feed ( module , feed_url ): \"\"\"Tests private python feed referenced in conda\"\"\" component_uses_private_python_feed ( os . path . join ( module , \"module_spec.yaml\" ), feed_url )","title":"module_uses_private_python_feed()"},{"location":"pipeline/testing-module-run-tests/#shrike.pipeline.testing.module_run_tests.run_py_import","text":"Try importing run.py, just to check if basic script passes syntax/imports checks Source code in shrike/pipeline/testing/module_run_tests.py def run_py_import ( module ): \"\"\"Try importing run.py, just to check if basic script passes syntax/imports checks\"\"\" component_run_py_import ( os . path . join ( module , \"module_spec.yaml\" ))","title":"run_py_import()"},{"location":"pipeline/testing-pipeline-class-test/","text":"PyTest suite for testing all module specification: deeptest_graph ( pipeline , definition , path = 'ROOT' ) Recursively compare a pipeline object to a serialized definition [EXPERIMENTAL] Parameters: Name Type Description Default pipeline json source for the comparison required definition json target/reference for the comparison required path str current path of the comparison (in the json tree) 'ROOT' Returns: Type Description None Source code in shrike/pipeline/testing/pipeline_class_test.py def deeptest_graph ( pipeline , definition , path = \"ROOT\" ): \"\"\"Recursively compare a pipeline object to a serialized definition [EXPERIMENTAL] Args: pipeline (json): source for the comparison definition (json): target/reference for the comparison path (str): current path of the comparison (in the json tree) Returns: None \"\"\" if definition is None : # no definition provided, let's stop inspection at this path log . info ( f \"deeptest_graph @ { path } : nop, definition is None\" ) return # is inspecting a dictionary structure, iterate on keys if isinstance ( pipeline , dict ) and isinstance ( definition , dict ): log . info ( f \"deeptest_graph @ { path } : checking dictionary\" ) for key in definition : assert ( key in pipeline ), f \"pipeline graph does not have key { key } at level @ { path } \" # ignoring all ids if key in { \"id\" , \"node_id\" , \"module_id\" , \"dataset_id\" }: log . info ( f \"deeptest_graph @ { path } : ignore id key { key } \" ) return if ( key in { \"run_settings\" , \"compute_run_settings\" } and definition [ key ] is not None ): # this is a specific kind of key containing a list we're transforming into a dict log . info ( f \"deeptest_graph @ { path } : refactoring key { key } as dict\" ) pipeline_run_settings = dict ( [( entry [ \"name\" ], entry ) for entry in pipeline [ key ]] ) definition_run_settings = dict ( [( entry [ \"name\" ], entry ) for entry in definition [ key ]] ) deeptest_graph ( pipeline_run_settings , definition_run_settings , path + \".(runsettings)\" + key , ) else : deeptest_graph ( pipeline [ key ], definition [ key ], path + \".\" + key ) return # is inspecting a list structure, each element MUST passed # NOTE: this should be improved in case list can be shuffled ? if isinstance ( pipeline , list ) and isinstance ( definition , list ): log . info ( f \"deeptest_graph @ { path } : checking list\" ) for key , entry in enumerate ( definition ): deeptest_graph ( pipeline [ key ], entry , path + \"[\" + str ( key ) + \"]\" ) return # if anything else (int, str, unknown), just test plain equality log . info ( f \"deeptest_graph @ { path } : checking equality { pipeline } == { definition } \" ) assert pipeline == definition , f \"values mismatch @ { path } \" deeptest_graph_comparison ( pipeline_export_file , pipeline_definition_file ) Compare a pipeline object to a serialized definition [EXPERIMENTAL] Parameters: Name Type Description Default pipeline_export_file str path to pipeline exported file required pipeline_definition_file str path to reference file required Returns: Type Description None Source code in shrike/pipeline/testing/pipeline_class_test.py def deeptest_graph_comparison ( pipeline_export_file , pipeline_definition_file ): \"\"\"Compare a pipeline object to a serialized definition [EXPERIMENTAL] Args: pipeline_export_file (str): path to pipeline exported file pipeline_definition_file (str): path to reference file Returns: None \"\"\" # checks the exported file in temp dir assert os . path . isfile ( pipeline_export_file ), f \"deeptest_graph_comparison() expects a file as first argument but { pipeline_export_file } does not exist.\" assert os . path . isfile ( pipeline_definition_file ), f \"deeptest_graph_comparison() expects a file as second argument but { pipeline_definition_file } does not exist.\" # read the exported graph with open ( pipeline_export_file , \"r\" ) as export_file : pipeline = json . loads ( export_file . read ()) assert ( pipeline is not None ), f \"deeptest_graph_comparison() expects first argument to be a parsable json, instead it found None\" with open ( pipeline_definition_file , \"r\" ) as definition_file : definition = json . loads ( definition_file . read ()) assert ( definition is not None ), f \"deeptest_graph_comparison() expects first argument to be a parsable json, instead it found None\" deeptest_graph ( pipeline , definition ) get_config_class ( pipeline_class ) Test if the get_arg_parser() method is in there and behaves correctly Source code in shrike/pipeline/testing/pipeline_class_test.py def get_config_class ( pipeline_class ): \"\"\"Test if the get_arg_parser() method is in there and behaves correctly\"\"\" try : config_class = pipeline_class . get_config_class () except : assert ( False ), \"getting config class for pipeline class {} resulted in an exception: {} \" . format ( pipeline_class . __name__ , traceback . format_exc () ) pipeline_required_modules ( pipeline_class ) Test if the required_modules() returns the right list of modules with all required keys Source code in shrike/pipeline/testing/pipeline_class_test.py def pipeline_required_modules ( pipeline_class ): \"\"\"Test if the required_modules() returns the right list of modules with all required keys\"\"\" modules_manifest = pipeline_class . required_modules () assert isinstance ( modules_manifest , dict ), \"required_modules() must return a dictionary.\" error_log = [] for module_key , module_description in modules_manifest . items (): if not isinstance ( module_description , dict ): error_log . append ( f \"values in dictionary returned by required_modules() must be dictionaries (under key= { module_key } , found value of type= { module_description . __name__ } )\" ) continue if \"yaml_spec\" not in module_description : error_log . append ( f \"In pipeline class module { pipeline_class . __name__ } ,\" + f \" module under key= { module_key } (in required_modules() function)\" + \" does not provide any yaml_spec key.\" + \" You need to give such a yaml_spec path before creating your pull request\" + \" so that we're able to consume this module when running pre-merge tests (detonation chamber)\" ) if \"remote_module_name\" not in module_description : error_log . append ( f \"In pipeline class module { pipeline_class . __name__ } ,\" + f \" module under key= { module_key } (in required_modules() function)\" + \" does not provide any remote_module_name.\" + \" You need to give such a name before creating your pull request\" + \" so that we're able to consume this module when running in production.\" ) # if \"namespace\" not in module_description: # error_log.append( # f\"In pipeline class module {pipeline_class.__name__},\" # + f\" module under key={module_key} (in required_modules() function)\" # + \" does not provide any namespace.\" # ) # TODO : verify if the version exists or is in the yaml spec? assert not ( error_log ), ( f \"In pipeline class module { pipeline_class . __name__ } , validation of the dictionary returned by required_modules() method shows errors: \\n \" + \" \\n \" . join ( error_log ) ) pipeline_required_subgraphs ( pipeline_class ) Tests if the required_subgraphs() returns the right list of modules with all requires keys Source code in shrike/pipeline/testing/pipeline_class_test.py def pipeline_required_subgraphs ( pipeline_class ): \"\"\"Tests if the required_subgraphs() returns the right list of modules with all requires keys\"\"\" subgraphs_manifest = pipeline_class . required_subgraphs () assert isinstance ( subgraphs_manifest , dict ), \"required_subgraphs() must return a dictionary.\" error_log = [] for subgraph_key , subgraph_class in subgraphs_manifest . items (): if not issubclass ( subgraph_class , AMLPipelineHelper ): error_log . append ( f \"In pipeline class module { pipeline_class . __name__ } , values in dictionary returned by required_subgraphs() must be subclass of AMLPipelineHelper (under key= { subgraph_key } , found object { subgraph_class . __name__ } )\" ) continue assert not ( error_log ), ( f \"In pipeline class module { pipeline_class . __name__ } , validation of the dictionary returned by required_subgraphs() shows errors: \" + \" \\n \" . join ( error_log ) )","title":"testing.pipeline_class_test"},{"location":"pipeline/testing-pipeline-class-test/#shrike.pipeline.testing.pipeline_class_test.deeptest_graph","text":"Recursively compare a pipeline object to a serialized definition [EXPERIMENTAL] Parameters: Name Type Description Default pipeline json source for the comparison required definition json target/reference for the comparison required path str current path of the comparison (in the json tree) 'ROOT' Returns: Type Description None Source code in shrike/pipeline/testing/pipeline_class_test.py def deeptest_graph ( pipeline , definition , path = \"ROOT\" ): \"\"\"Recursively compare a pipeline object to a serialized definition [EXPERIMENTAL] Args: pipeline (json): source for the comparison definition (json): target/reference for the comparison path (str): current path of the comparison (in the json tree) Returns: None \"\"\" if definition is None : # no definition provided, let's stop inspection at this path log . info ( f \"deeptest_graph @ { path } : nop, definition is None\" ) return # is inspecting a dictionary structure, iterate on keys if isinstance ( pipeline , dict ) and isinstance ( definition , dict ): log . info ( f \"deeptest_graph @ { path } : checking dictionary\" ) for key in definition : assert ( key in pipeline ), f \"pipeline graph does not have key { key } at level @ { path } \" # ignoring all ids if key in { \"id\" , \"node_id\" , \"module_id\" , \"dataset_id\" }: log . info ( f \"deeptest_graph @ { path } : ignore id key { key } \" ) return if ( key in { \"run_settings\" , \"compute_run_settings\" } and definition [ key ] is not None ): # this is a specific kind of key containing a list we're transforming into a dict log . info ( f \"deeptest_graph @ { path } : refactoring key { key } as dict\" ) pipeline_run_settings = dict ( [( entry [ \"name\" ], entry ) for entry in pipeline [ key ]] ) definition_run_settings = dict ( [( entry [ \"name\" ], entry ) for entry in definition [ key ]] ) deeptest_graph ( pipeline_run_settings , definition_run_settings , path + \".(runsettings)\" + key , ) else : deeptest_graph ( pipeline [ key ], definition [ key ], path + \".\" + key ) return # is inspecting a list structure, each element MUST passed # NOTE: this should be improved in case list can be shuffled ? if isinstance ( pipeline , list ) and isinstance ( definition , list ): log . info ( f \"deeptest_graph @ { path } : checking list\" ) for key , entry in enumerate ( definition ): deeptest_graph ( pipeline [ key ], entry , path + \"[\" + str ( key ) + \"]\" ) return # if anything else (int, str, unknown), just test plain equality log . info ( f \"deeptest_graph @ { path } : checking equality { pipeline } == { definition } \" ) assert pipeline == definition , f \"values mismatch @ { path } \"","title":"deeptest_graph()"},{"location":"pipeline/testing-pipeline-class-test/#shrike.pipeline.testing.pipeline_class_test.deeptest_graph_comparison","text":"Compare a pipeline object to a serialized definition [EXPERIMENTAL] Parameters: Name Type Description Default pipeline_export_file str path to pipeline exported file required pipeline_definition_file str path to reference file required Returns: Type Description None Source code in shrike/pipeline/testing/pipeline_class_test.py def deeptest_graph_comparison ( pipeline_export_file , pipeline_definition_file ): \"\"\"Compare a pipeline object to a serialized definition [EXPERIMENTAL] Args: pipeline_export_file (str): path to pipeline exported file pipeline_definition_file (str): path to reference file Returns: None \"\"\" # checks the exported file in temp dir assert os . path . isfile ( pipeline_export_file ), f \"deeptest_graph_comparison() expects a file as first argument but { pipeline_export_file } does not exist.\" assert os . path . isfile ( pipeline_definition_file ), f \"deeptest_graph_comparison() expects a file as second argument but { pipeline_definition_file } does not exist.\" # read the exported graph with open ( pipeline_export_file , \"r\" ) as export_file : pipeline = json . loads ( export_file . read ()) assert ( pipeline is not None ), f \"deeptest_graph_comparison() expects first argument to be a parsable json, instead it found None\" with open ( pipeline_definition_file , \"r\" ) as definition_file : definition = json . loads ( definition_file . read ()) assert ( definition is not None ), f \"deeptest_graph_comparison() expects first argument to be a parsable json, instead it found None\" deeptest_graph ( pipeline , definition )","title":"deeptest_graph_comparison()"},{"location":"pipeline/testing-pipeline-class-test/#shrike.pipeline.testing.pipeline_class_test.get_config_class","text":"Test if the get_arg_parser() method is in there and behaves correctly Source code in shrike/pipeline/testing/pipeline_class_test.py def get_config_class ( pipeline_class ): \"\"\"Test if the get_arg_parser() method is in there and behaves correctly\"\"\" try : config_class = pipeline_class . get_config_class () except : assert ( False ), \"getting config class for pipeline class {} resulted in an exception: {} \" . format ( pipeline_class . __name__ , traceback . format_exc () )","title":"get_config_class()"},{"location":"pipeline/testing-pipeline-class-test/#shrike.pipeline.testing.pipeline_class_test.pipeline_required_modules","text":"Test if the required_modules() returns the right list of modules with all required keys Source code in shrike/pipeline/testing/pipeline_class_test.py def pipeline_required_modules ( pipeline_class ): \"\"\"Test if the required_modules() returns the right list of modules with all required keys\"\"\" modules_manifest = pipeline_class . required_modules () assert isinstance ( modules_manifest , dict ), \"required_modules() must return a dictionary.\" error_log = [] for module_key , module_description in modules_manifest . items (): if not isinstance ( module_description , dict ): error_log . append ( f \"values in dictionary returned by required_modules() must be dictionaries (under key= { module_key } , found value of type= { module_description . __name__ } )\" ) continue if \"yaml_spec\" not in module_description : error_log . append ( f \"In pipeline class module { pipeline_class . __name__ } ,\" + f \" module under key= { module_key } (in required_modules() function)\" + \" does not provide any yaml_spec key.\" + \" You need to give such a yaml_spec path before creating your pull request\" + \" so that we're able to consume this module when running pre-merge tests (detonation chamber)\" ) if \"remote_module_name\" not in module_description : error_log . append ( f \"In pipeline class module { pipeline_class . __name__ } ,\" + f \" module under key= { module_key } (in required_modules() function)\" + \" does not provide any remote_module_name.\" + \" You need to give such a name before creating your pull request\" + \" so that we're able to consume this module when running in production.\" ) # if \"namespace\" not in module_description: # error_log.append( # f\"In pipeline class module {pipeline_class.__name__},\" # + f\" module under key={module_key} (in required_modules() function)\" # + \" does not provide any namespace.\" # ) # TODO : verify if the version exists or is in the yaml spec? assert not ( error_log ), ( f \"In pipeline class module { pipeline_class . __name__ } , validation of the dictionary returned by required_modules() method shows errors: \\n \" + \" \\n \" . join ( error_log ) )","title":"pipeline_required_modules()"},{"location":"pipeline/testing-pipeline-class-test/#shrike.pipeline.testing.pipeline_class_test.pipeline_required_subgraphs","text":"Tests if the required_subgraphs() returns the right list of modules with all requires keys Source code in shrike/pipeline/testing/pipeline_class_test.py def pipeline_required_subgraphs ( pipeline_class ): \"\"\"Tests if the required_subgraphs() returns the right list of modules with all requires keys\"\"\" subgraphs_manifest = pipeline_class . required_subgraphs () assert isinstance ( subgraphs_manifest , dict ), \"required_subgraphs() must return a dictionary.\" error_log = [] for subgraph_key , subgraph_class in subgraphs_manifest . items (): if not issubclass ( subgraph_class , AMLPipelineHelper ): error_log . append ( f \"In pipeline class module { pipeline_class . __name__ } , values in dictionary returned by required_subgraphs() must be subclass of AMLPipelineHelper (under key= { subgraph_key } , found object { subgraph_class . __name__ } )\" ) continue assert not ( error_log ), ( f \"In pipeline class module { pipeline_class . __name__ } , validation of the dictionary returned by required_subgraphs() shows errors: \" + \" \\n \" . join ( error_log ) )","title":"pipeline_required_subgraphs()"},{"location":"rfc/federated-learning/","text":"API design for Federated Learning To facilitate federated learning, we propose some new APIs for shrike. Code structure The code structure for a federated learning experiment is consistent with the existing one: repo \u251c\u2500\u2500 components # contains subfolder corresponds to an AML component \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 component1 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 component2 \u2514\u2500\u2500 pipelines \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 aml \u2502 \u2502 \u2514\u2500\u2500\u2500 eyesoff.yaml # workspace info: subscription_id, resource_group, etc. \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 compute \u2502 \u2502 \u2514\u2500\u2500\u2500 eyesoff.yaml # see example below; info on compute targets, data stores, silos, etc. \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 experiments \u2502 \u2502 \u2514\u2500\u2500\u2500 submit.yaml # see example below; info on experiment name, hdi_conf, datasets, etc. \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 modules \u2502 \u2502 \u2514\u2500\u2500\u2500 module_defaults.yaml # contains some yaml files describing which components in `components` or remote copies will be used \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 experiments \u2502 \u2514\u2500\u2500 submit.py # see example below; main script for defining and submiting the experiment \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 subgraphs \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500 pipeline_definition.py # see example below; subgraphs used in the main script submit.py Federated learning workflow We consider four fundamental building blocks: preprocess pipeline, midprocess pipeline, postprocess pipeline, training pipeline. The workflow is shown as in the diagram: preprocess (orchestrator) -> training (silos) -> midprocess (orchestrator) -> training (silos) -> midprocess (orchestrator) ... many rounds -> training (silos) -> postprocess (orchestrator) In Azure ML, each \"job\" can be implemented as a subgraph, or a simple component. On web UI, it is expected to show as a single experiment, with multiple steps and subgraphs, just like the diagram above. Note that this diagram assumes the strategy will be always synchronous and centralized. See \"open questions\" for scenarios that may not be fitted into this digram. Main submission script: submit.py We propose two designs. In the first design, Shrike provides a class FederatedPipelineBase with built-in methods for the user to inherit and override. The second design requires the user to explictly write the loop, and define the hook connecting upstream and downstream pipelines. While the second design provides more flexibility, the first design serves a similar role as PyTorch Lightning by provoding an easy structured way to do federated learning. Design 1: Shrike handles the loop Shrike will implement a new FederatedPipelineBase class which extends AMLPipelineHelper . The FederatedPipelineBase class provides the structure for federated learning, through four methods preprocess() , midprocess() , training() , and postprocess() . Each method returns the output data that will be fed into the next pipeline. Shrike will automatically validate, anonymize, and transfer the outputs from upstream pipelines to downstream pipelines (through calling apply_federated_runsettings from the backend). User can override the inherited methods. In this design, we will avoid many user errors that might happen if they write the loop explicitly. However, it could be tricky to define how the pipelines are connected. from shrike.pipeline import FederatedPipelineBase from .pipeline_definition import TrainingPipeline , PreprocessPipeline , MidProcessPipeline , PostProcessPipeline class MyCoolPipeline ( FederatedPipelineBase ): # TODO: what would this look like? # define each pipeline # define parameters (groups) @classmethod def required_subgraphs ( cls ): # implement this new method # need a better name return { \"TrainingPipeline\" : TrainingPipeline , \"PreprocessPipeline\" : PreprocessPipeline , \"MidProcessPipeline\" : MidProcessPipeline , \"PostProcessPipeline\" : PostProcessPipeline } # each function returns the output that will pass on to the next pipeline def preprocess ( self , config ): # required function, will run inside orchestrator preprocess_pipeline = self . subgraph_load ( \"PreprocessPipeline\" ) # also allowing loading a single component preprocess_pipeline_step = preprocess_pipeline ( input = config . federation . preprocess . input , param = config . federation . preprocess . param ) return preprocess . outputs . output1 def midprocess ( self , config ): # required function, will run inside orchestrator midprocess_pipeline = self . subgraph_load ( \"MidProcessPipeline\" ) # outputs from all silos will be moved into a shared folder, and midprocess will take this folder as input midprocess_pipeline_step = midprocess_pipeline ( input = training . outputs . output1 ) return midprocess_pipeline_step . outputs . output1 def postprocess ( self , config ): # required function, will run inside orchestrator postprocess_pipeline = self . subgraph_load ( \"PostProcessPipeline\" ) postprocess_pipeline_step = postprocess_pipeline ( input = training . outputs . output1 , param_1 = self . config . federation . postprocess . param_1 , param_2 = self . config . federation . postprocess . param_2 ) return postprocess_pipeline_step . outputs . output1 def training ( self , config , input ): # required function, will run in each silo # Shrike handles this: input = preprocess.outputs.output1 if self.first_epoch else midprocess.outputs.output1 training_pipeline = self . subgraph_load ( \"TrainingPipeline\" ) training_pipeline_step = training_pipeline ( input = config . federation . training . input1 , weights = input , param1 = self . config . federation . training . param1 , param2 = self . config . federation . training . param2 ) return training_pipeline_step . outputs . output1 # if __name__ == \"__main__\": # MyCoolPipeline() Design 2: explicitly writing the loop by user As the main pipeline script, here we need to define the subgraphs and how they are connected. This solution requires the user to explicity write the loop and the data movement. We provide apply_federated_runsettings(pipeline_instance, silos=*, validation=True, secure_aggregation=True) method and the pipeline_instance will be running in all silos configured in the yaml file. With secure_aggregation=True , Shrike will anonymize the data and make it ready for transfer. from shrike.pipeline import AMLPipelineHelper from .pipeline_definition import TrainingPipeline , PreprocessPipeline , MidProcessPipeline , PostProcessPipeline class MyCoolPipeline ( AMLPipelineHelper ): # TODO: what would this look like? # define each pipeline # define parameters (groups) @classmethod def required_subgraphs ( cls ): # implement this new method # need a better name return { \"TrainingPipeline\" : TrainingPipeline , \"PreprocessPipeline\" : PreprocessPipeline , \"MidProcessPipeline\" : MidProcessPipeline , \"PostProcessPipeline\" : PostProcessPipeline } def build ( self , config ): preprocess_pipeline = self . subgraph_load ( \"PreprocessPipeline\" ) # also allowing loading a single component midprocess_pipeline = self . subgraph_load ( \"MidProcessPipeline\" ) postprocess_pipeline = self . subgraph_load ( \"PostProcessPipeline\" ) training_pipeline = self . subgraph_load ( \"TrainingPipeline\" ) data_transfer = self . component_load ( \"DataTransfer\" ) @dsl . pipeline ( name = \"federated-learning-pipeline\" , description = \"\" , default_datastore = config . compute . compliant_datastore , ) def federated_pipeline_function (): # initialization preprocess_pipeline_step = preprocess_pipeline ( input = config . federation . preprocess . input , param = config . federation . preprocess . param ) # self.apply_smart_runsettings(preprocess_pipeline_step) # required if preprocess is a component, instead of a subgraph iter = 0 self . apply_federated_runsettings ( preprocess_pipeline_step ) data_transfer_step = data_transfer ( input = preprocess_pipeline_step . outputs . output1 , fanout = True ) # iterations while iter < config . federation . max_iterations : training_pipeline_step = training_pipeline ( input = config . federation . training . input1 , weights = data_transfer_step . outputs . output1 , param2 = config . federation . training . param ) self . apply_federated_runsettings ( training_pipeline_step ) # apply_federated_runsettings(pipeline_instance, silos = config.federation.silos) data_transfer_step = data_transfer ( input = training_pipeline_step . outputs . output1 , fanout = False ) self . apply_federated_runsettings ( data_transfer_step ) midprocess_pipeline_step = midprocess_pipeline ( input = data_transfer_step . outputs . output1 param = config . federation . midprocess . param ) # self.apply_smart_runsettings(midprocess_pipeline_step) data_transfer_step = data_transfer ( input = midprocess_pipeline_ste , fanout = True ) # finalization postprocess_pipeline_step = postprocess_pipeline ( input = data_transfer_step . outputs . output1 , param_1 = config . federation . postprocess . param_1 , param_2 = config . federation . postprocess . param_2 ) # self.apply_smart_runsettings(postprocess_pipeline_step) return federated_pipeline_function def pipeline_instance ( self , pipeline_function , config ): my_cool_pipeline = pipeline_function () return my_cool_pipeline if __name__ == \"__main__\" : MyCoolPipeline () apply_federated_runsettings() method We will implement the following method so that the user can assign pipeline to silos in a simple way. Also, with validation=True and secure_aggregation=True , Shrike will validate and anonymize the data, thus making it ready for transfer. Secure aggregation is not required in the \"fan-out\" stage, while validation is required before any cross-silo movement. The arg silos defines in which silos the pipeline_instance will be performing in. def apply_federated_runsettings ( pipeline_instance , silos =* , validation = True , secure_aggregation = True ): ... This method should be called after apply_smart_runsettings() by copying the pipeline_instance to multiple silos and ensuring the outputs are safe for transfer, and should be able to override settings already configured in apply_smart_runsettings() . Compute settings eyesoff.yaml We add computes and storage info for each silo, also notice the datasets that live in each silo are also specified here, e.g. input1 . # name of default target default_compute_target : \"cpu-cluster-0-dc\" # where intermediary output is written compliant_datastore : \"heron_sandbox_storage\" noncompliant_datastore : \"cosmos14_office_adhoc\" # Linux targets linux_cpu_dc_target : \"cpu-cluster-0-dc\" linux_cpu_prod_target : \"cpu-cluster-0\" linux_gpu_dc_target : \"gpu-cluster-dc\" linux_gpu_prod_target : \"gpu-cluster-0\" # data I/O for linux modules linux_input_mode : \"mount\" linux_output_mode : \"mount\" # Windows targets windows_cpu_prod_target : \"cpu-cluster-win\" windows_cpu_dc_target : \"cpu-cluster-win\" # data I/O for windows modules windows_input_mode : \"download\" windows_output_mode : \"upload\" # hdi cluster hdi_prod_target : \"hdi-cluster\" # data transfer cluster datatransfer_target : \"data-factory\" ## new! silos : # override priority: default_config < customized shared config < per-silo setting default_config : compute : gpu-cluster-1 # by default, all silos will inherit this default_config param1 : 0 foo_config : # customized config that multiple silos could share compute : gpu-cluster-foo bar_config : datatransfer_target : \"data-factory-bar\" silo1 : compute : gpu-cluster-2 compliant_datastore : heron_sandbox_storage datatransfer_target : \"data-factory\" input1 : dummy_dataset_on_silo1 silo2 : inherit : foo_config, bar_config # merge default_config, foo_config, and bar_config (highest order) ... input1 : dummy_dataset_on_silo2 param1 : 1 silo3 : inherit : foo_config ... input1 : dummy_dataset_on_silo3 param1 : 2 Configuration file: submit.yaml The user needs to add an additional section federation with required max_iterations and silos . Also the user can specify the input datasets and parameters for pipelines running inside the orchestrator and silos here. For input dataset in silos, the syntax is <pipeline_input_name>: silos.<dataset_name> , where <dataset_name> is specified for each silo in the compute settings eyesoff.yaml . defaults : ... run : experiment_name : \"sample_federated_pipeline\" ... # new! federation : max_iterations = 100 silos = \"silo1, silo2, silo3\" # each silo corresponds to a section in compute/eyesoff.yaml # could also use: `silos = *`, `silos = \"!silo1\"` preprocess : input : dummy_dataset param : 1 midprocess : param : \"a\" postprocess : param_1 : 1 param_2 : True training : input : silos.input1 # per silo input dataset param1 : silos.param1 # per silo parameter param2 : 1 # shared parameters/datasets across all silos Define a subgraph as usual: pipeline_definition.py As usual, the user can define each \"job\" as a subgraph or a single component, and import using load_component() or load_subgraph() in the pipeline script. Here we # skeleton code for defining a pipeline from azure.ml.component import dsl from shrike.pipeline import AMLPipelineHelper class TrainingPipeline ( AMLPipelineHealer ): def build ( self , config ): # load components my_component = self . component_load ( \"my-component\" ) # create an instantce of a pipeline function @dsl . pipeline ( name = \"A-simple-pipeline\" , description = \"\" , default_datastore = config . compute . compliant_datastore , ) def my_pipeline_function ( InputData , Weights , Param ): # assume my_component takes three arguments, an input dataset, an epoch that should increments each run, and an parameter that can be dynamically changed in each run step = my_component ( input = InputData , weights = Weights , param = Param ) self . apply_smart_runsettings ( step ) return my_pipeline_function class PreprocessPipeline ( AMLPipelineHealer ): ... class MidprocessPipeline ( AMLPipelineHealer ): ... class PostprocessPipeline ( AMLPipelineHealer ): ... Flexibility Per-silo training We should allow some per-silo variability, which should be handled through apply_federated_runsettings() . For design 1, by default, the pipeline defined in def training() will be applied to all silos with an implicit call apply_federated_runsettings(pipeline_instance, silos=*) . When this function returns more than one pipeline, then the user could specify the mapping, e.g. per_silo_training_1 is applied to silo 1 and 2, and per_silo_training_2 is applied to all other silos, def training ( self , config , input ): per_silo_training_1 = self . subgraph_load ( \"TrainingPipeline\" ) per_silo_training_1_step = per_silo_training_1 ( input = config . federation . training . input1 , weights = input , param1 = self . config . federation . training . param1 , param2 = self . config . federation . training . param2 ) self . apply_federated_runsettings ( per_silo_training_1_step , silos = \"silo1, silo2\" ) per_silo_training_2 = self . subgraph_load ( \"TrainingPipeline2\" ) per_silo_training_2_step = per_silo_training_2 ( input = config . federation . training . input1 , weights = input , param1 = self . config . federation . training . param1 , param2 = self . config . federation . training . param2 ) self . apply_federated_runsettings ( per_silo_training_2_step , silos = \"*silo1, *silo2\" ) return [ per_silo_training_1_step . outputs . output1 , per_silo_training_2_step . outputs . output1 ] For design 2, it is more straightforward and the user can just define the per-silo training directly. while iter < config . federation . max_iterations : per_silo_training_1_step = ... self . apply_federated_runsettings ( per_silo_training_1_step , silos = \"silo1, silo2\" ) per_silo_training_2_step = ... self . apply_federated_runsettings ( per_silo_training_2_step , silos = \"!silo1, !silo2\" ) Output anything There is no limitation that what the four methods shouls output, i.e. we are not limited to the scenario where we are just outputting and aggregating the model weights. A use case is federated distillation where the server asks for predictions over some public dataset, instead of models or gradients. Debug mode The library will also have \"debug\" mode where it turns off all data transfer and assumes no approval endpoint. This way, data scientists could quickly debug with \"dummy\" (artificial) silos in eyes-on or reserach context. Open questions How are the model weights aggregated after each \"training\" round? User provides a weight in config file. Shrike can configure the output path to /output_epoch_<n>/silo<x> , and the downstream pipeline will read from /output_epoch_<n> . Platform ask: args (N input datasets) as input to a single component. Worst-case scenario, we can do some kind of \"merge folders\" hack in advance. What parameters need to be changed dynamically, i.e. not-fixed for each iteration, other than \"input data set\"? We should cache things like optimizer state (learning rate, etc.), previous models/gradients on disk so that they can be restored in the next round. What other parameters are required in federation section in submit.yaml ? How to handle regional outages? Even though silos are reliable Azure-hosted clusters, they'll occasionally go down. Hopefully, AML pipelines can, as a general feature, allow configurable per-step retry. How to handle early stopping? Once conditional components land, those should be configurable as well (maybe after aggregation, if configured, in each round?). The design also assumes the orchestrator has significant power over the silos- whoever runs code in the orchestrator can effectively inject any code they want to run in each silo. Maybe the design should allow room for the individual silos to control their own internal train() pipeline and this method is more of an RPC into the silo to execute code owned by the silo. How to handle the drop-out or failure of some training processes? Does the federated learning pipeline always look like as the diagram shown above? Does the upstream pipeline just generate one output (e.g. weights) to feed into the downstream pipeline? Scenario : asynchronicity, e.g. client A is taking a very long time to do a round, and in the meantime all the other clients can do 5 rounds. In our current design, the downstream pipeline need to wait until all clients finish their work. How much flexibility do we need to provide to each silo, e.g. PerSiloPipeline ? See section Flexibility/Per-silo training. [Need discussion] Do we support subgraph + component as a \"pipeline\", or just subgraph? What is a good naming for the four methods (preprocess/training/midprocess/postprocess)? Scenarios Alice creates a FL project with 30 silos, and submits a debugging pipeline to 3 silos. She can specify in submit.yaml : silos = \"silo1, silo2, silo3\" Alice wants to extend the pipeline to 27 silos. She can specify in submit.yaml : silos = \"!silo1, !silo2, !silo3\" Alice wants a different training pipeline for silo X. See section Flexibility/Per-silo training. The previous experiment fails after 10 epochs, and Alice wants to restart from there.","title":"API design for Federated Learning"},{"location":"rfc/federated-learning/#api-design-for-federated-learning","text":"To facilitate federated learning, we propose some new APIs for shrike.","title":"API design for Federated Learning"},{"location":"rfc/federated-learning/#code-structure","text":"The code structure for a federated learning experiment is consistent with the existing one: repo \u251c\u2500\u2500 components # contains subfolder corresponds to an AML component \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 component1 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 component2 \u2514\u2500\u2500 pipelines \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 aml \u2502 \u2502 \u2514\u2500\u2500\u2500 eyesoff.yaml # workspace info: subscription_id, resource_group, etc. \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 compute \u2502 \u2502 \u2514\u2500\u2500\u2500 eyesoff.yaml # see example below; info on compute targets, data stores, silos, etc. \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 experiments \u2502 \u2502 \u2514\u2500\u2500\u2500 submit.yaml # see example below; info on experiment name, hdi_conf, datasets, etc. \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 modules \u2502 \u2502 \u2514\u2500\u2500\u2500 module_defaults.yaml # contains some yaml files describing which components in `components` or remote copies will be used \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 experiments \u2502 \u2514\u2500\u2500 submit.py # see example below; main script for defining and submiting the experiment \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 subgraphs \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500 pipeline_definition.py # see example below; subgraphs used in the main script submit.py","title":"Code structure"},{"location":"rfc/federated-learning/#federated-learning-workflow","text":"We consider four fundamental building blocks: preprocess pipeline, midprocess pipeline, postprocess pipeline, training pipeline. The workflow is shown as in the diagram: preprocess (orchestrator) -> training (silos) -> midprocess (orchestrator) -> training (silos) -> midprocess (orchestrator) ... many rounds -> training (silos) -> postprocess (orchestrator) In Azure ML, each \"job\" can be implemented as a subgraph, or a simple component. On web UI, it is expected to show as a single experiment, with multiple steps and subgraphs, just like the diagram above. Note that this diagram assumes the strategy will be always synchronous and centralized. See \"open questions\" for scenarios that may not be fitted into this digram.","title":"Federated learning workflow"},{"location":"rfc/federated-learning/#main-submission-script-submitpy","text":"We propose two designs. In the first design, Shrike provides a class FederatedPipelineBase with built-in methods for the user to inherit and override. The second design requires the user to explictly write the loop, and define the hook connecting upstream and downstream pipelines. While the second design provides more flexibility, the first design serves a similar role as PyTorch Lightning by provoding an easy structured way to do federated learning.","title":"Main submission script: submit.py"},{"location":"rfc/federated-learning/#design-1-shrike-handles-the-loop","text":"Shrike will implement a new FederatedPipelineBase class which extends AMLPipelineHelper . The FederatedPipelineBase class provides the structure for federated learning, through four methods preprocess() , midprocess() , training() , and postprocess() . Each method returns the output data that will be fed into the next pipeline. Shrike will automatically validate, anonymize, and transfer the outputs from upstream pipelines to downstream pipelines (through calling apply_federated_runsettings from the backend). User can override the inherited methods. In this design, we will avoid many user errors that might happen if they write the loop explicitly. However, it could be tricky to define how the pipelines are connected. from shrike.pipeline import FederatedPipelineBase from .pipeline_definition import TrainingPipeline , PreprocessPipeline , MidProcessPipeline , PostProcessPipeline class MyCoolPipeline ( FederatedPipelineBase ): # TODO: what would this look like? # define each pipeline # define parameters (groups) @classmethod def required_subgraphs ( cls ): # implement this new method # need a better name return { \"TrainingPipeline\" : TrainingPipeline , \"PreprocessPipeline\" : PreprocessPipeline , \"MidProcessPipeline\" : MidProcessPipeline , \"PostProcessPipeline\" : PostProcessPipeline } # each function returns the output that will pass on to the next pipeline def preprocess ( self , config ): # required function, will run inside orchestrator preprocess_pipeline = self . subgraph_load ( \"PreprocessPipeline\" ) # also allowing loading a single component preprocess_pipeline_step = preprocess_pipeline ( input = config . federation . preprocess . input , param = config . federation . preprocess . param ) return preprocess . outputs . output1 def midprocess ( self , config ): # required function, will run inside orchestrator midprocess_pipeline = self . subgraph_load ( \"MidProcessPipeline\" ) # outputs from all silos will be moved into a shared folder, and midprocess will take this folder as input midprocess_pipeline_step = midprocess_pipeline ( input = training . outputs . output1 ) return midprocess_pipeline_step . outputs . output1 def postprocess ( self , config ): # required function, will run inside orchestrator postprocess_pipeline = self . subgraph_load ( \"PostProcessPipeline\" ) postprocess_pipeline_step = postprocess_pipeline ( input = training . outputs . output1 , param_1 = self . config . federation . postprocess . param_1 , param_2 = self . config . federation . postprocess . param_2 ) return postprocess_pipeline_step . outputs . output1 def training ( self , config , input ): # required function, will run in each silo # Shrike handles this: input = preprocess.outputs.output1 if self.first_epoch else midprocess.outputs.output1 training_pipeline = self . subgraph_load ( \"TrainingPipeline\" ) training_pipeline_step = training_pipeline ( input = config . federation . training . input1 , weights = input , param1 = self . config . federation . training . param1 , param2 = self . config . federation . training . param2 ) return training_pipeline_step . outputs . output1 # if __name__ == \"__main__\": # MyCoolPipeline()","title":"Design 1: Shrike handles the loop"},{"location":"rfc/federated-learning/#design-2-explicitly-writing-the-loop-by-user","text":"As the main pipeline script, here we need to define the subgraphs and how they are connected. This solution requires the user to explicity write the loop and the data movement. We provide apply_federated_runsettings(pipeline_instance, silos=*, validation=True, secure_aggregation=True) method and the pipeline_instance will be running in all silos configured in the yaml file. With secure_aggregation=True , Shrike will anonymize the data and make it ready for transfer. from shrike.pipeline import AMLPipelineHelper from .pipeline_definition import TrainingPipeline , PreprocessPipeline , MidProcessPipeline , PostProcessPipeline class MyCoolPipeline ( AMLPipelineHelper ): # TODO: what would this look like? # define each pipeline # define parameters (groups) @classmethod def required_subgraphs ( cls ): # implement this new method # need a better name return { \"TrainingPipeline\" : TrainingPipeline , \"PreprocessPipeline\" : PreprocessPipeline , \"MidProcessPipeline\" : MidProcessPipeline , \"PostProcessPipeline\" : PostProcessPipeline } def build ( self , config ): preprocess_pipeline = self . subgraph_load ( \"PreprocessPipeline\" ) # also allowing loading a single component midprocess_pipeline = self . subgraph_load ( \"MidProcessPipeline\" ) postprocess_pipeline = self . subgraph_load ( \"PostProcessPipeline\" ) training_pipeline = self . subgraph_load ( \"TrainingPipeline\" ) data_transfer = self . component_load ( \"DataTransfer\" ) @dsl . pipeline ( name = \"federated-learning-pipeline\" , description = \"\" , default_datastore = config . compute . compliant_datastore , ) def federated_pipeline_function (): # initialization preprocess_pipeline_step = preprocess_pipeline ( input = config . federation . preprocess . input , param = config . federation . preprocess . param ) # self.apply_smart_runsettings(preprocess_pipeline_step) # required if preprocess is a component, instead of a subgraph iter = 0 self . apply_federated_runsettings ( preprocess_pipeline_step ) data_transfer_step = data_transfer ( input = preprocess_pipeline_step . outputs . output1 , fanout = True ) # iterations while iter < config . federation . max_iterations : training_pipeline_step = training_pipeline ( input = config . federation . training . input1 , weights = data_transfer_step . outputs . output1 , param2 = config . federation . training . param ) self . apply_federated_runsettings ( training_pipeline_step ) # apply_federated_runsettings(pipeline_instance, silos = config.federation.silos) data_transfer_step = data_transfer ( input = training_pipeline_step . outputs . output1 , fanout = False ) self . apply_federated_runsettings ( data_transfer_step ) midprocess_pipeline_step = midprocess_pipeline ( input = data_transfer_step . outputs . output1 param = config . federation . midprocess . param ) # self.apply_smart_runsettings(midprocess_pipeline_step) data_transfer_step = data_transfer ( input = midprocess_pipeline_ste , fanout = True ) # finalization postprocess_pipeline_step = postprocess_pipeline ( input = data_transfer_step . outputs . output1 , param_1 = config . federation . postprocess . param_1 , param_2 = config . federation . postprocess . param_2 ) # self.apply_smart_runsettings(postprocess_pipeline_step) return federated_pipeline_function def pipeline_instance ( self , pipeline_function , config ): my_cool_pipeline = pipeline_function () return my_cool_pipeline if __name__ == \"__main__\" : MyCoolPipeline ()","title":"Design 2: explicitly writing the loop by user"},{"location":"rfc/federated-learning/#apply_federated_runsettings-method","text":"We will implement the following method so that the user can assign pipeline to silos in a simple way. Also, with validation=True and secure_aggregation=True , Shrike will validate and anonymize the data, thus making it ready for transfer. Secure aggregation is not required in the \"fan-out\" stage, while validation is required before any cross-silo movement. The arg silos defines in which silos the pipeline_instance will be performing in. def apply_federated_runsettings ( pipeline_instance , silos =* , validation = True , secure_aggregation = True ): ... This method should be called after apply_smart_runsettings() by copying the pipeline_instance to multiple silos and ensuring the outputs are safe for transfer, and should be able to override settings already configured in apply_smart_runsettings() .","title":"apply_federated_runsettings() method"},{"location":"rfc/federated-learning/#compute-settings-eyesoffyaml","text":"We add computes and storage info for each silo, also notice the datasets that live in each silo are also specified here, e.g. input1 . # name of default target default_compute_target : \"cpu-cluster-0-dc\" # where intermediary output is written compliant_datastore : \"heron_sandbox_storage\" noncompliant_datastore : \"cosmos14_office_adhoc\" # Linux targets linux_cpu_dc_target : \"cpu-cluster-0-dc\" linux_cpu_prod_target : \"cpu-cluster-0\" linux_gpu_dc_target : \"gpu-cluster-dc\" linux_gpu_prod_target : \"gpu-cluster-0\" # data I/O for linux modules linux_input_mode : \"mount\" linux_output_mode : \"mount\" # Windows targets windows_cpu_prod_target : \"cpu-cluster-win\" windows_cpu_dc_target : \"cpu-cluster-win\" # data I/O for windows modules windows_input_mode : \"download\" windows_output_mode : \"upload\" # hdi cluster hdi_prod_target : \"hdi-cluster\" # data transfer cluster datatransfer_target : \"data-factory\" ## new! silos : # override priority: default_config < customized shared config < per-silo setting default_config : compute : gpu-cluster-1 # by default, all silos will inherit this default_config param1 : 0 foo_config : # customized config that multiple silos could share compute : gpu-cluster-foo bar_config : datatransfer_target : \"data-factory-bar\" silo1 : compute : gpu-cluster-2 compliant_datastore : heron_sandbox_storage datatransfer_target : \"data-factory\" input1 : dummy_dataset_on_silo1 silo2 : inherit : foo_config, bar_config # merge default_config, foo_config, and bar_config (highest order) ... input1 : dummy_dataset_on_silo2 param1 : 1 silo3 : inherit : foo_config ... input1 : dummy_dataset_on_silo3 param1 : 2","title":"Compute settings eyesoff.yaml"},{"location":"rfc/federated-learning/#configuration-file-submityaml","text":"The user needs to add an additional section federation with required max_iterations and silos . Also the user can specify the input datasets and parameters for pipelines running inside the orchestrator and silos here. For input dataset in silos, the syntax is <pipeline_input_name>: silos.<dataset_name> , where <dataset_name> is specified for each silo in the compute settings eyesoff.yaml . defaults : ... run : experiment_name : \"sample_federated_pipeline\" ... # new! federation : max_iterations = 100 silos = \"silo1, silo2, silo3\" # each silo corresponds to a section in compute/eyesoff.yaml # could also use: `silos = *`, `silos = \"!silo1\"` preprocess : input : dummy_dataset param : 1 midprocess : param : \"a\" postprocess : param_1 : 1 param_2 : True training : input : silos.input1 # per silo input dataset param1 : silos.param1 # per silo parameter param2 : 1 # shared parameters/datasets across all silos","title":"Configuration file: submit.yaml"},{"location":"rfc/federated-learning/#define-a-subgraph-as-usual-pipeline_definitionpy","text":"As usual, the user can define each \"job\" as a subgraph or a single component, and import using load_component() or load_subgraph() in the pipeline script. Here we # skeleton code for defining a pipeline from azure.ml.component import dsl from shrike.pipeline import AMLPipelineHelper class TrainingPipeline ( AMLPipelineHealer ): def build ( self , config ): # load components my_component = self . component_load ( \"my-component\" ) # create an instantce of a pipeline function @dsl . pipeline ( name = \"A-simple-pipeline\" , description = \"\" , default_datastore = config . compute . compliant_datastore , ) def my_pipeline_function ( InputData , Weights , Param ): # assume my_component takes three arguments, an input dataset, an epoch that should increments each run, and an parameter that can be dynamically changed in each run step = my_component ( input = InputData , weights = Weights , param = Param ) self . apply_smart_runsettings ( step ) return my_pipeline_function class PreprocessPipeline ( AMLPipelineHealer ): ... class MidprocessPipeline ( AMLPipelineHealer ): ... class PostprocessPipeline ( AMLPipelineHealer ): ...","title":"Define a subgraph as usual: pipeline_definition.py"},{"location":"rfc/federated-learning/#flexibility","text":"","title":"Flexibility"},{"location":"rfc/federated-learning/#per-silo-training","text":"We should allow some per-silo variability, which should be handled through apply_federated_runsettings() . For design 1, by default, the pipeline defined in def training() will be applied to all silos with an implicit call apply_federated_runsettings(pipeline_instance, silos=*) . When this function returns more than one pipeline, then the user could specify the mapping, e.g. per_silo_training_1 is applied to silo 1 and 2, and per_silo_training_2 is applied to all other silos, def training ( self , config , input ): per_silo_training_1 = self . subgraph_load ( \"TrainingPipeline\" ) per_silo_training_1_step = per_silo_training_1 ( input = config . federation . training . input1 , weights = input , param1 = self . config . federation . training . param1 , param2 = self . config . federation . training . param2 ) self . apply_federated_runsettings ( per_silo_training_1_step , silos = \"silo1, silo2\" ) per_silo_training_2 = self . subgraph_load ( \"TrainingPipeline2\" ) per_silo_training_2_step = per_silo_training_2 ( input = config . federation . training . input1 , weights = input , param1 = self . config . federation . training . param1 , param2 = self . config . federation . training . param2 ) self . apply_federated_runsettings ( per_silo_training_2_step , silos = \"*silo1, *silo2\" ) return [ per_silo_training_1_step . outputs . output1 , per_silo_training_2_step . outputs . output1 ] For design 2, it is more straightforward and the user can just define the per-silo training directly. while iter < config . federation . max_iterations : per_silo_training_1_step = ... self . apply_federated_runsettings ( per_silo_training_1_step , silos = \"silo1, silo2\" ) per_silo_training_2_step = ... self . apply_federated_runsettings ( per_silo_training_2_step , silos = \"!silo1, !silo2\" )","title":"Per-silo training"},{"location":"rfc/federated-learning/#output-anything","text":"There is no limitation that what the four methods shouls output, i.e. we are not limited to the scenario where we are just outputting and aggregating the model weights. A use case is federated distillation where the server asks for predictions over some public dataset, instead of models or gradients.","title":"Output anything"},{"location":"rfc/federated-learning/#debug-mode","text":"The library will also have \"debug\" mode where it turns off all data transfer and assumes no approval endpoint. This way, data scientists could quickly debug with \"dummy\" (artificial) silos in eyes-on or reserach context.","title":"Debug mode"},{"location":"rfc/federated-learning/#open-questions","text":"How are the model weights aggregated after each \"training\" round? User provides a weight in config file. Shrike can configure the output path to /output_epoch_<n>/silo<x> , and the downstream pipeline will read from /output_epoch_<n> . Platform ask: args (N input datasets) as input to a single component. Worst-case scenario, we can do some kind of \"merge folders\" hack in advance. What parameters need to be changed dynamically, i.e. not-fixed for each iteration, other than \"input data set\"? We should cache things like optimizer state (learning rate, etc.), previous models/gradients on disk so that they can be restored in the next round. What other parameters are required in federation section in submit.yaml ? How to handle regional outages? Even though silos are reliable Azure-hosted clusters, they'll occasionally go down. Hopefully, AML pipelines can, as a general feature, allow configurable per-step retry. How to handle early stopping? Once conditional components land, those should be configurable as well (maybe after aggregation, if configured, in each round?). The design also assumes the orchestrator has significant power over the silos- whoever runs code in the orchestrator can effectively inject any code they want to run in each silo. Maybe the design should allow room for the individual silos to control their own internal train() pipeline and this method is more of an RPC into the silo to execute code owned by the silo. How to handle the drop-out or failure of some training processes? Does the federated learning pipeline always look like as the diagram shown above? Does the upstream pipeline just generate one output (e.g. weights) to feed into the downstream pipeline? Scenario : asynchronicity, e.g. client A is taking a very long time to do a round, and in the meantime all the other clients can do 5 rounds. In our current design, the downstream pipeline need to wait until all clients finish their work. How much flexibility do we need to provide to each silo, e.g. PerSiloPipeline ? See section Flexibility/Per-silo training. [Need discussion] Do we support subgraph + component as a \"pipeline\", or just subgraph? What is a good naming for the four methods (preprocess/training/midprocess/postprocess)?","title":"Open questions"},{"location":"rfc/federated-learning/#scenarios","text":"Alice creates a FL project with 30 silos, and submits a debugging pipeline to 3 silos. She can specify in submit.yaml : silos = \"silo1, silo2, silo3\" Alice wants to extend the pipeline to 27 silos. She can specify in submit.yaml : silos = \"!silo1, !silo2, !silo3\" Alice wants a different training pipeline for silo X. See section Flexibility/Per-silo training. The previous experiment fails after 10 epochs, and Alice wants to restart from there.","title":"Scenarios"},{"location":"rfc/pipeline-image-override/","text":"Submission-Time Image Override Owner Approvers Participants Fuhui Fang Daniel Miller AML DS Team To allow components to operate across multiple workspaces (eyes-off vs. eyes-on), the image and base url of Python Package Index need to be adjusted accordingly. For example, given a signed component in Heron PROD environment, if the user wants to run it in MyData, then he/she need to change the base image from polymerprod to mcr or polymerdev . Currently, this change must be manually done by the user, which is not an optimal Data Science experience. This RFC ( R equest F or C omment) proposes the code patterns and expected behavior of automatic image override at submission time. It begins by giving a concrete proposal with sample code, followed by concerns and known risks of the proposal. A summary of the proposed workflow is also presented at the end. The following topics are out of scope for this RFC: - Switching between remote copies of a component: this should be handled through shrike.build . Here we only discuss how to switch from a remote copy to a local copy Proposal The shrike.pipeline library should automatically detect the tenant id and decide if image override is required. If required, it will modify component_spec.yaml for all components listed in use_local using the mapping defined, and revert the changes after submission . Note that in addition to base image, tags, and other fields should be adjusted accordingly. From a user perspective, the following change is required: in pipeline.yaml , adding a section for tenant_overrides : defaults : - aml : eyesoff - compute : eyesoff - modules : module_defaults run : experiment_name : test tags : accelerator_repo : test module_loader : local_steps_folder : ../../../components ## Adding the following section for this new feature tenant_overrides : # MSIT tenant 72f988bf-86f1-41af-91ab-2d7cd011db47 : environment.docker.image : polymerprod.azurecr.io/polymercd/prod_official/azureml_base_gpu_openmpi312cuda101cudnn7 : mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04 polymerprod.azurecr.io/polymercd/prod_official/fake_image : mcr.microsoft.com/azureml/fake_image tags : eyesoff : eyeson fake_tenant_id_using_dev : environment.docker.image : polymerprod.azurecr.io/polymercd/prod_official/azureml_base_gpu_openmpi312cuda101cudnn7 : polymerdev.azurecr.io/polymercd/dev_official/azureml_base_gpu_openmpi312cuda101cudnn7 tags : eyesoff : dev personal : # using file name to specify tenant instead of id environment.docker.image : 'polymerprod.azurecr.io/polymercd/prod_official/(.+)' : 'polymerdev.azurecr.io/polymercd/dev_official/$1' , with an example component spec.yaml defined as # yaml-language-server: $schema=https://componentsdk.blob.core.windows.net/jsonschema/DistributedComponent.json $schema : http://azureml/sdk-1-5/DistributedComponent.json name : canary-gpu version : 0.0.2 tags : eyesoff type : DistributedComponent is_deterministic : False description : test inputs : args : type : String optional : true environment : docker : image : polymerprod.azurecr.io/polymercd/prod_official/azureml_base_gpu_openmpi312cuda101cudnn7 conda : conda_dependencies : dependencies : - python=3.7 - pip : - azureml-core==1.27.0 - --index-url https://o365exchange.pkgs.visualstudio.com/_packaging/PolymerPythonPackages/pypi/simple/ launcher : type : mpi additional_arguments : >- python run.py [ {inputs.args} ] When a user submits the experiment to an eyes-on workspace under MSIT using command: python pipelines / experiments / test . py - -config-dir pipelines / config - -config-name experiments / test aml = eyeson compute = eyeson The spec.yaml would be modified for this submission as follows: $schema : http://azureml/sdk-1-5/DistributedComponent.json name : canary-gpu version : 0.0.2 tags : eyeson # Note change of tags type : DistributedComponent is_deterministic : False description : test inputs : args : type : String optional : true environment : docker : image : mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04 # Note change of image conda : conda_dependencies : dependencies : - python=3.7 - pip : - azureml-core==1.27.0 - --index-url https://o365exchange.pkgs.visualstudio.com/_packaging/PolymerPythonPackages/pypi/simple/ # This will be kept, as MyData/eyes-on workspaces have a connection to this feed. launcher : type : mpi additional_arguments : >- python run.py [ {inputs.args} ] Similarly, if the detected tenant is fake_tenant_id_using_dev , the base image will be pulled from polymerdev.azurecr.io and tag will be updated as dev , as defined. The following fields are changed after switching the tenant: - environment.docker.image - tags The added code should be interpreted as a JSONPath expression, and both \"naive\" string substitution and regex-based substitution are allowed, e.g.: 'polymerprod.azurecr.io/polymercd/prod_official/(.+)' : 'polymerdev.azurecr.io/polymercd/dev_official/$1' Question: what else should we allow override? Concerns and Risks This would edit the users' YAML files. Potential for concern with registered components. Solution 1: spec modification + revert in try/except block Solution 2: create a copy of spec and the delete We would then have to support editing all other fields (tags, etc.) MCR and Polymer have different image names, both of which may change over time. There's no clear 1:1 mapping. Fortunately, polymerdev is in MSIT and you can already get that attached to MyData. Also, clear 1:1 mapping between that and *prod. (add-on feature) In addition to explicitly specify tenant_id , user might also want to use a more readable keyword, e.g. the file name for workspace configs eyes-off , personal , etc.. There might be some issues, and we will investigate as we implement the feature. Summary The proposed workflow can be summarized as: Signed components designed for eyes-off User adds tenant_overrides to their pipeline yaml file, using \"naive\" string substitution and/or regex-based substitution User runs the command to submit the experiment to an eyes-on workspace The library detects the change-of-tenant, and modifies all component_spec.yaml to address image, PyPI, tags, etc. change component_spec.yaml reverted after experiment submitted","title":"Submission-time override (design)"},{"location":"rfc/pipeline-image-override/#submission-time-image-override","text":"Owner Approvers Participants Fuhui Fang Daniel Miller AML DS Team To allow components to operate across multiple workspaces (eyes-off vs. eyes-on), the image and base url of Python Package Index need to be adjusted accordingly. For example, given a signed component in Heron PROD environment, if the user wants to run it in MyData, then he/she need to change the base image from polymerprod to mcr or polymerdev . Currently, this change must be manually done by the user, which is not an optimal Data Science experience. This RFC ( R equest F or C omment) proposes the code patterns and expected behavior of automatic image override at submission time. It begins by giving a concrete proposal with sample code, followed by concerns and known risks of the proposal. A summary of the proposed workflow is also presented at the end. The following topics are out of scope for this RFC: - Switching between remote copies of a component: this should be handled through shrike.build . Here we only discuss how to switch from a remote copy to a local copy","title":"Submission-Time Image Override"},{"location":"rfc/pipeline-image-override/#proposal","text":"The shrike.pipeline library should automatically detect the tenant id and decide if image override is required. If required, it will modify component_spec.yaml for all components listed in use_local using the mapping defined, and revert the changes after submission . Note that in addition to base image, tags, and other fields should be adjusted accordingly. From a user perspective, the following change is required: in pipeline.yaml , adding a section for tenant_overrides : defaults : - aml : eyesoff - compute : eyesoff - modules : module_defaults run : experiment_name : test tags : accelerator_repo : test module_loader : local_steps_folder : ../../../components ## Adding the following section for this new feature tenant_overrides : # MSIT tenant 72f988bf-86f1-41af-91ab-2d7cd011db47 : environment.docker.image : polymerprod.azurecr.io/polymercd/prod_official/azureml_base_gpu_openmpi312cuda101cudnn7 : mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04 polymerprod.azurecr.io/polymercd/prod_official/fake_image : mcr.microsoft.com/azureml/fake_image tags : eyesoff : eyeson fake_tenant_id_using_dev : environment.docker.image : polymerprod.azurecr.io/polymercd/prod_official/azureml_base_gpu_openmpi312cuda101cudnn7 : polymerdev.azurecr.io/polymercd/dev_official/azureml_base_gpu_openmpi312cuda101cudnn7 tags : eyesoff : dev personal : # using file name to specify tenant instead of id environment.docker.image : 'polymerprod.azurecr.io/polymercd/prod_official/(.+)' : 'polymerdev.azurecr.io/polymercd/dev_official/$1' , with an example component spec.yaml defined as # yaml-language-server: $schema=https://componentsdk.blob.core.windows.net/jsonschema/DistributedComponent.json $schema : http://azureml/sdk-1-5/DistributedComponent.json name : canary-gpu version : 0.0.2 tags : eyesoff type : DistributedComponent is_deterministic : False description : test inputs : args : type : String optional : true environment : docker : image : polymerprod.azurecr.io/polymercd/prod_official/azureml_base_gpu_openmpi312cuda101cudnn7 conda : conda_dependencies : dependencies : - python=3.7 - pip : - azureml-core==1.27.0 - --index-url https://o365exchange.pkgs.visualstudio.com/_packaging/PolymerPythonPackages/pypi/simple/ launcher : type : mpi additional_arguments : >- python run.py [ {inputs.args} ] When a user submits the experiment to an eyes-on workspace under MSIT using command: python pipelines / experiments / test . py - -config-dir pipelines / config - -config-name experiments / test aml = eyeson compute = eyeson The spec.yaml would be modified for this submission as follows: $schema : http://azureml/sdk-1-5/DistributedComponent.json name : canary-gpu version : 0.0.2 tags : eyeson # Note change of tags type : DistributedComponent is_deterministic : False description : test inputs : args : type : String optional : true environment : docker : image : mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04 # Note change of image conda : conda_dependencies : dependencies : - python=3.7 - pip : - azureml-core==1.27.0 - --index-url https://o365exchange.pkgs.visualstudio.com/_packaging/PolymerPythonPackages/pypi/simple/ # This will be kept, as MyData/eyes-on workspaces have a connection to this feed. launcher : type : mpi additional_arguments : >- python run.py [ {inputs.args} ] Similarly, if the detected tenant is fake_tenant_id_using_dev , the base image will be pulled from polymerdev.azurecr.io and tag will be updated as dev , as defined. The following fields are changed after switching the tenant: - environment.docker.image - tags The added code should be interpreted as a JSONPath expression, and both \"naive\" string substitution and regex-based substitution are allowed, e.g.: 'polymerprod.azurecr.io/polymercd/prod_official/(.+)' : 'polymerdev.azurecr.io/polymercd/dev_official/$1' Question: what else should we allow override?","title":"Proposal"},{"location":"rfc/pipeline-image-override/#concerns-and-risks","text":"This would edit the users' YAML files. Potential for concern with registered components. Solution 1: spec modification + revert in try/except block Solution 2: create a copy of spec and the delete We would then have to support editing all other fields (tags, etc.) MCR and Polymer have different image names, both of which may change over time. There's no clear 1:1 mapping. Fortunately, polymerdev is in MSIT and you can already get that attached to MyData. Also, clear 1:1 mapping between that and *prod. (add-on feature) In addition to explicitly specify tenant_id , user might also want to use a more readable keyword, e.g. the file name for workspace configs eyes-off , personal , etc.. There might be some issues, and we will investigate as we implement the feature.","title":"Concerns and Risks"},{"location":"rfc/pipeline-image-override/#summary","text":"The proposed workflow can be summarized as: Signed components designed for eyes-off User adds tenant_overrides to their pipeline yaml file, using \"naive\" string substitution and/or regex-based substitution User runs the command to submit the experiment to an eyes-on workspace The library detects the change-of-tenant, and modifies all component_spec.yaml to address image, PyPI, tags, etc. change component_spec.yaml reverted after experiment submitted","title":"Summary"},{"location":"rfc/strict-component-validation/","text":"Strict Validation of Azure ML Components Owners Approvers Participants Haozhe Zhang Daniel Miller AML DS 1P enablers This document presents a proposal on how to enable strict validation of Azure ML components in the signing and registering builds by using the shrike library. Motivation At the prepare step of a signing and registering build, the shrike.build.command.prepare.validate_all_components() function executes an azure cli command az ml component validate --file ${component_spec_path} to validate whether the given component spec YAML file has any syntax errors or matches the strucuture of the pre-defined schema. The validation procedure, which is critical to satisfactory user experience, could greatly improve the code quality of user codebase and eliminate user errors before runtime. Nevertheless, different teams may have different \"rules\" for component validation. Apart from basic syntax checking and schema structure matching, our customers may have \"stricter\" validation requirements, to list a few, enforcing naming conventions(e.g., office.smartcompose.*) requiring email address in the component description forcing all input parameters to have non-empty descriptions To address this feature request, we plan to leverage (1) regular expression matching and (2) JSONPath expression in the shrike library. Proposal We will introduce two parameters - enable_component_validation (type: boolean , default: False ) and component_validation (type: dict , default: None ) into the configuration dataclass in shrike.build.core . The logic for performing strict component validation will be If config.enable_component_validation is True : if len(config.component_validation) == 0 : run the compliance validation only else: run the compliance validation, then run the user-provided customized validation, which is written as JSONPath expression and regular expression in config.component_validation . else Pass Compliance validation Compliance validation refers to checking whether a given component spec YAML file meets all the requirements for running in the compliant AML. Currently, we have three requirements to enforce, which are Check whether the image URL is compliant Check whether the pip index-url is https://o365exchange.pkgs.visualstudio.com/_packaging/PolymerPythonPackages/pypi/simple/ Check that \"default\" is only Conda feed In the future, if we find more requirements to obey, we will add them into compliance validation. Customized validation First, we ask users to write JSONPath expressions to query Azure ML component spec YAML elements. For example, the path of component name is $.name , while the path of image is $.environment.docker.image . Second, users are expected to translate their specific \"strict\" validation rules to regular expression patterns. For example, enforcing the component name to start with \"office.smartcompose.\" could be translated to a string pattern ^office.smartcompose.[A-Za-z0-9-_.]+$ . Then, the JSONPath expressions and corresponding regular expressions will be combined into a dict and assigned to config.component_validation in the configuration file. Assuming we enforce three \"strict\" validation requirements on the command component: (1) the component name starts with office.smartcompose. , (2) the image URL is empty or starts with polymerprod.azurecr.io , and (3) all the input parameter descriptions start with a capital letter. Below is an example of the configuration file that specifies the above three validation requirements. activation_method : all compliant_branch : ^refs/heads/develop$ component_specification_glob : 'steps/**/module_spec.yaml' log_format : '[%(name)s][%(levelname)s] - %(message)s' signing_mode : aml workspaces : - /subscriptions/2dea9532-70d2-472d-8ebd-6f53149ad551/resourcegroups/MOP.HERON.PROD.9e0f4782-7fd1-463a-8bcf-46afb7eeaca1/providers/Microsoft.MachineLearningServices/workspaces/amlworkspacedxb7igyvbjdhc allow_duplicate_versions : True use_build_number : True # strict component validation enable_component_validation : True component_validation : '$.name' : '^office.smartcompose.[A-Za-z0-9-_.]+$' '$.environment.docker.image' : '^$|^polymerprod.azurecr.io*$' '$.inputs..description' : '^[A-Z].*' The jsonpath-ng library is a JSONPath implementation for Python. We will leverage it to parse JSONPath in the shrike library, an example of which is shown below. >>> import yaml >>> from jsonpath_ng import parse >>> >>> with open ( 'tests/tests_pipeline/sample/steps/multinode_trainer/module_spec.yaml' , 'r' ) as file : ... spec = yaml . load ( file , Loader = yaml . FullLoader ) ... >>> jsonpath_expr = parse ( '$.name' ) >>> jsonpath_expr . find ( spec )[ 0 ] . value 'microsoft.com.amlds.multinodetrainer' >>> >>> jsonpath_expr = parse ( '$.environment.docker.image' ) >>> jsonpath_expr . find ( spec )[ 0 ] . value 'polymerprod.azurecr.io/training/pytorch:scpilot-rc2' >>> >>> jsonpath_expr = parse ( '$.inputs..description' ) >>> [ match . value for match in jsonpath_expr . find ( spec )] [ 'Vocabulary file used to encode data, can be a single file or a directory with a single file' , 'File used for training, can be raw text or already encoded using vocab' , 'File used for validation, can be raw text or already encoded using vocab' ]","title":"Component validation"},{"location":"rfc/strict-component-validation/#strict-validation-of-azure-ml-components","text":"Owners Approvers Participants Haozhe Zhang Daniel Miller AML DS 1P enablers This document presents a proposal on how to enable strict validation of Azure ML components in the signing and registering builds by using the shrike library.","title":"Strict Validation of Azure ML Components"},{"location":"rfc/strict-component-validation/#motivation","text":"At the prepare step of a signing and registering build, the shrike.build.command.prepare.validate_all_components() function executes an azure cli command az ml component validate --file ${component_spec_path} to validate whether the given component spec YAML file has any syntax errors or matches the strucuture of the pre-defined schema. The validation procedure, which is critical to satisfactory user experience, could greatly improve the code quality of user codebase and eliminate user errors before runtime. Nevertheless, different teams may have different \"rules\" for component validation. Apart from basic syntax checking and schema structure matching, our customers may have \"stricter\" validation requirements, to list a few, enforcing naming conventions(e.g., office.smartcompose.*) requiring email address in the component description forcing all input parameters to have non-empty descriptions To address this feature request, we plan to leverage (1) regular expression matching and (2) JSONPath expression in the shrike library.","title":"Motivation"},{"location":"rfc/strict-component-validation/#proposal","text":"We will introduce two parameters - enable_component_validation (type: boolean , default: False ) and component_validation (type: dict , default: None ) into the configuration dataclass in shrike.build.core . The logic for performing strict component validation will be If config.enable_component_validation is True : if len(config.component_validation) == 0 : run the compliance validation only else: run the compliance validation, then run the user-provided customized validation, which is written as JSONPath expression and regular expression in config.component_validation . else Pass","title":"Proposal"},{"location":"rfc/strict-component-validation/#compliance-validation","text":"Compliance validation refers to checking whether a given component spec YAML file meets all the requirements for running in the compliant AML. Currently, we have three requirements to enforce, which are Check whether the image URL is compliant Check whether the pip index-url is https://o365exchange.pkgs.visualstudio.com/_packaging/PolymerPythonPackages/pypi/simple/ Check that \"default\" is only Conda feed In the future, if we find more requirements to obey, we will add them into compliance validation.","title":"Compliance validation"},{"location":"rfc/strict-component-validation/#customized-validation","text":"First, we ask users to write JSONPath expressions to query Azure ML component spec YAML elements. For example, the path of component name is $.name , while the path of image is $.environment.docker.image . Second, users are expected to translate their specific \"strict\" validation rules to regular expression patterns. For example, enforcing the component name to start with \"office.smartcompose.\" could be translated to a string pattern ^office.smartcompose.[A-Za-z0-9-_.]+$ . Then, the JSONPath expressions and corresponding regular expressions will be combined into a dict and assigned to config.component_validation in the configuration file. Assuming we enforce three \"strict\" validation requirements on the command component: (1) the component name starts with office.smartcompose. , (2) the image URL is empty or starts with polymerprod.azurecr.io , and (3) all the input parameter descriptions start with a capital letter. Below is an example of the configuration file that specifies the above three validation requirements. activation_method : all compliant_branch : ^refs/heads/develop$ component_specification_glob : 'steps/**/module_spec.yaml' log_format : '[%(name)s][%(levelname)s] - %(message)s' signing_mode : aml workspaces : - /subscriptions/2dea9532-70d2-472d-8ebd-6f53149ad551/resourcegroups/MOP.HERON.PROD.9e0f4782-7fd1-463a-8bcf-46afb7eeaca1/providers/Microsoft.MachineLearningServices/workspaces/amlworkspacedxb7igyvbjdhc allow_duplicate_versions : True use_build_number : True # strict component validation enable_component_validation : True component_validation : '$.name' : '^office.smartcompose.[A-Za-z0-9-_.]+$' '$.environment.docker.image' : '^$|^polymerprod.azurecr.io*$' '$.inputs..description' : '^[A-Z].*' The jsonpath-ng library is a JSONPath implementation for Python. We will leverage it to parse JSONPath in the shrike library, an example of which is shown below. >>> import yaml >>> from jsonpath_ng import parse >>> >>> with open ( 'tests/tests_pipeline/sample/steps/multinode_trainer/module_spec.yaml' , 'r' ) as file : ... spec = yaml . load ( file , Loader = yaml . FullLoader ) ... >>> jsonpath_expr = parse ( '$.name' ) >>> jsonpath_expr . find ( spec )[ 0 ] . value 'microsoft.com.amlds.multinodetrainer' >>> >>> jsonpath_expr = parse ( '$.environment.docker.image' ) >>> jsonpath_expr . find ( spec )[ 0 ] . value 'polymerprod.azurecr.io/training/pytorch:scpilot-rc2' >>> >>> jsonpath_expr = parse ( '$.inputs..description' ) >>> [ match . value for match in jsonpath_expr . find ( spec )] [ 'Vocabulary file used to encode data, can be a single file or a directory with a single file' , 'File used for training, can be raw text or already encoded using vocab' , 'File used for validation, can be raw text or already encoded using vocab' ]","title":"Customized validation"},{"location":"spark/examples/","text":"Examples The simplest way to use this library is with the common pySpark entry script. It will expect command line arguments --zipFile and --binaryName , the values of which will be used to determine the archive and binary inside that archive to invoke using the .NET runner. All other command line arguments are passed directly to the compiled assembly. # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Demonstrate minimal use of the Spark .NET runner functionality. \"\"\" from shrike.spark import run_spark_net if __name__ == \"__main__\" : run_spark_net () :warning: The compiled C# assembly is responsible for prefixing log statements with SystemLog: and catching + prefixing exception stack traces. Long-term, that functionality will be provided by This library could provide a C# version of the logging functionality , but for now you will have to home-brew your own. It is possible to easily customize the command-line arguments for \"zip file\" and \"assembly name\", e.g. like this. run_spark_net ( \"--zip-file\" , \"--assembly-name\" ) For more advanced configuration, e.g. customizing the Spark session, use the run_spark_net_from_known_assembly method like below. # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Demonstrate advanced use of the Spark .NET runner functionality. \"\"\" import argparse from pyspark.sql import SparkSession from shrike.spark import run_spark_net_from_known_assembly def main ( args ): spark = SparkSession . builder . appName ( args . app_name ) . getOrCreate () run_spark_net_from_known_assembly ( spark , \"dotnet-publish.zip\" , \"assembly-name\" , [ \"--input\" , args . input_path ] ) if __name__ == \"__main__\" : parser = argparse . ArgumentParser () parser . add_argument ( \"--app-name\" ) parser . add_argument ( \"--input-path\" ) args = parser . parse_args () main ( args )","title":"Examples"},{"location":"spark/examples/#examples","text":"The simplest way to use this library is with the common pySpark entry script. It will expect command line arguments --zipFile and --binaryName , the values of which will be used to determine the archive and binary inside that archive to invoke using the .NET runner. All other command line arguments are passed directly to the compiled assembly. # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Demonstrate minimal use of the Spark .NET runner functionality. \"\"\" from shrike.spark import run_spark_net if __name__ == \"__main__\" : run_spark_net () :warning: The compiled C# assembly is responsible for prefixing log statements with SystemLog: and catching + prefixing exception stack traces. Long-term, that functionality will be provided by This library could provide a C# version of the logging functionality , but for now you will have to home-brew your own. It is possible to easily customize the command-line arguments for \"zip file\" and \"assembly name\", e.g. like this. run_spark_net ( \"--zip-file\" , \"--assembly-name\" ) For more advanced configuration, e.g. customizing the Spark session, use the run_spark_net_from_known_assembly method like below. # Copyright (c) Microsoft Corporation. # Licensed under the MIT license. \"\"\" Demonstrate advanced use of the Spark .NET runner functionality. \"\"\" import argparse from pyspark.sql import SparkSession from shrike.spark import run_spark_net_from_known_assembly def main ( args ): spark = SparkSession . builder . appName ( args . app_name ) . getOrCreate () run_spark_net_from_known_assembly ( spark , \"dotnet-publish.zip\" , \"assembly-name\" , [ \"--input\" , args . input_path ] ) if __name__ == \"__main__\" : parser = argparse . ArgumentParser () parser . add_argument ( \"--app-name\" ) parser . add_argument ( \"--input-path\" ) args = parser . parse_args () main ( args )","title":"Examples"},{"location":"spark/spark_net/","text":"Spark .NET Namespace containing Spark .NET utilities. Inspired by the closed-source implementations: https://dev.azure.com/msdata/Vienna/_git/aml-ds?version=GC94d20cb3f190e942b016c548308becc107fcede8&path=/recipes/signed-components/canary-hdi/run.py # noqa: E501 https://dev.azure.com/eemo/TEE/_git/TEEGit?version=GC8f000c8c61ae67cf1009d7a70753f0175968ef81&path=/Offline/SGI/src/python/src/sparknet/run.py # noqa: E501 https://o365exchange.visualstudio.com/O365%20Core/_git/EuclidOdinML?version=GC849128898f37d138725695fbba3992cd5f5f4474&path=/sources/dev/Projects/dotnet/OdinMLDotNet/DotnetSpark-0.4.0.py # noqa: E501 full_pyfile_path ( spark , py_file_name ) Resolve the full HDFS path of a file out of the Spark Session's PyFiles object. Source code in shrike/spark/spark_net.py def full_pyfile_path ( spark : SparkSession , py_file_name : str ) -> str : \"\"\" Resolve the full HDFS path of a file out of the Spark Session's PyFiles object. \"\"\" spark_conf = spark . sparkContext . getConf () py_files = spark_conf . get ( \"spark.yarn.dist.pyFiles\" ) file_names = py_files . split ( \",\" ) log . info ( f \"Searching py_files for { py_file_name } : ' { py_files } '\" , category = DataCategory . PUBLIC , ) for file_name in file_names : if file_name . split ( \"/\" )[ - 1 ] == py_file_name : return file_name raise PublicValueError ( f \"py_files do not contain { py_file_name } : { py_files } \" ) get_default_spark_session () Resolve a default Spark session for running Spark .NET applications. Source code in shrike/spark/spark_net.py def get_default_spark_session () -> SparkSession : \"\"\" Resolve a default Spark session for running Spark .NET applications. \"\"\" # https://stackoverflow.com/a/534847 random = str ( uuid . uuid4 ())[: 8 ] app_name = f \"spark-net- { random } \" log . info ( f \"Application name: { app_name } \" , category = DataCategory . PUBLIC ) rv = SparkSession . builder . appName ( app_name ) . getOrCreate () return rv java_args ( spark , args ) Convert a Python list into the corresponding Java argument array. Source code in shrike/spark/spark_net.py def java_args ( spark : SparkSession , args : List [ str ]): \"\"\" Convert a Python list into the corresponding Java argument array. \"\"\" rv = SparkContext . _gateway . new_array ( spark . _jvm . java . lang . String , len ( args )) # https://stackoverflow.com/a/522578 for index , arg in enumerate ( args ): rv [ index ] = arg return rv run_spark_net ( zip_file = '--zipFile' , binary_name = '--binaryName' , spark = None , args = None ) Easy entry point to one-line run a Spark .NET application. Simplest sample usage is: run_spark_net_with_smart_args() Source code in shrike/spark/spark_net.py def run_spark_net ( zip_file : str = \"--zipFile\" , binary_name : str = \"--binaryName\" , spark : Optional [ SparkSession ] = None , args : Optional [ list ] = None , ) -> None : \"\"\" Easy entry point to one-line run a Spark .NET application. Simplest sample usage is: > run_spark_net_with_smart_args() \"\"\" if not spark : spark = get_default_spark_session () if not args : args = sys . argv enable_compliant_logging () parser = argparse . ArgumentParser () parser . add_argument ( zip_file , dest = \"ZIP_FILE\" ) parser . add_argument ( binary_name , dest = \"BINARY_NAME\" ) try : ( known_args , unknown_args ) = parser . parse_known_args ( args ) zf = known_args . ZIP_FILE bn = known_args . BINARY_NAME except BaseException as e : raise PublicArgumentError ( None , str ( e )) from e run_spark_net_from_known_assembly ( spark , zf , bn , unknown_args ) run_spark_net_from_known_assembly ( spark , zip_file_name , assembly_name , args ) Invoke the binary assembly_name inside zip_file_name with the command line parameters args , using the provided Spark session. Print the Java stack trace if the job fails. Source code in shrike/spark/spark_net.py def run_spark_net_from_known_assembly ( spark : SparkSession , zip_file_name : str , assembly_name : str , args : List [ str ] ) -> None : \"\"\" Invoke the binary `assembly_name` inside `zip_file_name` with the command line parameters `args`, using the provided Spark session. Print the Java stack trace if the job fails. \"\"\" fully_resolved_zip_file_name = full_pyfile_path ( spark , zip_file_name ) dotnet_args = [ fully_resolved_zip_file_name , assembly_name ] + args log . info ( f \"Calling dotnet with arguments: { dotnet_args } \" , category = DataCategory . PUBLIC ) dotnet_args_java = java_args ( spark , dotnet_args ) message = None try : spark . _jvm . org . apache . spark . deploy . dotnet . DotnetRunner . main ( dotnet_args_java ) except py4j . protocol . Py4JJavaError as err : log . error ( \"Dotnet failed\" , category = DataCategory . PUBLIC ) for line in err . java_exception . getStackTrace (): log . error ( str ( line ), category = DataCategory . PUBLIC ) message = f \" { err . errmsg } { err . java_exception } \" if message : # Don't re-raise the existing exception since it's unprintable. # https://github.com/bartdag/py4j/issues/306 raise PublicRuntimeError ( message ) log . info ( \"Done running dotnet\" , category = DataCategory . PUBLIC )","title":"spark_net"},{"location":"spark/spark_net/#spark-net","text":"Namespace containing Spark .NET utilities. Inspired by the closed-source implementations: https://dev.azure.com/msdata/Vienna/_git/aml-ds?version=GC94d20cb3f190e942b016c548308becc107fcede8&path=/recipes/signed-components/canary-hdi/run.py # noqa: E501 https://dev.azure.com/eemo/TEE/_git/TEEGit?version=GC8f000c8c61ae67cf1009d7a70753f0175968ef81&path=/Offline/SGI/src/python/src/sparknet/run.py # noqa: E501 https://o365exchange.visualstudio.com/O365%20Core/_git/EuclidOdinML?version=GC849128898f37d138725695fbba3992cd5f5f4474&path=/sources/dev/Projects/dotnet/OdinMLDotNet/DotnetSpark-0.4.0.py # noqa: E501","title":"Spark .NET"},{"location":"spark/spark_net/#shrike.spark.spark_net.full_pyfile_path","text":"Resolve the full HDFS path of a file out of the Spark Session's PyFiles object. Source code in shrike/spark/spark_net.py def full_pyfile_path ( spark : SparkSession , py_file_name : str ) -> str : \"\"\" Resolve the full HDFS path of a file out of the Spark Session's PyFiles object. \"\"\" spark_conf = spark . sparkContext . getConf () py_files = spark_conf . get ( \"spark.yarn.dist.pyFiles\" ) file_names = py_files . split ( \",\" ) log . info ( f \"Searching py_files for { py_file_name } : ' { py_files } '\" , category = DataCategory . PUBLIC , ) for file_name in file_names : if file_name . split ( \"/\" )[ - 1 ] == py_file_name : return file_name raise PublicValueError ( f \"py_files do not contain { py_file_name } : { py_files } \" )","title":"full_pyfile_path()"},{"location":"spark/spark_net/#shrike.spark.spark_net.get_default_spark_session","text":"Resolve a default Spark session for running Spark .NET applications. Source code in shrike/spark/spark_net.py def get_default_spark_session () -> SparkSession : \"\"\" Resolve a default Spark session for running Spark .NET applications. \"\"\" # https://stackoverflow.com/a/534847 random = str ( uuid . uuid4 ())[: 8 ] app_name = f \"spark-net- { random } \" log . info ( f \"Application name: { app_name } \" , category = DataCategory . PUBLIC ) rv = SparkSession . builder . appName ( app_name ) . getOrCreate () return rv","title":"get_default_spark_session()"},{"location":"spark/spark_net/#shrike.spark.spark_net.java_args","text":"Convert a Python list into the corresponding Java argument array. Source code in shrike/spark/spark_net.py def java_args ( spark : SparkSession , args : List [ str ]): \"\"\" Convert a Python list into the corresponding Java argument array. \"\"\" rv = SparkContext . _gateway . new_array ( spark . _jvm . java . lang . String , len ( args )) # https://stackoverflow.com/a/522578 for index , arg in enumerate ( args ): rv [ index ] = arg return rv","title":"java_args()"},{"location":"spark/spark_net/#shrike.spark.spark_net.run_spark_net","text":"Easy entry point to one-line run a Spark .NET application. Simplest sample usage is: run_spark_net_with_smart_args() Source code in shrike/spark/spark_net.py def run_spark_net ( zip_file : str = \"--zipFile\" , binary_name : str = \"--binaryName\" , spark : Optional [ SparkSession ] = None , args : Optional [ list ] = None , ) -> None : \"\"\" Easy entry point to one-line run a Spark .NET application. Simplest sample usage is: > run_spark_net_with_smart_args() \"\"\" if not spark : spark = get_default_spark_session () if not args : args = sys . argv enable_compliant_logging () parser = argparse . ArgumentParser () parser . add_argument ( zip_file , dest = \"ZIP_FILE\" ) parser . add_argument ( binary_name , dest = \"BINARY_NAME\" ) try : ( known_args , unknown_args ) = parser . parse_known_args ( args ) zf = known_args . ZIP_FILE bn = known_args . BINARY_NAME except BaseException as e : raise PublicArgumentError ( None , str ( e )) from e run_spark_net_from_known_assembly ( spark , zf , bn , unknown_args )","title":"run_spark_net()"},{"location":"spark/spark_net/#shrike.spark.spark_net.run_spark_net_from_known_assembly","text":"Invoke the binary assembly_name inside zip_file_name with the command line parameters args , using the provided Spark session. Print the Java stack trace if the job fails. Source code in shrike/spark/spark_net.py def run_spark_net_from_known_assembly ( spark : SparkSession , zip_file_name : str , assembly_name : str , args : List [ str ] ) -> None : \"\"\" Invoke the binary `assembly_name` inside `zip_file_name` with the command line parameters `args`, using the provided Spark session. Print the Java stack trace if the job fails. \"\"\" fully_resolved_zip_file_name = full_pyfile_path ( spark , zip_file_name ) dotnet_args = [ fully_resolved_zip_file_name , assembly_name ] + args log . info ( f \"Calling dotnet with arguments: { dotnet_args } \" , category = DataCategory . PUBLIC ) dotnet_args_java = java_args ( spark , dotnet_args ) message = None try : spark . _jvm . org . apache . spark . deploy . dotnet . DotnetRunner . main ( dotnet_args_java ) except py4j . protocol . Py4JJavaError as err : log . error ( \"Dotnet failed\" , category = DataCategory . PUBLIC ) for line in err . java_exception . getStackTrace (): log . error ( str ( line ), category = DataCategory . PUBLIC ) message = f \" { err . errmsg } { err . java_exception } \" if message : # Don't re-raise the existing exception since it's unprintable. # https://github.com/bartdag/py4j/issues/306 raise PublicRuntimeError ( message ) log . info ( \"Done running dotnet\" , category = DataCategory . PUBLIC )","title":"run_spark_net_from_known_assembly()"}]}